{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# import modules\n",
    "##############################\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# define functions\n",
    "##############################\n",
    "\n",
    "def length(sequence):\n",
    "    \"\"\"\n",
    "    function that computes the real, unpadded lenghts for every sequence in batch\n",
    "    \"\"\"\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "\n",
    "def mlp(_X, _weights, _biases):\n",
    "    \"\"\"\n",
    "    function that defines a multilayer perceptron in the graph\n",
    "    input shape: parse_steps (=?) x filtered_words (=3) x lstm_output_length (=400)\n",
    "    output shape: parse_steps (=?) x num_classes\n",
    "    \"\"\"\n",
    "    # ReLU hidden layer (output shape: parse_steps x n_hidden)\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.einsum('ijk,kl->il', _X, _weights['h']), _biases['b'])) \n",
    "    # return output layer (output shape: parse_steps x n_classes)\n",
    "    return tf.add(tf.einsum('ik,kl->il', layer_1, _weights['out']), _biases['out'])\n",
    "\n",
    "def embedding_lookup(sentences, max_seq_length, vec_length):\n",
    "    \"\"\"\n",
    "    function that looks up embeddings.\n",
    "    input: list of sentences, length of sentences, length of word vectors\n",
    "    output: 3D array of word vectors per sentence \n",
    "                (dims #sentences x sentence_length x embedding_size)\n",
    "    \"\"\"\n",
    "    sentence_embeddings = np.empty((0,max_seq_length,vec_length))\n",
    "    for sentence in sentences:\n",
    "        word_embeddings = np.empty((0,vec_length))\n",
    "        for word in sentence:\n",
    "            word_embeddings = np.vstack([word_embeddings, model[word]])\n",
    "        if len(sentence) < max_seq_length:\n",
    "            zero_padding_length = max_seq_length - len(sentence)\n",
    "            word_embeddings = np.vstack([word_embeddings, np.zeros((zero_padding_length, vec_length))])\n",
    "        sentence_embeddings = np.append(sentence_embeddings, np.array([word_embeddings]), axis=0)\n",
    "    return sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was already loaded\n",
      "Sentence embedding shape (np-array):  (2, 7, 189)\n",
      "BiLSTM output shape:  (2, 7, 400)\n",
      "MLP output shape S0:  (4, 50)\n",
      "MLP output shape S1:  (3, 50)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ik (Lucas) staar me er een beetje blind op nu, dus Niels, zou jij \n",
    "kunnen nadenken over onderstaande punten en sowieso even naar de \n",
    "code kunnen kijken?\n",
    "\n",
    "TODO:\n",
    "\n",
    "- Ik ga er nu vanuit dat ik een lijst van zinnen en een lijst van parses\n",
    "terugkrijg. Ik weet niet goed hoe ik die moet opdelen in batches voor\n",
    "training. De dummy data die ik nu gebruik moet een batch voorstellen.\n",
    "\n",
    "- Tweede, gerelateerde issue: het model gaat nu er basically vanuit dat\n",
    "het een batch ontvangt. Ik denk dus dat het het beste is om een groot\n",
    "deel van de onderstaande code waarin ik de graph define (dus voordat\n",
    "de session begint) te wrappen in een functie (de \"train_op\") met \n",
    "tf.placeholders die via een feed_dict worden gevuld met de huidige batch.\n",
    "\n",
    "- Als ik vectoren probeer op te halen uit het model, maar dan voor \n",
    "woorden die er niet inzitten, dan krijg ik een error. Dit moeten we \n",
    "op een of andere manier zien te fixen.\n",
    "\n",
    "- Ik zit even vast en weet niet goed hoe ik de uiteindelijke session\n",
    "(= voornamelijk de training loop) die de graph runt, moet opbouwen. Er\n",
    "staan voorbeelden van online, maar ik kan ze nog niet concreet toepassen\n",
    "omdat ik nog geen concrete input data heb, en omdat ik nog niet weet hoe\n",
    "ik batches maak/afhandel. DIT IS EEN BELANGRIJK PUNT NOG!!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "##############################\n",
    "# build graph\n",
    "##############################\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # hyperparameters (from Cross & Huang, 2016)\n",
    "    batch_size = 10\n",
    "    n_input = 400\n",
    "    n_hidden = 200\n",
    "    n_classes = 50 # TODO: how many classes for the actual data?\n",
    "    lstm_units = 200\n",
    "    num_epochs = 10\n",
    "    dropout = 0.5\n",
    "    L2_penalty = 0.\n",
    "    rho = 0.99\n",
    "    epsilon = 1e-07\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h': tf.Variable(tf.random_normal([n_input, n_hidden], dtype=tf.float64)),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], dtype=tf.float64))\n",
    "    }\n",
    "    biases = {\n",
    "        'b': tf.Variable(tf.random_normal([n_hidden], dtype=tf.float64)),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes], dtype=tf.float64))\n",
    "    }\n",
    "\n",
    "    # load word2vec model\n",
    "    model_name = \"dep_parser_word2vec_total\"\n",
    "    if not 'model' in locals():\n",
    "        model = models.word2vec.Word2Vec.load(model_name)\n",
    "        print(\"Model loaded from disk\")\n",
    "    else:\n",
    "        print(\"Model was already loaded\")\n",
    "\n",
    "    \n",
    "    # dummy data:\n",
    "    # toy sentences (= list of lists of words)\n",
    "    sentences = [[\"the\", \"by\", \"an\", \"on\", \"the\", \"in\", \"an\"], [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"ground\"]]\n",
    "    \n",
    "    # toy parses (= array with an action and word indices)\n",
    "    parses = [np.array([[1,0,1,2], \n",
    "                        [0,1,2,3],\n",
    "                        [0,1,2,3],\n",
    "                        [1,0,1,2]]),\n",
    "              np.array([[1,0,1,2], \n",
    "                        [0,1,2,3], \n",
    "                        [3,2,3,4]])]\n",
    "\n",
    "    # variables from sentences\n",
    "    vec_length = model['a'].size\n",
    "    seq_lengths = [len(sentence) for sentence in sentences]\n",
    "    max_seq_length = max(seq_lengths)\n",
    "    \n",
    "\n",
    "    # look up embeddings of words in sentence\n",
    "    embeddings = embedding_lookup(sentences, max_seq_length, vec_length)\n",
    "    print(\"Sentence embedding shape (np-array): \", embeddings.shape)\n",
    "\n",
    "    # define LSTM cell + dropout wrapper (like Cross & Huang)\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=lstm_units, state_is_tuple=True)\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=0.5)\n",
    "\n",
    "    # define bidirectional architecture\n",
    "    outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=cell,\n",
    "        cell_bw=cell,\n",
    "        dtype=tf.float64,\n",
    "        sequence_length=seq_lengths,\n",
    "        inputs=embeddings\n",
    "    )\n",
    "\n",
    "    # fw/bw output (num_sequences x max_seq_length x lstm_units)\n",
    "    output_fw, output_bw = outputs\n",
    "\n",
    "    # concatenate forward & backward outputs per word\n",
    "    output_lstm = tf.concat(2, outputs)\n",
    "    print(\"BiLSTM output shape: \", output_lstm.get_shape())\n",
    "\n",
    "\n",
    "    # MLP layer filtering LSTM outputs based on parse steps\n",
    "    batch_cost = 0\n",
    "    for i in range(0, len(sentences)):\n",
    "        sentence, parse = output_lstm[i], parses[i]\n",
    "        # input: parse_steps (=?) x filtered_words (=3) x lstm_output_length (=400)\n",
    "        # output: parse_steps (=?) x num_classes\n",
    "        output_mlp = mlp(tf.gather(sentence, parse[:,1:]), weights, biases)\n",
    "        print(\"MLP output shape S{}: \".format(i), output_mlp.get_shape())\n",
    "        cost = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(output_mlp, parse[:,0]))\n",
    "        batch_cost += cost\n",
    "    batch_cost /= len(sentences)\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = tf.train.AdadeltaOptimizer(rho=rho, epsilon=epsilon).minimize(batch_cost)\n",
    "\n",
    "    \n",
    "    ##############################\n",
    "    # start session in graph\n",
    "    ##############################\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        # TODO: HOE WERKT TRAINING?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
