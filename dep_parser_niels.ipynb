{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import word2vec \n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as myfile:\n",
    "        f = myfile.readlines()\n",
    "        s_num = 0\n",
    "        i =0\n",
    "        word_s = []\n",
    "        tag_s = []\n",
    "        dep_s = []\n",
    "        s  = []   # sentence\n",
    "        p = []    # tag\n",
    "        d = []    # dependency\n",
    "        for l in f:\n",
    "            \n",
    "            v = l.replace('\\n','').split(\"\\t\")\n",
    "            v.append(s_num)\n",
    "            if len(l) != 1:\n",
    "                data.append(v)\n",
    "                dep = v[6] + '_' + v[7]\n",
    "                word = v[1].lower()\n",
    "                if any(char.isdigit() for char in word):\n",
    "                    word = 'NUM'       # replace numbers with NUM\n",
    "                s.append(word)\n",
    "                p.append(v[3])\n",
    "                d.append(dep)\n",
    "                i +=1\n",
    "            else:\n",
    "                word_s.append(s)\n",
    "                tag_s.append(p)\n",
    "                dep_s.append(d)\n",
    "                s_num +=1\n",
    "                s  = []\n",
    "                p = []\n",
    "                d = []\n",
    "        \n",
    "    return data, word_s, tag_s, dep_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  [['1', 'In', '_', 'IN', 'IN', '_', '45', 'prep', '_', '_', 0], ['2', 'an', '_', 'DT', 'DT', '_', '5', 'det', '_', '_', 0], ['3', 'Oct.', '_', 'NNP', 'NNP', '_', '5', 'nn', '_', '_', 0], ['4', '19', '_', 'CD', 'CD', '_', '5', 'num', '_', '_', 0], ['5', 'review', '_', 'NN', 'NN', '_', '1', 'pobj', '_', '_', 0]]\n",
      "words sentences:  [['in', 'an', 'oct.', 'NUM', 'review', 'of', '``', 'the', 'misanthrope', \"''\", 'at', 'chicago', \"'s\", 'goodman', 'theatre', '-lrb-', '``', 'revitalized', 'classics', 'take', 'the', 'stage', 'in', 'windy', 'city', ',', \"''\", 'leisure', '&', 'arts', '-rrb-', ',', 'the', 'role', 'of', 'celimene', ',', 'played', 'by', 'kim', 'cattrall', ',', 'was', 'mistakenly', 'attributed', 'to', 'christina', 'haag', '.'], ['ms.', 'haag', 'plays', 'elianti', '.'], ['rolls-royce', 'motor', 'cars', 'inc.', 'said', 'it', 'expects', 'its', 'u.s.', 'sales', 'to', 'remain', 'steady', 'at', 'about', 'NUM', 'cars', 'in', 'NUM', '.'], ['the', 'luxury', 'auto', 'maker', 'last', 'year', 'sold', 'NUM', 'cars', 'in', 'the', 'u.s.'], ['howard', 'mosher', ',', 'president', 'and', 'chief', 'executive', 'officer', ',', 'said', 'he', 'anticipates', 'growth', 'for', 'the', 'luxury', 'auto', 'maker', 'in', 'britain', 'and', 'europe', ',', 'and', 'in', 'far', 'eastern', 'markets', '.']]\n",
      "tags sentences:  [['IN', 'DT', 'NNP', 'CD', 'NN', 'IN', '``', 'DT', 'NN', \"''\", 'IN', 'NNP', 'POS', 'NNP', 'NNP', '-LRB-', '``', 'VBN', 'NNS', 'VBP', 'DT', 'NN', 'IN', 'NNP', 'NNP', ',', \"''\", 'NN', 'CC', 'NNS', '-RRB-', ',', 'DT', 'NN', 'IN', 'NNP', ',', 'VBN', 'IN', 'NNP', 'NNP', ',', 'VBD', 'RB', 'VBN', 'TO', 'NNP', 'NNP', '.'], ['NNP', 'NNP', 'VBZ', 'NNP', '.'], ['NNP', 'NNP', 'NNPS', 'NNP', 'VBD', 'PRP', 'VBZ', 'PRP$', 'NNP', 'NNS', 'TO', 'VB', 'JJ', 'IN', 'IN', 'CD', 'NNS', 'IN', 'CD', '.'], ['DT', 'NN', 'NN', 'NN', 'JJ', 'NN', 'VBD', 'CD', 'NNS', 'IN', 'DT', 'NNP'], ['NNP', 'NNP', ',', 'NN', 'CC', 'JJ', 'NN', 'NN', ',', 'VBD', 'PRP', 'VBZ', 'NN', 'IN', 'DT', 'NN', 'NN', 'NN', 'IN', 'NNP', 'CC', 'NNP', ',', 'CC', 'IN', 'JJ', 'JJ', 'NNS', '.']]\n",
      "dependencies:  [['45_prep', '5_det', '5_nn', '5_num', '1_pobj', '5_prep', '9_punct', '9_det', '6_pobj', '9_punct', '9_prep', '15_poss', '12_possessive', '15_nn', '11_pobj', '20_punct', '20_punct', '19_amod', '20_nsubj', '5_dep', '22_det', '20_dobj', '20_prep', '25_nn', '23_pobj', '20_punct', '20_punct', '20_dep', '28_cc', '28_conj', '20_punct', '45_punct', '34_det', '45_nsubjpass', '34_prep', '35_pobj', '34_punct', '34_partmod', '38_prep', '41_nn', '39_pobj', '34_punct', '45_auxpass', '45_advmod', '0_root', '45_prep', '48_nn', '46_pobj', '45_punct'], ['2_nn', '3_nsubj', '0_root', '3_dobj', '3_punct'], ['4_nn', '4_nn', '4_nn', '5_nsubj', '0_root', '7_nsubj', '5_ccomp', '10_poss', '10_nn', '12_nsubj', '12_aux', '7_xcomp', '12_acomp', '12_prep', '16_quantmod', '17_num', '14_pobj', '12_prep', '18_pobj', '5_punct'], ['4_det', '4_nn', '4_nn', '7_nsubj', '6_amod', '7_tmod', '0_root', '9_num', '7_dobj', '7_prep', '12_det', '10_pobj'], ['2_nn', '10_nsubj', '2_punct', '2_appos', '4_cc', '8_amod', '8_nn', '4_conj', '2_punct', '0_root', '12_nsubj', '10_ccomp', '12_dobj', '13_prep', '18_det', '18_nn', '18_nn', '14_pobj', '13_prep', '19_pobj', '20_cc', '20_conj', '19_punct', '19_cc', '19_conj', '27_amod', '28_amod', '25_pobj', '10_punct']]\n",
      "training word2vec model\n"
     ]
    }
   ],
   "source": [
    "train_data, train_words, train_tags, train_dependencies = read_data('./data/train-stanford-raw.conll')\n",
    "dev_data, dev_words, dev_tags, dev_dependencies = read_data('./data/dev-stanford-raw.conll')\n",
    "test_data, test_words, test_tags, test_dependencies = read_data('./data/test-stanford-raw.conll')\n",
    "print('data: ',data[:5])\n",
    "print('words sentences: ', words[2:5])\n",
    "print('tags sentences: ', tags[2:5])\n",
    "print('dependencies: ', dependencies[2:5])\n",
    "print(\"training word2vec model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(words,size=189, min_count=1) # here moeten we wat tweaken met de parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V (length of training vocabulary): 39547\n"
     ]
    }
   ],
   "source": [
    "vocab = model.vocab.keys()\n",
    "print(\"V (length of training vocabulary): \" + str(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
