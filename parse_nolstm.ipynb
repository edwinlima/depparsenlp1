{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "# import modules\n",
    "##############################\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim import models\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the file, \n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as myfile:\n",
    "        f = myfile.readlines()\n",
    "        s_num = 0\n",
    "        i =0\n",
    "        sentence_s = []\n",
    "        tag_s = []\n",
    "        dep_s = []\n",
    "        s  = []   # sentence\n",
    "        p = []    # tag\n",
    "        d = []    # dependency\n",
    "        for l in f:\n",
    "            \n",
    "            v = l.replace('\\n','').split(\"\\t\")\n",
    "            v.append(s_num)\n",
    "            if len(l) != 1:\n",
    "                data.append(v)\n",
    "                dep = v[6] + '_' + v[7]\n",
    "                word = v[1].lower()\n",
    "                if any(char.isdigit() for char in word):\n",
    "                    word = 'NUM'       # replace numbers with NUM\n",
    "                s.append(word)\n",
    "                p.append(v[3])\n",
    "                d.append(dep)\n",
    "                i +=1\n",
    "            else:\n",
    "                sentence_s.append(s)\n",
    "                tag_s.append(p)\n",
    "                dep_s.append(d)\n",
    "                s_num +=1\n",
    "                s  = []\n",
    "                p = []\n",
    "                d = []\n",
    "        \n",
    "    return data, sentence_s, tag_s, dep_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(dataname):\n",
    "    #reads in files, produces data structure with all actions\n",
    "        #does so by applying produce_rule_list to every sentence.\n",
    "        #for loop that sets actions to empty, calls p_r_l giving it\n",
    "        #the stack and buffer, actions and correct_parse, adds finished action list\n",
    "        #to new data file, for each sentence in the input data\n",
    "    #input: name of the data file with all parses. Run with data file in same directory.\n",
    "    #output: data file with all actions\n",
    "    file = open(dataname)\n",
    "    data = file.read()\n",
    "    correct_parses = correct_parse_list(data)\n",
    "    #gets rid of final whitespace\n",
    "    del correct_parses[len(correct_parses)-1]\n",
    "    \n",
    "    #iterates over all parses, producing action list for each\n",
    "    complete_rule_list = []\n",
    "    arc_dict = {'Shift':0,'L_root':1,'R_root':2}\n",
    "    for sentence_parse in correct_parses:\n",
    "        stack = []\n",
    "#         print(len(sentence_parse))\n",
    "        buff = list(range(1,len(sentence_parse)+1))\n",
    "        actions = []\n",
    "        rule_list, arc_dict = produce_rule_list(stack, buff, actions, sentence_parse, arc_dict)\n",
    "        complete_rule_list.append(np.array(rule_list))\n",
    "\n",
    "    \n",
    "    return complete_rule_list, arc_dict\n",
    "\n",
    "def correct_parse_list(data):\n",
    "    #Turns data into a list of lists of lists with relevant information\n",
    "    correct_parse = data.split(\"\\n\\n\")\n",
    "    for index, paragraph in enumerate(correct_parse):\n",
    "        correct_parse[index] = paragraph.split(\"\\n\")\n",
    "    for paragraph in correct_parse:\n",
    "        for index, line in enumerate(paragraph):\n",
    "            paragraph[index] = line.split(\"\\t\")\n",
    "    return correct_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_rule_list(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #recursive function that works through words in the sentence (stack/buffer)\n",
    "        #until only one word is left, creating the list of actions \n",
    "        #that was taken to parse it.\n",
    "    #input: stack, buffer, actions, correct parse\n",
    "    #output: actions with the actions taken for each buff/stack configuration\n",
    "    \n",
    "    #base case\n",
    "    if len(stack) == 1 and len(buff) == 0:\n",
    "        #actions.append([stack[:], \"empty\", \"R_arc\"])\n",
    "        actions.append([0,stack[0], 0, 2])\n",
    "        return actions, arc_dict\n",
    "\n",
    "    #If enough of the sentence is still left:\n",
    "    #If there is not enough material in the stack, shift:\n",
    "    if len(stack) == 0 :\n",
    "        #print('chose S - small stack')\n",
    "        actions.append([0,0,buff[0], 0])\n",
    "        stack.append(buff[0])\n",
    "        del buff[0]        \n",
    "        return produce_rule_list(stack,buff,actions,sentence_parse, arc_dict)\n",
    "    if len(stack) == 1:\n",
    "        actions.append([0,stack[-1],buff[0], 0])\n",
    "        stack.append(buff[0])\n",
    "        del buff[0]\n",
    "        return produce_rule_list(stack,buff,actions,sentence_parse, arc_dict)\n",
    "    #If there are 2 or more words in the stack, decide which action to perform and perform it\n",
    "    if len(stack) > 1:\n",
    "        action = rule_decision(stack,buff,sentence_parse)\n",
    "        stack, buff, actions, arc_dict = action(stack,buff,actions, sentence_parse, arc_dict)\n",
    "        return produce_rule_list(stack,buff,actions,sentence_parse, arc_dict)\n",
    "    \n",
    "\n",
    "def rule_decision(stack, buff, sentence_parse):\n",
    "    #determines which action to apply\n",
    "    #input: words on stack, words on buff, correct parse\n",
    "    #output: one of three methods, Shift(), L_arc(), R_arc()\n",
    "\n",
    "    #find ids/heads (index [6]) from stack and sentence_parse\n",
    "    s1 = stack[-2]\n",
    "    head_of_s1 = int(sentence_parse[s1-1][6])\n",
    "    s2 = stack[-1]\n",
    "    head_of_s2 = int(sentence_parse[s2-1][6])\n",
    "    \n",
    "    #L arcs can always be applied if possible\n",
    "    if head_of_s1 == s2:\n",
    "        action = L_arc\n",
    "        #print('chose L')\n",
    "    else:\n",
    "        #R arcs can only be applied if there is no word in the buffer which has the last word in the stack as a head\n",
    "        if head_of_s2 == s1:\n",
    "            buff_heads = [int(sentence_parse[x-1][6]) for x in buff]\n",
    "            if s2 in buff_heads:\n",
    "                action = Shift\n",
    "                #print('chose S - s2 in buffheads')\n",
    "            else:\n",
    "                action = R_arc\n",
    "                #print('chose R')\n",
    "        #if there is no match between s1 and s2, simply shift another word from the buffer\n",
    "        else:\n",
    "            action = Shift\n",
    "            #print('chose S - no matching s1s2')\n",
    "\n",
    "    return action\n",
    "\n",
    "#The following methods perform an arc or shift. These can be changed if more data is needed in the network.\n",
    "\n",
    "def L_arc(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #removes second to last item from stack, writes action to actions\n",
    "    #input: stack and actions\n",
    "    #output: new stack and actions with one L_arc line\n",
    "    #s1, s2, b1, action\n",
    "    s1 = int(stack[-2])\n",
    "    s2 = int(stack[-1])\n",
    "    if len(buff) == 0:\n",
    "        b1 = 0\n",
    "    else:\n",
    "        b1 = int(buff[0])\n",
    "    relation = \"L_\"+sentence_parse[s1-1][7]\n",
    "\n",
    "    if relation not in arc_dict:\n",
    "        maximum = max(arc_dict, key=arc_dict.get)\n",
    "        arc_dict['L_'+relation[2:]] = arc_dict[maximum]+1\n",
    "        arc_dict['R_'+relation[2:]] = arc_dict[maximum]+2\n",
    "    \n",
    "\n",
    "    actions.append([s1,s2,b1, arc_dict[relation]])\n",
    "    del stack[-2]\n",
    "    return stack, buff, actions, arc_dict\n",
    "\n",
    "\n",
    "\n",
    "def R_arc(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #removes last item from the stack, writes action to actions\n",
    "    #input: stack and actions\n",
    "    #output: new stack and actions with one R_arc line\n",
    "    #s1, s2, b1, action\n",
    "    s1 = int(stack[-2])\n",
    "    s2 = int(stack[-1])\n",
    "    if len(buff) == 0:\n",
    "        b1 = 0\n",
    "    else:\n",
    "        b1 = int(buff[0])\n",
    "        \n",
    "    relation = \"R_\"+sentence_parse[s2-1][7]\n",
    "\n",
    "    if relation not in arc_dict:\n",
    "        maximum = max(arc_dict, key=arc_dict.get)\n",
    "        arc_dict['L_'+relation[2:]] = arc_dict[maximum]+1\n",
    "        arc_dict['R_'+relation[2:]] = arc_dict[maximum]+2 \n",
    "    \n",
    "    actions.append([s1,s2,b1, arc_dict[relation]])\n",
    "    del stack[-1]\n",
    "    return stack, buff, actions, arc_dict\n",
    "\n",
    "\n",
    "\n",
    "def Shift(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #moves an item from the buff to the stack, writes action to actions\n",
    "    #input: stack, buff and actions\n",
    "    #output: new stack and actions with one extra shift line\n",
    "    #s1, s2, b1, action\n",
    "    s1 = int(stack[-2])\n",
    "    s2 = int(stack[-1])\n",
    "    b1 = int(buff[0])\n",
    "    #actions.append([stack[:], buff[:], \"Shift\"])\n",
    "    actions.append([s1,s2,b1, 0])\n",
    "    stack.append(buff[0])\n",
    "    del buff[0]\n",
    "    return stack, buff, actions, arc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  [['1', 'In', '_', 'IN', 'IN', '_', '45', 'prep', '_', '_', 0], ['2', 'an', '_', 'DT', 'DT', '_', '5', 'det', '_', '_', 0]]\n",
      "words sentences:  [['rolls-royce', 'motor', 'cars', 'inc.', 'said', 'it', 'expects', 'its', 'u.s.', 'sales', 'to', 'remain', 'steady', 'at', 'about', 'NUM', 'cars', 'in', 'NUM', '.'], ['the', 'luxury', 'auto', 'maker', 'last', 'year', 'sold', 'NUM', 'cars', 'in', 'the', 'u.s.']]\n",
      "tags sentences:  [['NNP', 'NNP', 'NNPS', 'NNP', 'VBD', 'PRP', 'VBZ', 'PRP$', 'NNP', 'NNS', 'TO', 'VB', 'JJ', 'IN', 'IN', 'CD', 'NNS', 'IN', 'CD', '.'], ['DT', 'NN', 'NN', 'NN', 'JJ', 'NN', 'VBD', 'CD', 'NNS', 'IN', 'DT', 'NNP']]\n",
      "dependencies:  [['4_nn', '4_nn', '4_nn', '5_nsubj', '0_root', '7_nsubj', '5_ccomp', '10_poss', '10_nn', '12_nsubj', '12_aux', '7_xcomp', '12_acomp', '12_prep', '16_quantmod', '17_num', '14_pobj', '12_prep', '18_pobj', '5_punct'], ['4_det', '4_nn', '4_nn', '7_nsubj', '6_amod', '7_tmod', '0_root', '9_num', '7_dobj', '7_prep', '12_det', '10_pobj']]\n"
     ]
    }
   ],
   "source": [
    "train_data, train_sentences, train_tags, train_dependencies = read_data('./data/train-stanford-raw.conll')\n",
    "dev_data, dev_sentences, dev_tags, dev_dependencies = read_data('./data/dev-stanford-raw.conll')\n",
    "test_data, test_sentences, test_tags, test_dependencies = read_data('./data/test-stanford-raw.conll')\n",
    "\n",
    "# create a full set of all the words in our train, test, and dev sets for word2vec model\n",
    "# in order to avoid unseen words during test and validation\n",
    "total_sentences = train_sentences + dev_sentences + test_sentences\n",
    "print('data: ', train_data[:2])\n",
    "print('words sentences: ', total_sentences[2:4])\n",
    "print('tags sentences: ', train_tags[2:4])\n",
    "print('dependencies: ', train_dependencies[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action_data, arc_dict = process_data('./data/train-stanford-raw.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Interactive parser and evaluation functions #\n",
    "###############################################\n",
    "\n",
    "def sentences_to_conll(sentences, arc_dict, file_name):\n",
    "    action_dict = {v:k for k,v in arc_dict.items()}\n",
    "    all_sentences_listed = []\n",
    "    for sentence in sentences:\n",
    "        sentence_parse = [[i+1,word,'_','_','_','_','_','_','_','_'] \n",
    "                          for i, word in enumerate(sentence)]\n",
    "        w2v_matrix = create_sentence_embeddings([sentence])[0]\n",
    "        stack = []\n",
    "        buff = list(range(1,len(sentence)+1))\n",
    "        sentence_parse = single_sentence_parse(stack, buff, sentence_parse, action_dict, w2v_matrix)\n",
    "        all_sentences_listed.append(sentence_parse)\n",
    "    convert_to_conll(all_sentences_listed, file_name)        \n",
    "    return\n",
    "\n",
    "def single_sentence_parse(stack, buff, sentence_parse, action_dict, w2v_matrix):\n",
    "    #If there are 2 or more words in the stack, decide which action to perform and perform it\n",
    "    if len(stack) > 1:\n",
    "        s1 = int(stack[-2])\n",
    "        s2 = int(stack[-1])\n",
    "        #checks whether buffer contains words\n",
    "        if len(buff) > 0:\n",
    "            b1 = int(buff[0])\n",
    "            action = model_action_decision(w2v_matrix,s1,s2,b1,False)\n",
    "        else:\n",
    "            b1 = 0\n",
    "            action = model_action_decision(w2v_matrix,s1,s2,b1,True)\n",
    "        \n",
    "        if action == 0:\n",
    "            # perform a shift\n",
    "            stack, buff = Shift(stack, buff)\n",
    "        elif action%2 == 1:\n",
    "            # left-arc. All left tags are odd in the dictionary\n",
    "            stack, sentence_parse = L_arc(stack,s1,s2, sentence_parse, action_dict, action)\n",
    "        else:\n",
    "            # right-arc. All right tags are even in the dictionary\n",
    "            stack, sentence_parse = R_arc(stack,s1,s2, sentence_parse, action_dict, action)\n",
    "        return single_sentence_parse(stack, buff, sentence_parse, action_dict, w2v_matrix)\n",
    "    \n",
    "    #base case (R_arc): if only one word is left, perform the last right arc with root.\n",
    "    if len(stack) == 1 and len(buff) == 0:\n",
    "        sentence_parse[stack[0]-1][6] = 0\n",
    "        sentence_parse[stack[0]-1][7] = 'root'\n",
    "        return sentence_parse    \n",
    "\n",
    "    #If there is not enough material in the stack, shift:\n",
    "    if len(stack) == 0 :\n",
    "        #print('chose S - small stack')\n",
    "        stack, buff = Shift(stack, buff)       \n",
    "        return single_sentence_parse(stack, buff, sentence_parse, action_dict, w2v_matrix)\n",
    "    if len(stack) == 1:\n",
    "        stack, buff = Shift(stack, buff)\n",
    "        return single_sentence_parse(stack, buff, sentence_parse, action_dict, w2v_matrix)\n",
    "    \n",
    "def model_action_decision(w2v_matrix,s1,s2,b1,emptybuffer):\n",
    "    #if emptybuffer is true, exclude option 0 (shift).\n",
    "    pred_input = {sentence_length: [len(w2v_matrix)], \n",
    "                  lstm_x: [w2v_matrix],  parse_indices: \n",
    "                  [np.array([[s1,s2,b1]])]} # feed_dict without labels\n",
    "    prediction = session.run(output_mlp, pred_input)[0]\n",
    "    if emptybuffer == False:\n",
    "        action = np.argmax(prediction)\n",
    "    else:\n",
    "        pred = np.delete(prediction,[0])\n",
    "        action = np.argmax(pred)+1\n",
    "    return action\n",
    "        \n",
    "    \n",
    "def L_arc(stack,s1,s2, sentence_parse, action_dict, action):\n",
    "    #removes second to last item from stack, sends info to sentence_parse\n",
    "\n",
    "    action_type = action_dict[action]\n",
    "    \n",
    "    #update head and relation for s1\n",
    "    sentence_parse[s1-1][6] = s2\n",
    "    sentence_parse[s1-1][7] = action_type[2:]\n",
    "    \n",
    "    del stack[-2]\n",
    "    return stack, sentence_parse\n",
    "\n",
    "\n",
    "def R_arc(stack,s1,s2, sentence_parse, action_dict, action):\n",
    "    #removes last item from the stack, sends info to sentence_parse\n",
    "    \n",
    "    action_type = action_dict[action]\n",
    "\n",
    "    #update head and relation for s2\n",
    "    sentence_parse[s2-1][6] = s1\n",
    "    sentence_parse[s2-1][7] = action_type[2:]\n",
    "    \n",
    "    del stack[-1]\n",
    "    return stack, sentence_parse\n",
    "\n",
    "\n",
    "def Shift(stack, buff):\n",
    "    #moves an item from the buff to the stack\n",
    "    #input: stack, buff\n",
    "    #output: new stack and buff\n",
    "    stack.append(buff[0])\n",
    "    del buff[0]\n",
    "    return stack, buff\n",
    "\n",
    "def convert_to_conll(sentences, file_name):\n",
    "    content = \"\\n\\n\".join([\"\\n\".join([\"\\t\".join([str(var) \n",
    "                                                 for var in word]) \n",
    "                                      for word in sentence]) \n",
    "                           for sentence in sentences]) + \"\\n\"\n",
    "    with open(file_name+\".conll\", \"w\") as text_file:\n",
    "        text_file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# TF functions\n",
    "##############################\n",
    "\n",
    "def mlp(_X, _weights, _biases):\n",
    "    \"\"\"\n",
    "    function that defines a multilayer perceptron in the graph\n",
    "    input shape: parse_steps (=?) x filtered_words (=3) x lstm_output_length (=400)\n",
    "    output shape: parse_steps (=?) x num_classes\n",
    "    \"\"\"\n",
    "    # ReLU hidden layer (output shape: parse_steps x n_hidden)\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h']), _biases['b'])) \n",
    "    # return output layer (output shape: parse_steps x n_classes)\n",
    "    return tf.add(tf.matmul(layer_1, _weights['out']), _biases['out'])\n",
    "\n",
    "\n",
    "def create_sentence_embeddings(sentences):\n",
    "    \"\"\"\n",
    "    for each sentence, get embedded representation\n",
    "    \"\"\"\n",
    "    embedded_train_sentences = []\n",
    "    for sentence in sentences:\n",
    "        embed = model[sentence]\n",
    "        embedded_train_sentences.append(embed)\n",
    "    return embedded_train_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# load word2vec model & input data\n",
    "##############################\n",
    "\n",
    "model_name = \"dep_parser_word2vec_total\"\n",
    "model = word2vec.Word2Vec.load(model_name)\n",
    "\n",
    "# embeddings for all sentences\n",
    "sentence_embeddings = create_sentence_embeddings(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 , iteration  0 \n",
      " current average loss:  285.287768747\n",
      "evaluating..\n",
      "  Labeled   attachment score: 1 / 167 * 100 = 0.60 %\n",
      "  Unlabeled attachment score: 20 / 167 * 100 = 11.98 %\n",
      "  Label accuracy score:       2 / 167 * 100 = 1.20 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-0-0\n",
      "epoch  0 , iteration  1000 \n",
      " current average loss:  130.067720395\n",
      "epoch  0 , iteration  2000 \n",
      " current average loss:  109.833931728\n",
      "epoch  0 , iteration  3000 \n",
      " current average loss:  89.2713291921\n",
      "epoch  0 , iteration  4000 \n",
      " current average loss:  83.0232905261\n",
      "epoch  0 , iteration  5000 \n",
      " current average loss:  56.3369658198\n",
      "evaluating..\n",
      "  Labeled   attachment score: 11 / 167 * 100 = 6.59 %\n",
      "  Unlabeled attachment score: 23 / 167 * 100 = 13.77 %\n",
      "  Label accuracy score:       22 / 167 * 100 = 13.17 %\n",
      "\n",
      "epoch  0 , iteration  6000 \n",
      " current average loss:  80.5913741313\n",
      "epoch  0 , iteration  7000 \n",
      " current average loss:  109.479593096\n",
      "epoch  0 , iteration  8000 \n",
      " current average loss:  55.7923794984\n",
      "epoch  0 , iteration  9000 \n",
      " current average loss:  56.0668615468\n",
      "epoch  0 , iteration  10000 \n",
      " current average loss:  22.5732258581\n",
      "evaluating..\n",
      "  Labeled   attachment score: 22 / 167 * 100 = 13.17 %\n",
      "  Unlabeled attachment score: 34 / 167 * 100 = 20.36 %\n",
      "  Label accuracy score:       39 / 167 * 100 = 23.35 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-0-10000\n",
      "epoch  0 , iteration  11000 \n",
      " current average loss:  57.6160235194\n",
      "epoch  0 , iteration  12000 \n",
      " current average loss:  48.1315401419\n",
      "epoch  0 , iteration  13000 \n",
      " current average loss:  20.990479334\n",
      "epoch  0 , iteration  14000 \n",
      " current average loss:  54.3761512823\n",
      "epoch  0 , iteration  15000 \n",
      " current average loss:  23.793822333\n",
      "evaluating..\n",
      "  Labeled   attachment score: 31 / 167 * 100 = 18.56 %\n",
      "  Unlabeled attachment score: 43 / 167 * 100 = 25.75 %\n",
      "  Label accuracy score:       53 / 167 * 100 = 31.74 %\n",
      "\n",
      "epoch  0 , iteration  16000 \n",
      " current average loss:  34.5800925762\n",
      "epoch  0 , iteration  17000 \n",
      " current average loss:  14.2670361143\n",
      "epoch  0 , iteration  18000 \n",
      " current average loss:  12.2378944356\n",
      "epoch  0 , iteration  19000 \n",
      " current average loss:  49.9636865091\n",
      "epoch  0 , iteration  20000 \n",
      " current average loss:  25.1026141844\n",
      "evaluating..\n",
      "  Labeled   attachment score: 33 / 167 * 100 = 19.76 %\n",
      "  Unlabeled attachment score: 48 / 167 * 100 = 28.74 %\n",
      "  Label accuracy score:       52 / 167 * 100 = 31.14 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-0-20000\n",
      "epoch  0 , iteration  21000 \n",
      " current average loss:  27.7896757739\n",
      "epoch  0 , iteration  22000 \n",
      " current average loss:  23.4898945073\n",
      "epoch  0 , iteration  23000 \n",
      " current average loss:  35.1885812693\n",
      "epoch  0 , iteration  24000 \n",
      " current average loss:  32.5799588785\n",
      "epoch  0 , iteration  25000 \n",
      " current average loss:  19.9497160338\n",
      "evaluating..\n",
      "  Labeled   attachment score: 33 / 167 * 100 = 19.76 %\n",
      "  Unlabeled attachment score: 49 / 167 * 100 = 29.34 %\n",
      "  Label accuracy score:       57 / 167 * 100 = 34.13 %\n",
      "\n",
      "epoch  0 , iteration  26000 \n",
      " current average loss:  27.6138486597\n",
      "epoch  0 , iteration  27000 \n",
      " current average loss:  34.2991855478\n",
      "epoch  0 , iteration  28000 \n",
      " current average loss:  26.9403137279\n",
      "epoch  0 , iteration  29000 \n",
      " current average loss:  26.9797205902\n",
      "epoch  0 , iteration  30000 \n",
      " current average loss:  16.2580743012\n",
      "evaluating..\n",
      "  Labeled   attachment score: 36 / 167 * 100 = 21.56 %\n",
      "  Unlabeled attachment score: 55 / 167 * 100 = 32.93 %\n",
      "  Label accuracy score:       60 / 167 * 100 = 35.93 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-0-30000\n",
      "epoch  0 , iteration  31000 \n",
      " current average loss:  18.3413147241\n",
      "epoch  0 , iteration  32000 \n",
      " current average loss:  22.0380687637\n",
      "epoch  0 , iteration  33000 \n",
      " current average loss:  14.1723097108\n",
      "epoch  0 , iteration  34000 \n",
      " current average loss:  21.3148953764\n",
      "epoch  0 , iteration  35000 \n",
      " current average loss:  26.0004495559\n",
      "evaluating..\n",
      "  Labeled   attachment score: 43 / 167 * 100 = 25.75 %\n",
      "  Unlabeled attachment score: 65 / 167 * 100 = 38.92 %\n",
      "  Label accuracy score:       65 / 167 * 100 = 38.92 %\n",
      "\n",
      "epoch  0 , iteration  36000 \n",
      " current average loss:  13.8503604805\n",
      "epoch  0 , iteration  37000 \n",
      " current average loss:  26.9558918846\n",
      "epoch  0 , iteration  38000 \n",
      " current average loss:  20.7789400552\n",
      "epoch  0 , iteration  39000 \n",
      " current average loss:  21.3589563107\n",
      "saving model at end of epoch..\n",
      "epoch  1 , iteration  0 \n",
      " current average loss:  16.7154757846\n",
      "evaluating..\n",
      "  Labeled   attachment score: 47 / 167 * 100 = 28.14 %\n",
      "  Unlabeled attachment score: 69 / 167 * 100 = 41.32 %\n",
      "  Label accuracy score:       70 / 167 * 100 = 41.92 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-1-0\n",
      "epoch  1 , iteration  1000 \n",
      " current average loss:  14.5916698184\n",
      "epoch  1 , iteration  2000 \n",
      " current average loss:  16.4202239714\n",
      "epoch  1 , iteration  3000 \n",
      " current average loss:  12.9047376237\n",
      "epoch  1 , iteration  4000 \n",
      " current average loss:  18.7757306286\n",
      "epoch  1 , iteration  5000 \n",
      " current average loss:  10.6338252584\n",
      "evaluating..\n",
      "  Labeled   attachment score: 50 / 167 * 100 = 29.94 %\n",
      "  Unlabeled attachment score: 71 / 167 * 100 = 42.51 %\n",
      "  Label accuracy score:       75 / 167 * 100 = 44.91 %\n",
      "\n",
      "epoch  1 , iteration  6000 \n",
      " current average loss:  17.7917658092\n",
      "epoch  1 , iteration  7000 \n",
      " current average loss:  40.1221874541\n",
      "epoch  1 , iteration  8000 \n",
      " current average loss:  13.259746202\n",
      "epoch  1 , iteration  9000 \n",
      " current average loss:  11.5309000332\n",
      "epoch  1 , iteration  10000 \n",
      " current average loss:  6.83303726391\n",
      "evaluating..\n",
      "  Labeled   attachment score: 53 / 167 * 100 = 31.74 %\n",
      "  Unlabeled attachment score: 74 / 167 * 100 = 44.31 %\n",
      "  Label accuracy score:       79 / 167 * 100 = 47.31 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-1-10000\n",
      "epoch  1 , iteration  11000 \n",
      " current average loss:  22.8024031702\n",
      "epoch  1 , iteration  12000 \n",
      " current average loss:  18.3510590069\n",
      "epoch  1 , iteration  13000 \n",
      " current average loss:  7.56759084817\n",
      "epoch  1 , iteration  14000 \n",
      " current average loss:  23.7691192742\n",
      "epoch  1 , iteration  15000 \n",
      " current average loss:  11.0391364551\n",
      "evaluating..\n",
      "  Labeled   attachment score: 54 / 167 * 100 = 32.34 %\n",
      "  Unlabeled attachment score: 74 / 167 * 100 = 44.31 %\n",
      "  Label accuracy score:       79 / 167 * 100 = 47.31 %\n",
      "\n",
      "epoch  1 , iteration  16000 \n",
      " current average loss:  16.5757184939\n",
      "epoch  1 , iteration  17000 \n",
      " current average loss:  6.63114010909\n",
      "epoch  1 , iteration  18000 \n",
      " current average loss:  5.24396740343\n",
      "epoch  1 , iteration  19000 \n",
      " current average loss:  28.183585448\n",
      "epoch  1 , iteration  20000 \n",
      " current average loss:  2.98304531028\n",
      "evaluating..\n",
      "  Labeled   attachment score: 55 / 167 * 100 = 32.93 %\n",
      "  Unlabeled attachment score: 74 / 167 * 100 = 44.31 %\n",
      "  Label accuracy score:       81 / 167 * 100 = 48.50 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-1-20000\n",
      "epoch  1 , iteration  21000 \n",
      " current average loss:  19.2966893074\n",
      "epoch  1 , iteration  22000 \n",
      " current average loss:  10.3203273637\n",
      "epoch  1 , iteration  23000 \n",
      " current average loss:  17.9154383603\n",
      "epoch  1 , iteration  24000 \n",
      " current average loss:  19.7960239943\n",
      "epoch  1 , iteration  25000 \n",
      " current average loss:  10.9093263636\n",
      "evaluating..\n",
      "  Labeled   attachment score: 54 / 167 * 100 = 32.34 %\n",
      "  Unlabeled attachment score: 68 / 167 * 100 = 40.72 %\n",
      "  Label accuracy score:       82 / 167 * 100 = 49.10 %\n",
      "\n",
      "epoch  1 , iteration  26000 \n",
      " current average loss:  12.1698895385\n",
      "epoch  1 , iteration  27000 \n",
      " current average loss:  16.1569673909\n",
      "epoch  1 , iteration  28000 \n",
      " current average loss:  15.9632755821\n",
      "epoch  1 , iteration  29000 \n",
      " current average loss:  16.6241817317\n",
      "epoch  1 , iteration  30000 \n",
      " current average loss:  8.15260092718\n",
      "evaluating..\n",
      "  Labeled   attachment score: 56 / 167 * 100 = 33.53 %\n",
      "  Unlabeled attachment score: 71 / 167 * 100 = 42.51 %\n",
      "  Label accuracy score:       83 / 167 * 100 = 49.70 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-1-30000\n",
      "epoch  1 , iteration  31000 \n",
      " current average loss:  10.1499298221\n",
      "epoch  1 , iteration  32000 \n",
      " current average loss:  16.0061800282\n",
      "epoch  1 , iteration  33000 \n",
      " current average loss:  8.54114902577\n",
      "epoch  1 , iteration  34000 \n",
      " current average loss:  9.72628810502\n",
      "epoch  1 , iteration  35000 \n",
      " current average loss:  16.1196152307\n",
      "evaluating..\n",
      "  Labeled   attachment score: 60 / 167 * 100 = 35.93 %\n",
      "  Unlabeled attachment score: 73 / 167 * 100 = 43.71 %\n",
      "  Label accuracy score:       89 / 167 * 100 = 53.29 %\n",
      "\n",
      "epoch  1 , iteration  36000 \n",
      " current average loss:  7.57912722592\n",
      "epoch  1 , iteration  37000 \n",
      " current average loss:  17.5688839712\n",
      "epoch  1 , iteration  38000 \n",
      " current average loss:  11.1460129417\n",
      "epoch  1 , iteration  39000 \n",
      " current average loss:  15.5136075613\n",
      "saving model at end of epoch..\n",
      "epoch  2 , iteration  0 \n",
      " current average loss:  11.2721976936\n",
      "evaluating..\n",
      "  Labeled   attachment score: 58 / 167 * 100 = 34.73 %\n",
      "  Unlabeled attachment score: 71 / 167 * 100 = 42.51 %\n",
      "  Label accuracy score:       88 / 167 * 100 = 52.69 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-2-0\n",
      "epoch  2 , iteration  1000 \n",
      " current average loss:  10.0775846847\n",
      "epoch  2 , iteration  2000 \n",
      " current average loss:  7.10463646744\n",
      "epoch  2 , iteration  3000 \n",
      " current average loss:  10.2745464223\n",
      "epoch  2 , iteration  4000 \n",
      " current average loss:  10.1531268499\n",
      "epoch  2 , iteration  5000 \n",
      " current average loss:  6.24768782685\n",
      "evaluating..\n",
      "  Labeled   attachment score: 60 / 167 * 100 = 35.93 %\n",
      "  Unlabeled attachment score: 74 / 167 * 100 = 44.31 %\n",
      "  Label accuracy score:       86 / 167 * 100 = 51.50 %\n",
      "\n",
      "epoch  2 , iteration  6000 \n",
      " current average loss:  10.0222844368\n",
      "epoch  2 , iteration  7000 \n",
      " current average loss:  25.2389047178\n",
      "epoch  2 , iteration  8000 \n",
      " current average loss:  7.19934232129\n",
      "epoch  2 , iteration  9000 \n",
      " current average loss:  5.45551838832\n",
      "epoch  2 , iteration  10000 \n",
      " current average loss:  4.23151566867\n",
      "evaluating..\n",
      "  Labeled   attachment score: 60 / 167 * 100 = 35.93 %\n",
      "  Unlabeled attachment score: 77 / 167 * 100 = 46.11 %\n",
      "  Label accuracy score:       89 / 167 * 100 = 53.29 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-2-10000\n",
      "epoch  2 , iteration  11000 \n",
      " current average loss:  14.4171549231\n",
      "epoch  2 , iteration  12000 \n",
      " current average loss:  12.6438055778\n",
      "epoch  2 , iteration  13000 \n",
      " current average loss:  3.70284758012\n",
      "epoch  2 , iteration  14000 \n",
      " current average loss:  17.3221682239\n",
      "epoch  2 , iteration  15000 \n",
      " current average loss:  8.63125509345\n",
      "evaluating..\n",
      "  Labeled   attachment score: 63 / 167 * 100 = 37.72 %\n",
      "  Unlabeled attachment score: 78 / 167 * 100 = 46.71 %\n",
      "  Label accuracy score:       90 / 167 * 100 = 53.89 %\n",
      "\n",
      "epoch  2 , iteration  16000 \n",
      " current average loss:  9.34677931242\n",
      "epoch  2 , iteration  17000 \n",
      " current average loss:  4.49987276764\n",
      "epoch  2 , iteration  18000 \n",
      " current average loss:  2.40484007969\n",
      "epoch  2 , iteration  19000 \n",
      " current average loss:  20.383933701\n",
      "epoch  2 , iteration  20000 \n",
      " current average loss:  1.56868248236\n",
      "evaluating..\n",
      "  Labeled   attachment score: 62 / 167 * 100 = 37.13 %\n",
      "  Unlabeled attachment score: 80 / 167 * 100 = 47.90 %\n",
      "  Label accuracy score:       90 / 167 * 100 = 53.89 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-2-20000\n",
      "epoch  2 , iteration  21000 \n",
      " current average loss:  13.5776816701\n",
      "epoch  2 , iteration  22000 \n",
      " current average loss:  5.67838734911\n",
      "epoch  2 , iteration  23000 \n",
      " current average loss:  12.8698348872\n",
      "epoch  2 , iteration  24000 \n",
      " current average loss:  13.5533480227\n",
      "epoch  2 , iteration  25000 \n",
      " current average loss:  6.97632577867\n",
      "evaluating..\n",
      "  Labeled   attachment score: 66 / 167 * 100 = 39.52 %\n",
      "  Unlabeled attachment score: 80 / 167 * 100 = 47.90 %\n",
      "  Label accuracy score:       93 / 167 * 100 = 55.69 %\n",
      "\n",
      "epoch  2 , iteration  26000 \n",
      " current average loss:  9.50462408056\n",
      "epoch  2 , iteration  27000 \n",
      " current average loss:  12.101733534\n",
      "epoch  2 , iteration  28000 \n",
      " current average loss:  11.6645915885\n",
      "epoch  2 , iteration  29000 \n",
      " current average loss:  12.790881619\n",
      "epoch  2 , iteration  30000 \n",
      " current average loss:  6.3319641273\n",
      "evaluating..\n",
      "  Labeled   attachment score: 67 / 167 * 100 = 40.12 %\n",
      "  Unlabeled attachment score: 84 / 167 * 100 = 50.30 %\n",
      "  Label accuracy score:       91 / 167 * 100 = 54.49 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-2-30000\n",
      "epoch  2 , iteration  31000 \n",
      " current average loss:  7.46069639013\n",
      "epoch  2 , iteration  32000 \n",
      " current average loss:  12.3798678409\n",
      "epoch  2 , iteration  33000 \n",
      " current average loss:  6.4580825301\n",
      "epoch  2 , iteration  34000 \n",
      " current average loss:  9.77023699077\n",
      "epoch  2 , iteration  35000 \n",
      " current average loss:  12.3665740194\n",
      "evaluating..\n",
      "  Labeled   attachment score: 68 / 167 * 100 = 40.72 %\n",
      "  Unlabeled attachment score: 83 / 167 * 100 = 49.70 %\n",
      "  Label accuracy score:       94 / 167 * 100 = 56.29 %\n",
      "\n",
      "epoch  2 , iteration  36000 \n",
      " current average loss:  5.04271075529\n",
      "epoch  2 , iteration  37000 \n",
      " current average loss:  12.9998258127\n",
      "epoch  2 , iteration  38000 \n",
      " current average loss:  6.97450165935\n",
      "epoch  2 , iteration  39000 \n",
      " current average loss:  12.1517878565\n",
      "saving model at end of epoch..\n",
      "epoch  3 , iteration  0 \n",
      " current average loss:  9.08803930064\n",
      "evaluating..\n",
      "  Labeled   attachment score: 67 / 167 * 100 = 40.12 %\n",
      "  Unlabeled attachment score: 83 / 167 * 100 = 49.70 %\n",
      "  Label accuracy score:       95 / 167 * 100 = 56.89 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-3-0\n",
      "epoch  3 , iteration  1000 \n",
      " current average loss:  8.1700481874\n",
      "epoch  3 , iteration  2000 \n",
      " current average loss:  5.73373119644\n",
      "epoch  3 , iteration  3000 \n",
      " current average loss:  8.69620631508\n",
      "epoch  3 , iteration  4000 \n",
      " current average loss:  7.22232173934\n",
      "epoch  3 , iteration  5000 \n",
      " current average loss:  4.7821894078\n",
      "evaluating..\n",
      "  Labeled   attachment score: 66 / 167 * 100 = 39.52 %\n",
      "  Unlabeled attachment score: 82 / 167 * 100 = 49.10 %\n",
      "  Label accuracy score:       94 / 167 * 100 = 56.29 %\n",
      "\n",
      "epoch  3 , iteration  6000 \n",
      " current average loss:  7.02615745714\n",
      "epoch  3 , iteration  7000 \n",
      " current average loss:  17.0144836049\n",
      "epoch  3 , iteration  8000 \n",
      " current average loss:  5.82131214668\n",
      "epoch  3 , iteration  9000 \n",
      " current average loss:  5.32283033064\n",
      "epoch  3 , iteration  10000 \n",
      " current average loss:  3.2108239976\n",
      "evaluating..\n",
      "  Labeled   attachment score: 68 / 167 * 100 = 40.72 %\n",
      "  Unlabeled attachment score: 82 / 167 * 100 = 49.10 %\n",
      "  Label accuracy score:       91 / 167 * 100 = 54.49 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-3-10000\n",
      "epoch  3 , iteration  11000 \n",
      " current average loss:  10.4985029199\n",
      "epoch  3 , iteration  12000 \n",
      " current average loss:  8.69039420924\n",
      "epoch  3 , iteration  13000 \n",
      " current average loss:  1.93906253101\n",
      "epoch  3 , iteration  14000 \n",
      " current average loss:  13.6983967652\n",
      "epoch  3 , iteration  15000 \n",
      " current average loss:  6.55191374239\n",
      "evaluating..\n",
      "  Labeled   attachment score: 70 / 167 * 100 = 41.92 %\n",
      "  Unlabeled attachment score: 83 / 167 * 100 = 49.70 %\n",
      "  Label accuracy score:       98 / 167 * 100 = 58.68 %\n",
      "\n",
      "epoch  3 , iteration  16000 \n",
      " current average loss:  5.10894450485\n",
      "epoch  3 , iteration  17000 \n",
      " current average loss:  2.98485371197\n",
      "epoch  3 , iteration  18000 \n",
      " current average loss:  1.42080431819\n",
      "epoch  3 , iteration  19000 \n",
      " current average loss:  16.0170545713\n",
      "epoch  3 , iteration  20000 \n",
      " current average loss:  0.483253294988\n",
      "evaluating..\n",
      "  Labeled   attachment score: 64 / 167 * 100 = 38.32 %\n",
      "  Unlabeled attachment score: 81 / 167 * 100 = 48.50 %\n",
      "  Label accuracy score:       90 / 167 * 100 = 53.89 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-3-20000\n",
      "epoch  3 , iteration  21000 \n",
      " current average loss:  11.2139035132\n",
      "epoch  3 , iteration  22000 \n",
      " current average loss:  3.68325644807\n",
      "epoch  3 , iteration  23000 \n",
      " current average loss:  9.55064671488\n",
      "epoch  3 , iteration  24000 \n",
      " current average loss:  8.4295319684\n",
      "epoch  3 , iteration  25000 \n",
      " current average loss:  4.92965002594\n",
      "evaluating..\n",
      "  Labeled   attachment score: 67 / 167 * 100 = 40.12 %\n",
      "  Unlabeled attachment score: 83 / 167 * 100 = 49.70 %\n",
      "  Label accuracy score:       89 / 167 * 100 = 53.29 %\n",
      "\n",
      "epoch  3 , iteration  26000 \n",
      " current average loss:  7.06077271548\n",
      "epoch  3 , iteration  27000 \n",
      " current average loss:  9.67546255552\n",
      "epoch  3 , iteration  28000 \n",
      " current average loss:  9.66436028482\n",
      "epoch  3 , iteration  29000 \n",
      " current average loss:  9.05703118751\n",
      "epoch  3 , iteration  30000 \n",
      " current average loss:  4.70413260881\n",
      "evaluating..\n",
      "  Labeled   attachment score: 63 / 167 * 100 = 37.72 %\n",
      "  Unlabeled attachment score: 79 / 167 * 100 = 47.31 %\n",
      "  Label accuracy score:       88 / 167 * 100 = 52.69 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-3-30000\n",
      "epoch  3 , iteration  31000 \n",
      " current average loss:  5.42420682138\n",
      "epoch  3 , iteration  32000 \n",
      " current average loss:  10.2566967331\n",
      "epoch  3 , iteration  33000 \n",
      " current average loss:  4.6884164545\n",
      "epoch  3 , iteration  34000 \n",
      " current average loss:  8.50004521234\n",
      "epoch  3 , iteration  35000 \n",
      " current average loss:  9.65266338493\n",
      "evaluating..\n",
      "  Labeled   attachment score: 65 / 167 * 100 = 38.92 %\n",
      "  Unlabeled attachment score: 80 / 167 * 100 = 47.90 %\n",
      "  Label accuracy score:       89 / 167 * 100 = 53.29 %\n",
      "\n",
      "epoch  3 , iteration  36000 \n",
      " current average loss:  3.98009676034\n",
      "epoch  3 , iteration  37000 \n",
      " current average loss:  9.79246360868\n",
      "epoch  3 , iteration  38000 \n",
      " current average loss:  4.87184031335\n",
      "epoch  3 , iteration  39000 \n",
      " current average loss:  9.08280411176\n",
      "saving model at end of epoch..\n",
      "epoch  4 , iteration  0 \n",
      " current average loss:  7.41111468689\n",
      "evaluating..\n",
      "  Labeled   attachment score: 63 / 167 * 100 = 37.72 %\n",
      "  Unlabeled attachment score: 74 / 167 * 100 = 44.31 %\n",
      "  Label accuracy score:       88 / 167 * 100 = 52.69 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-4-0\n",
      "epoch  4 , iteration  1000 \n",
      " current average loss:  6.63918139372\n",
      "epoch  4 , iteration  2000 \n",
      " current average loss:  4.02565549425\n",
      "epoch  4 , iteration  3000 \n",
      " current average loss:  6.81783427388\n",
      "epoch  4 , iteration  4000 \n",
      " current average loss:  5.25837044704\n",
      "epoch  4 , iteration  5000 \n",
      " current average loss:  4.0877597096\n",
      "evaluating..\n",
      "  Labeled   attachment score: 61 / 167 * 100 = 36.53 %\n",
      "  Unlabeled attachment score: 72 / 167 * 100 = 43.11 %\n",
      "  Label accuracy score:       88 / 167 * 100 = 52.69 %\n",
      "\n",
      "epoch  4 , iteration  6000 \n",
      " current average loss:  5.03419220198\n",
      "epoch  4 , iteration  7000 \n",
      " current average loss:  13.2606531309\n",
      "epoch  4 , iteration  8000 \n",
      " current average loss:  4.95290252475\n",
      "epoch  4 , iteration  9000 \n",
      " current average loss:  5.3667094514\n",
      "epoch  4 , iteration  10000 \n",
      " current average loss:  2.82071702177\n",
      "evaluating..\n",
      "  Labeled   attachment score: 66 / 167 * 100 = 39.52 %\n",
      "  Unlabeled attachment score: 80 / 167 * 100 = 47.90 %\n",
      "  Label accuracy score:       95 / 167 * 100 = 56.89 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-4-10000\n",
      "epoch  4 , iteration  11000 \n",
      " current average loss:  7.69422526821\n",
      "epoch  4 , iteration  12000 \n",
      " current average loss:  6.7625231883\n",
      "epoch  4 , iteration  13000 \n",
      " current average loss:  1.26313043179\n",
      "epoch  4 , iteration  14000 \n",
      " current average loss:  11.1968633371\n",
      "epoch  4 , iteration  15000 \n",
      " current average loss:  5.05488370993\n",
      "evaluating..\n",
      "  Labeled   attachment score: 69 / 167 * 100 = 41.32 %\n",
      "  Unlabeled attachment score: 87 / 167 * 100 = 52.10 %\n",
      "  Label accuracy score:       96 / 167 * 100 = 57.49 %\n",
      "\n",
      "epoch  4 , iteration  16000 \n",
      " current average loss:  3.28484158031\n",
      "epoch  4 , iteration  17000 \n",
      " current average loss:  1.95531105855\n",
      "epoch  4 , iteration  18000 \n",
      " current average loss:  1.30468321091\n",
      "epoch  4 , iteration  19000 \n",
      " current average loss:  12.7096401766\n",
      "epoch  4 , iteration  20000 \n",
      " current average loss:  0.406285055427\n",
      "evaluating..\n",
      "  Labeled   attachment score: 66 / 167 * 100 = 39.52 %\n",
      "  Unlabeled attachment score: 80 / 167 * 100 = 47.90 %\n",
      "  Label accuracy score:       92 / 167 * 100 = 55.09 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-4-20000\n",
      "epoch  4 , iteration  21000 \n",
      " current average loss:  9.0599378615\n",
      "epoch  4 , iteration  22000 \n",
      " current average loss:  2.78361346179\n",
      "epoch  4 , iteration  23000 \n",
      " current average loss:  7.35784763792\n",
      "epoch  4 , iteration  24000 \n",
      " current average loss:  6.14254816989\n",
      "epoch  4 , iteration  25000 \n",
      " current average loss:  3.50597418139\n",
      "evaluating..\n",
      "  Labeled   attachment score: 68 / 167 * 100 = 40.72 %\n",
      "  Unlabeled attachment score: 80 / 167 * 100 = 47.90 %\n",
      "  Label accuracy score:       94 / 167 * 100 = 56.29 %\n",
      "\n",
      "epoch  4 , iteration  26000 \n",
      " current average loss:  5.57978728037\n",
      "epoch  4 , iteration  27000 \n",
      " current average loss:  8.16674749906\n",
      "epoch  4 , iteration  28000 \n",
      " current average loss:  7.63236928112\n",
      "epoch  4 , iteration  29000 \n",
      " current average loss:  6.26775042217\n",
      "epoch  4 , iteration  30000 \n",
      " current average loss:  4.12815930956\n",
      "evaluating..\n",
      "  Labeled   attachment score: 68 / 167 * 100 = 40.72 %\n",
      "  Unlabeled attachment score: 80 / 167 * 100 = 47.90 %\n",
      "  Label accuracy score:       95 / 167 * 100 = 56.89 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-4-30000\n",
      "epoch  4 , iteration  31000 \n",
      " current average loss:  3.96793840345\n",
      "epoch  4 , iteration  32000 \n",
      " current average loss:  8.13158185853\n",
      "epoch  4 , iteration  33000 \n",
      " current average loss:  3.52141911237\n",
      "epoch  4 , iteration  34000 \n",
      " current average loss:  6.87607993479\n",
      "epoch  4 , iteration  35000 \n",
      " current average loss:  8.13743664151\n",
      "evaluating..\n",
      "  Labeled   attachment score: 65 / 167 * 100 = 38.92 %\n",
      "  Unlabeled attachment score: 82 / 167 * 100 = 49.10 %\n",
      "  Label accuracy score:       88 / 167 * 100 = 52.69 %\n",
      "\n",
      "epoch  4 , iteration  36000 \n",
      " current average loss:  3.14551692929\n",
      "epoch  4 , iteration  37000 \n",
      " current average loss:  7.42410870097\n",
      "epoch  4 , iteration  38000 \n",
      " current average loss:  3.81575772423\n",
      "epoch  4 , iteration  39000 \n",
      " current average loss:  7.26581005282\n",
      "saving model at end of epoch..\n",
      "epoch  5 , iteration  0 \n",
      " current average loss:  6.03827282786\n",
      "evaluating..\n",
      "  Labeled   attachment score: 63 / 167 * 100 = 37.72 %\n",
      "  Unlabeled attachment score: 80 / 167 * 100 = 47.90 %\n",
      "  Label accuracy score:       91 / 167 * 100 = 54.49 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-5-0\n",
      "epoch  5 , iteration  1000 \n",
      " current average loss:  5.58496176645\n",
      "epoch  5 , iteration  2000 \n",
      " current average loss:  3.36095880545\n",
      "epoch  5 , iteration  3000 \n",
      " current average loss:  6.23865933225\n",
      "epoch  5 , iteration  4000 \n",
      " current average loss:  3.62383578056\n",
      "epoch  5 , iteration  5000 \n",
      " current average loss:  3.6941525582\n",
      "evaluating..\n",
      "  Labeled   attachment score: 75 / 167 * 100 = 44.91 %\n",
      "  Unlabeled attachment score: 89 / 167 * 100 = 53.29 %\n",
      "  Label accuracy score:       96 / 167 * 100 = 57.49 %\n",
      "\n",
      "epoch  5 , iteration  6000 \n",
      " current average loss:  4.15915336732\n",
      "epoch  5 , iteration  7000 \n",
      " current average loss:  10.6300700877\n",
      "epoch  5 , iteration  8000 \n",
      " current average loss:  4.39383473687\n",
      "epoch  5 , iteration  9000 \n",
      " current average loss:  4.78351929007\n",
      "epoch  5 , iteration  10000 \n",
      " current average loss:  2.41621405592\n",
      "evaluating..\n",
      "  Labeled   attachment score: 73 / 167 * 100 = 43.71 %\n",
      "  Unlabeled attachment score: 91 / 167 * 100 = 54.49 %\n",
      "  Label accuracy score:       95 / 167 * 100 = 56.89 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-5-10000\n",
      "epoch  5 , iteration  11000 \n",
      " current average loss:  6.10140041648\n",
      "epoch  5 , iteration  12000 \n",
      " current average loss:  5.17376735506\n",
      "epoch  5 , iteration  13000 \n",
      " current average loss:  0.947980159723\n",
      "epoch  5 , iteration  14000 \n",
      " current average loss:  9.13463968971\n",
      "epoch  5 , iteration  15000 \n",
      " current average loss:  3.99767484114\n",
      "evaluating..\n",
      "  Labeled   attachment score: 75 / 167 * 100 = 44.91 %\n",
      "  Unlabeled attachment score: 93 / 167 * 100 = 55.69 %\n",
      "  Label accuracy score:       96 / 167 * 100 = 57.49 %\n",
      "\n",
      "epoch  5 , iteration  16000 \n",
      " current average loss:  2.3429776162\n",
      "epoch  5 , iteration  17000 \n",
      " current average loss:  1.11687031743\n",
      "epoch  5 , iteration  18000 \n",
      " current average loss:  1.18823592243\n",
      "epoch  5 , iteration  19000 \n",
      " current average loss:  9.95929167663\n",
      "epoch  5 , iteration  20000 \n",
      " current average loss:  0.394256933623\n",
      "evaluating..\n",
      "  Labeled   attachment score: 76 / 167 * 100 = 45.51 %\n",
      "  Unlabeled attachment score: 93 / 167 * 100 = 55.69 %\n",
      "  Label accuracy score:       97 / 167 * 100 = 58.08 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-5-20000\n",
      "epoch  5 , iteration  21000 \n",
      " current average loss:  7.69344087319\n",
      "epoch  5 , iteration  22000 \n",
      " current average loss:  2.10066649348\n",
      "epoch  5 , iteration  23000 \n",
      " current average loss:  5.58269645171\n",
      "epoch  5 , iteration  24000 \n",
      " current average loss:  5.60173565971\n",
      "epoch  5 , iteration  25000 \n",
      " current average loss:  2.4324546245\n",
      "evaluating..\n",
      "  Labeled   attachment score: 75 / 167 * 100 = 44.91 %\n",
      "  Unlabeled attachment score: 95 / 167 * 100 = 56.89 %\n",
      "  Label accuracy score:       97 / 167 * 100 = 58.08 %\n",
      "\n",
      "epoch  5 , iteration  26000 \n",
      " current average loss:  4.47740987805\n",
      "epoch  5 , iteration  27000 \n",
      " current average loss:  6.92866231717\n",
      "epoch  5 , iteration  28000 \n",
      " current average loss:  6.06936358938\n",
      "epoch  5 , iteration  29000 \n",
      " current average loss:  4.9590848856\n",
      "epoch  5 , iteration  30000 \n",
      " current average loss:  3.73169950205\n",
      "evaluating..\n",
      "  Labeled   attachment score: 71 / 167 * 100 = 42.51 %\n",
      "  Unlabeled attachment score: 91 / 167 * 100 = 54.49 %\n",
      "  Label accuracy score:       94 / 167 * 100 = 56.29 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-5-30000\n",
      "epoch  5 , iteration  31000 \n",
      " current average loss:  3.26019236804\n",
      "epoch  5 , iteration  32000 \n",
      " current average loss:  6.44853014626\n",
      "epoch  5 , iteration  33000 \n",
      " current average loss:  2.99724938545\n",
      "epoch  5 , iteration  34000 \n",
      " current average loss:  5.90391171458\n",
      "epoch  5 , iteration  35000 \n",
      " current average loss:  6.6023064004\n",
      "evaluating..\n",
      "  Labeled   attachment score: 70 / 167 * 100 = 41.92 %\n",
      "  Unlabeled attachment score: 93 / 167 * 100 = 55.69 %\n",
      "  Label accuracy score:       94 / 167 * 100 = 56.29 %\n",
      "\n",
      "epoch  5 , iteration  36000 \n",
      " current average loss:  2.54048775643\n",
      "epoch  5 , iteration  37000 \n",
      " current average loss:  5.46084366488\n",
      "epoch  5 , iteration  38000 \n",
      " current average loss:  2.72024139385\n",
      "epoch  5 , iteration  39000 \n",
      " current average loss:  6.11404227736\n",
      "saving model at end of epoch..\n",
      "epoch  6 , iteration  0 \n",
      " current average loss:  4.70253566973\n",
      "evaluating..\n",
      "  Labeled   attachment score: 71 / 167 * 100 = 42.51 %\n",
      "  Unlabeled attachment score: 94 / 167 * 100 = 56.29 %\n",
      "  Label accuracy score:       94 / 167 * 100 = 56.29 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-6-0\n",
      "epoch  6 , iteration  1000 \n",
      " current average loss:  5.02402746599\n",
      "epoch  6 , iteration  2000 \n",
      " current average loss:  3.85312514651\n",
      "epoch  6 , iteration  3000 \n",
      " current average loss:  5.74254119253\n",
      "epoch  6 , iteration  4000 \n",
      " current average loss:  2.47315208921\n",
      "epoch  6 , iteration  5000 \n",
      " current average loss:  3.43323519316\n",
      "evaluating..\n",
      "  Labeled   attachment score: 68 / 167 * 100 = 40.72 %\n",
      "  Unlabeled attachment score: 88 / 167 * 100 = 52.69 %\n",
      "  Label accuracy score:       93 / 167 * 100 = 55.69 %\n",
      "\n",
      "epoch  6 , iteration  6000 \n",
      " current average loss:  3.48013855626\n",
      "epoch  6 , iteration  7000 \n",
      " current average loss:  9.07817399612\n",
      "epoch  6 , iteration  8000 \n",
      " current average loss:  3.65813239797\n",
      "epoch  6 , iteration  9000 \n",
      " current average loss:  4.36403130377\n",
      "epoch  6 , iteration  10000 \n",
      " current average loss:  1.88479723291\n",
      "evaluating..\n",
      "  Labeled   attachment score: 72 / 167 * 100 = 43.11 %\n",
      "  Unlabeled attachment score: 92 / 167 * 100 = 55.09 %\n",
      "  Label accuracy score:       98 / 167 * 100 = 58.68 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-6-10000\n",
      "epoch  6 , iteration  11000 \n",
      " current average loss:  4.91047689679\n",
      "epoch  6 , iteration  12000 \n",
      " current average loss:  4.11122985971\n",
      "epoch  6 , iteration  13000 \n",
      " current average loss:  0.529430214526\n",
      "epoch  6 , iteration  14000 \n",
      " current average loss:  7.49480351556\n",
      "epoch  6 , iteration  15000 \n",
      " current average loss:  3.19772043947\n",
      "evaluating..\n",
      "  Labeled   attachment score: 69 / 167 * 100 = 41.32 %\n",
      "  Unlabeled attachment score: 91 / 167 * 100 = 54.49 %\n",
      "  Label accuracy score:       93 / 167 * 100 = 55.69 %\n",
      "\n",
      "epoch  6 , iteration  16000 \n",
      " current average loss:  1.81226626262\n",
      "epoch  6 , iteration  17000 \n",
      " current average loss:  0.968516547404\n",
      "epoch  6 , iteration  18000 \n",
      " current average loss:  1.13150557746\n",
      "epoch  6 , iteration  19000 \n",
      " current average loss:  7.48720656601\n",
      "epoch  6 , iteration  20000 \n",
      " current average loss:  0.312570895545\n",
      "evaluating..\n",
      "  Labeled   attachment score: 71 / 167 * 100 = 42.51 %\n",
      "  Unlabeled attachment score: 96 / 167 * 100 = 57.49 %\n",
      "  Label accuracy score:       96 / 167 * 100 = 57.49 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-6-20000\n",
      "epoch  6 , iteration  21000 \n",
      " current average loss:  6.36188751636\n",
      "epoch  6 , iteration  22000 \n",
      " current average loss:  1.81273969157\n",
      "epoch  6 , iteration  23000 \n",
      " current average loss:  4.28341277269\n",
      "epoch  6 , iteration  24000 \n",
      " current average loss:  5.27036283088\n",
      "epoch  6 , iteration  25000 \n",
      " current average loss:  1.70345948675\n",
      "evaluating..\n",
      "  Labeled   attachment score: 71 / 167 * 100 = 42.51 %\n",
      "  Unlabeled attachment score: 94 / 167 * 100 = 56.29 %\n",
      "  Label accuracy score:       97 / 167 * 100 = 58.08 %\n",
      "\n",
      "epoch  6 , iteration  26000 \n",
      " current average loss:  3.5364468171\n",
      "epoch  6 , iteration  27000 \n",
      " current average loss:  5.66114714087\n",
      "epoch  6 , iteration  28000 \n",
      " current average loss:  4.96934599594\n",
      "epoch  6 , iteration  29000 \n",
      " current average loss:  4.04359910672\n",
      "epoch  6 , iteration  30000 \n",
      " current average loss:  3.18933026406\n",
      "evaluating..\n",
      "  Labeled   attachment score: 73 / 167 * 100 = 43.71 %\n",
      "  Unlabeled attachment score: 98 / 167 * 100 = 58.68 %\n",
      "  Label accuracy score:       95 / 167 * 100 = 56.89 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-6-30000\n",
      "epoch  6 , iteration  31000 \n",
      " current average loss:  2.61991268817\n",
      "epoch  6 , iteration  32000 \n",
      " current average loss:  5.09877474328\n",
      "epoch  6 , iteration  33000 \n",
      " current average loss:  2.45290355989\n",
      "epoch  6 , iteration  34000 \n",
      " current average loss:  4.46248459896\n",
      "epoch  6 , iteration  35000 \n",
      " current average loss:  5.26296735069\n",
      "evaluating..\n",
      "  Labeled   attachment score: 69 / 167 * 100 = 41.32 %\n",
      "  Unlabeled attachment score: 93 / 167 * 100 = 55.69 %\n",
      "  Label accuracy score:       91 / 167 * 100 = 54.49 %\n",
      "\n",
      "epoch  6 , iteration  36000 \n",
      " current average loss:  1.98250863105\n",
      "epoch  6 , iteration  37000 \n",
      " current average loss:  4.10299589641\n",
      "epoch  6 , iteration  38000 \n",
      " current average loss:  2.1971397555\n",
      "epoch  6 , iteration  39000 \n",
      " current average loss:  5.02159523554\n",
      "saving model at end of epoch..\n",
      "epoch  7 , iteration  0 \n",
      " current average loss:  3.79293902798\n",
      "evaluating..\n",
      "  Labeled   attachment score: 70 / 167 * 100 = 41.92 %\n",
      "  Unlabeled attachment score: 95 / 167 * 100 = 56.89 %\n",
      "  Label accuracy score:       89 / 167 * 100 = 53.29 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-7-0\n",
      "epoch  7 , iteration  1000 \n",
      " current average loss:  4.27791123364\n",
      "epoch  7 , iteration  2000 \n",
      " current average loss:  3.31265721412\n",
      "epoch  7 , iteration  3000 \n",
      " current average loss:  5.8616748097\n",
      "epoch  7 , iteration  4000 \n",
      " current average loss:  2.0773093087\n",
      "epoch  7 , iteration  5000 \n",
      " current average loss:  2.97344992703\n",
      "evaluating..\n",
      "  Labeled   attachment score: 70 / 167 * 100 = 41.92 %\n",
      "  Unlabeled attachment score: 90 / 167 * 100 = 53.89 %\n",
      "  Label accuracy score:       96 / 167 * 100 = 57.49 %\n",
      "\n",
      "epoch  7 , iteration  6000 \n",
      " current average loss:  3.01758925504\n",
      "epoch  7 , iteration  7000 \n",
      " current average loss:  7.97615544865\n",
      "epoch  7 , iteration  8000 \n",
      " current average loss:  3.10083586502\n",
      "epoch  7 , iteration  9000 \n",
      " current average loss:  4.3612875372\n",
      "epoch  7 , iteration  10000 \n",
      " current average loss:  1.34681834081\n",
      "evaluating..\n",
      "  Labeled   attachment score: 66 / 167 * 100 = 39.52 %\n",
      "  Unlabeled attachment score: 86 / 167 * 100 = 51.50 %\n",
      "  Label accuracy score:       91 / 167 * 100 = 54.49 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-7-10000\n",
      "epoch  7 , iteration  11000 \n",
      " current average loss:  3.91127022742\n",
      "epoch  7 , iteration  12000 \n",
      " current average loss:  3.29335073693\n",
      "epoch  7 , iteration  13000 \n",
      " current average loss:  0.37880630961\n",
      "epoch  7 , iteration  14000 \n",
      " current average loss:  5.67992952893\n",
      "epoch  7 , iteration  15000 \n",
      " current average loss:  2.78673065961\n",
      "evaluating..\n",
      "  Labeled   attachment score: 72 / 167 * 100 = 43.11 %\n",
      "  Unlabeled attachment score: 90 / 167 * 100 = 53.89 %\n",
      "  Label accuracy score:       96 / 167 * 100 = 57.49 %\n",
      "\n",
      "epoch  7 , iteration  16000 \n",
      " current average loss:  1.63684632624\n",
      "epoch  7 , iteration  17000 \n",
      " current average loss:  0.858663453008\n",
      "epoch  7 , iteration  18000 \n",
      " current average loss:  0.965671336871\n",
      "epoch  7 , iteration  19000 \n",
      " current average loss:  5.82338752035\n",
      "epoch  7 , iteration  20000 \n",
      " current average loss:  0.328818578328\n",
      "evaluating..\n",
      "  Labeled   attachment score: 69 / 167 * 100 = 41.32 %\n",
      "  Unlabeled attachment score: 87 / 167 * 100 = 52.10 %\n",
      "  Label accuracy score:       91 / 167 * 100 = 54.49 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-7-20000\n",
      "epoch  7 , iteration  21000 \n",
      " current average loss:  5.37308094176\n",
      "epoch  7 , iteration  22000 \n",
      " current average loss:  1.47886128229\n",
      "epoch  7 , iteration  23000 \n",
      " current average loss:  3.39893956169\n",
      "epoch  7 , iteration  24000 \n",
      " current average loss:  5.03414001148\n",
      "epoch  7 , iteration  25000 \n",
      " current average loss:  1.28681607316\n",
      "evaluating..\n",
      "  Labeled   attachment score: 69 / 167 * 100 = 41.32 %\n",
      "  Unlabeled attachment score: 88 / 167 * 100 = 52.69 %\n",
      "  Label accuracy score:       91 / 167 * 100 = 54.49 %\n",
      "\n",
      "epoch  7 , iteration  26000 \n",
      " current average loss:  3.04141966344\n",
      "epoch  7 , iteration  27000 \n",
      " current average loss:  4.87890304577\n",
      "epoch  7 , iteration  28000 \n",
      " current average loss:  4.43464455968\n",
      "epoch  7 , iteration  29000 \n",
      " current average loss:  3.4226784973\n",
      "epoch  7 , iteration  30000 \n",
      " current average loss:  2.59489330045\n",
      "evaluating..\n",
      "  Labeled   attachment score: 67 / 167 * 100 = 40.12 %\n",
      "  Unlabeled attachment score: 85 / 167 * 100 = 50.90 %\n",
      "  Label accuracy score:       90 / 167 * 100 = 53.89 %\n",
      "\n",
      "saving model with name tf_models_nolstm/model-7-30000\n",
      "epoch  7 , iteration  31000 \n",
      " current average loss:  1.74889365284\n",
      "epoch  7 , iteration  32000 \n",
      " current average loss:  4.28726103547\n",
      "epoch  7 , iteration  33000 \n",
      " current average loss:  2.22001967439\n",
      "epoch  7 , iteration  34000 \n",
      " current average loss:  3.72770028376\n",
      "epoch  7 , iteration  35000 \n",
      " current average loss:  4.36162978273\n",
      "evaluating..\n",
      "  Labeled   attachment score: 62 / 167 * 100 = 37.13 %\n",
      "  Unlabeled attachment score: 79 / 167 * 100 = 47.31 %\n",
      "  Label accuracy score:       87 / 167 * 100 = 52.10 %\n",
      "\n",
      "epoch  7 , iteration  36000 \n",
      " current average loss:  1.66066200843\n",
      "epoch  7 , iteration  37000 \n",
      " current average loss:  3.35845220113\n",
      "epoch  7 , iteration  38000 \n",
      " current average loss:  2.08654499513\n",
      "epoch  7 , iteration  39000 \n",
      " current average loss:  4.28509532503\n",
      "saving model at end of epoch..\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "# TensorFlow model\n",
    "##############################\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "        \n",
    "    # hyperparameters (from Cross & Huang, 2016)\n",
    "    word2vec_length = model['a'].size\n",
    "    n_input = word2vec_length # for nolstm-model\n",
    "    n_hidden = 200\n",
    "    n_classes = 99 # there are 99 possible actions to take\n",
    "    num_epochs = 8\n",
    "    dropout = 0.5\n",
    "    L2_penalty = 0.\n",
    "    rho = 0.99\n",
    "    epsilon = 1e-07\n",
    "    learning_rate = 0.02 # default is 0.001, Cross & Huang do not specify learning rate\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h': tf.Variable(tf.random_normal([3*n_input, n_hidden], dtype=tf.float64), name='weights_h'),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], dtype=tf.float64), name='weights_out')\n",
    "    }\n",
    "    biases = {\n",
    "        'b': tf.Variable(tf.random_normal([n_hidden], dtype=tf.float64), name='biases_b'),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes], dtype=tf.float64), name='biases_out')\n",
    "    }\n",
    "    \n",
    "    # placeholders\n",
    "    sentence_length = tf.placeholder(tf.int32)\n",
    "    lstm_x = tf.placeholder(tf.float64, [1, None, word2vec_length])\n",
    "    parse_indices = tf.placeholder(tf.int64, [1, None, 3])\n",
    "    labels = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "    # directly use word2vec as output per word (so lstm_input = lstm_output)\n",
    "    output_lstm = lstm_x\n",
    "    \n",
    "    # zero-padding of LSTM-output (sentence gets a \"dummy word\" in front of it)\n",
    "    zero_padding = tf.zeros([1, 1, n_input], tf.float64)\n",
    "    output_lstm = tf.concat(1, [zero_padding, output_lstm])\n",
    "   \n",
    "    # mlp_x: make a matrix with all corresponding word vector 3-tuples, CONCATENATED (up to the no of parse steps)\n",
    "    mlp_x = tf.nn.embedding_lookup(output_lstm[0,:,:], parse_indices[0,:,:])\n",
    "    dims = tf.shape(mlp_x)\n",
    "    mlp_x = tf.reshape(mlp_x, [dims[0], dims[1]*dims[2]])\n",
    "\n",
    "    output_mlp = mlp(mlp_x, weights, biases)\n",
    "\n",
    "    cost = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(output_mlp, labels))\n",
    "    cost_indication = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(output_mlp, labels))\n",
    "\n",
    "    train_op = tf.train.AdadeltaOptimizer(rho=rho, epsilon=epsilon, learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # saver:\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        init = tf.initialize_all_variables()\n",
    "        session.run(init)\n",
    "        \n",
    "        for epoch in range(0,num_epochs): \n",
    "            for i in range(0,len(sentence_embeddings)): \n",
    "                \n",
    "                # get sentence embedding\n",
    "                sentence = sentence_embeddings[i]\n",
    "                \n",
    "                # get parse data for sentence\n",
    "                parse_data = action_data[i]\n",
    "                \n",
    "                indices = parse_data[:,:3]\n",
    "                actions = parse_data[:,3]\n",
    "                \n",
    "                # important variables\n",
    "                seq_length = len(sentence)\n",
    "                                \n",
    "                feed_dict_batch = {sentence_length: [seq_length], lstm_x: [sentence],  parse_indices: [indices], labels: actions}\n",
    "                \n",
    "                result = session.run([train_op, cost_indication], feed_dict_batch)\n",
    "                \n",
    "                if i%1000 == 0:\n",
    "                    print(\"epoch \", epoch, \", iteration \", i, \"\\n\", \"current average loss: \", result[1])\n",
    "                \n",
    "                if i%5000 == 0:\n",
    "                    print(\"evaluating..\")\n",
    "                    sentences_to_conll(train_sentences[:10], arc_dict, \"dev_pred\")\n",
    "                    val_output = subprocess.check_output([\"perl\", \"eval.pl\", \"-g\", \"dev_true.conll\", \"-s\", \"dev_pred.conll\", \"-q\"])\n",
    "                    print(val_output.decode(\"utf-8\"))\n",
    "                \n",
    "                if i%10000 == 0:\n",
    "                    name = \"tf_models_nolstm/model-\" + str(epoch) + \"-\" + str(i)\n",
    "                    print(\"saving model with name \" + name)\n",
    "                    saver.save(session, name)\n",
    "                \n",
    "                if i == len(sentence_embeddings)-1:\n",
    "                    name = \"tf_models_nolstm/model-\" + str(epoch)\n",
    "                    print(\"saving model at end of epoch..\")\n",
    "                    saver.save(session, name)\n",
    "                    \n",
    "        print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_h:0\n",
      "weights_out:0\n",
      "biases_b:0\n",
      "biases_out:0\n",
      "weights_h:0\n",
      "weights_out:0\n",
      "biases_b:0\n",
      "biases_out:0\n",
      "evaluating..\n",
      "  Labeled   attachment score: 0 / 167 * 100 = 0.00 %\n",
      "  Unlabeled attachment score: 13 / 167 * 100 = 7.78 %\n",
      "  Label accuracy score:       4 / 167 * 100 = 2.40 %\n",
      "\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "# TensorFlow model\n",
    "##############################\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "        \n",
    "    # hyperparameters (from Cross & Huang, 2016)\n",
    "    word2vec_length = model['a'].size\n",
    "    n_input = word2vec_length # for nolstm-model\n",
    "    n_hidden = 200\n",
    "    n_classes = 99 # there are 99 possible actions to take\n",
    "    num_epochs = 8\n",
    "    dropout = 0.5\n",
    "    L2_penalty = 0.\n",
    "    rho = 0.99\n",
    "    epsilon = 1e-07\n",
    "    learning_rate = 0.02 # default is 0.001, Cross & Huang do not specify learning rate\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h': tf.Variable(tf.random_normal([3*n_input, n_hidden], dtype=tf.float64), name='weights_h'),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], dtype=tf.float64), name='weights_out')\n",
    "    }\n",
    "    biases = {\n",
    "        'b': tf.Variable(tf.random_normal([n_hidden], dtype=tf.float64), name='biases_b'),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes], dtype=tf.float64), name='biases_out')\n",
    "    }\n",
    "    \n",
    "    # placeholders\n",
    "    sentence_length = tf.placeholder(tf.int32)\n",
    "    lstm_x = tf.placeholder(tf.float64, [1, None, word2vec_length])\n",
    "    parse_indices = tf.placeholder(tf.int64, [1, None, 3])\n",
    "    labels = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "    # directly use word2vec as output per word (so lstm_input = lstm_output)\n",
    "    output_lstm = lstm_x\n",
    "    \n",
    "    # zero-padding of LSTM-output (sentence gets a \"dummy word\" in front of it)\n",
    "    zero_padding = tf.zeros([1, 1, n_input], tf.float64)\n",
    "    output_lstm = tf.concat(1, [zero_padding, output_lstm])\n",
    "   \n",
    "    # mlp_x: make a matrix with all corresponding word vector 3-tuples, CONCATENATED (up to the no of parse steps)\n",
    "    mlp_x = tf.nn.embedding_lookup(output_lstm[0,:,:], parse_indices[0,:,:])\n",
    "    dims = tf.shape(mlp_x)\n",
    "    mlp_x = tf.reshape(mlp_x, [dims[0], dims[1]*dims[2]])\n",
    "\n",
    "    output_mlp = mlp(mlp_x, weights, biases)\n",
    "\n",
    "    cost = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(output_mlp, labels))\n",
    "    cost_indication = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(output_mlp, labels))\n",
    "\n",
    "    train_op = tf.train.AdadeltaOptimizer(rho=rho, epsilon=epsilon, learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        init = tf.initialize_all_variables()\n",
    "        session.run(init)\n",
    "        saver = tf.train.import_meta_graph('tf_models_nolstm/model-7.meta')\n",
    "        saver.restore(session, 'tf_models_nolstm/model-7')\n",
    "        all_vars = tf.trainable_variables()\n",
    "        for v in all_vars:\n",
    "            print(v.name)\n",
    "        session.run(all_vars)\n",
    "#         print(session.run(tf.all_variables()))\n",
    "\n",
    "        print(\"evaluating..\")\n",
    "        sentences_to_conll(train_sentences[:10], arc_dict, \"dev_pred\")\n",
    "        val_output = subprocess.check_output([\"perl\", \"eval.pl\", \"-g\", \"dev_true.conll\", \"-s\", \"dev_pred.conll\", \"-q\"])\n",
    "        print(val_output.decode(\"utf-8\"))\n",
    "\n",
    "\n",
    "    print(\"DONE\")\n",
    "    # saver:\n",
    "#     session = tf.Session()\n",
    "#     init = tf.initialize_all_variables()\n",
    "#     session.run(init)\n",
    "#     new_saver = tf.train.import_meta_graph('tf_models_nolstm/model-7.meta')\n",
    "# #     init = tf.initialize_all_variables()\n",
    "# #     session.run(init)\n",
    "#     new_saver.restore(session, tf.train.latest_checkpoint('tf_models_nolstm/'))\n",
    "#     all_vars = tf.trainable_variables()\n",
    "#     session.run(all_vars)\n",
    "#     for v in all_vars:\n",
    "#         print(v.name)\n",
    "    \n",
    "#     with tf.Session() as session:\n",
    "\n",
    "        \n",
    "#         init = tf.initialize_all_variables()\n",
    "#         session.run(init)\n",
    "        \n",
    "#         for epoch in range(0,num_epochs): \n",
    "#             for i in range(0,len(sentence_embeddings)): \n",
    "                \n",
    "#                 # get sentence embedding\n",
    "#                 sentence = sentence_embeddings[i]\n",
    "                \n",
    "#                 # get parse data for sentence\n",
    "#                 parse_data = action_data[i]\n",
    "                \n",
    "#                 indices = parse_data[:,:3]\n",
    "#                 actions = parse_data[:,3]\n",
    "                \n",
    "#                 # important variables\n",
    "#                 seq_length = len(sentence)\n",
    "                                \n",
    "#                 feed_dict_batch = {sentence_length: [seq_length], lstm_x: [sentence],  parse_indices: [indices], labels: actions}\n",
    "                \n",
    "#                 result = session.run([train_op, cost_indication], feed_dict_batch)\n",
    "                \n",
    "                \n",
    "#             if i%5000 == 0:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "# sess = tf.Session()\n",
    "# # get the model that was trained furthest. model-1-5000.meta means epoch 1, iter 5000\n",
    "# new_saver = tf.train.import_meta_graph('tf_models_nolstm/model-7.meta')\n",
    "# new_saver.restore(sess, tf.train.latest_checkpoint('tf_models_nolstm/'))\n",
    "# all_vars = tf.trainable_variables()\n",
    "# for v in all_vars:\n",
    "#     print(v.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph reset\n",
      "made new placeholders\n",
      "now initializing variables\n",
      "importing graph\n",
      "session restored\n",
      "weights_h:0\n",
      "weights_out:0\n",
      "biases_b:0\n",
      "biases_out:0\n",
      "evaluating...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    868\u001b[0m             subfeed_t = self.graph.as_graph_element(subfeed, allow_tensor=True,\n\u001b[0;32m--> 869\u001b[0;31m                                                     allow_operation=False)\n\u001b[0m\u001b[1;32m    870\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2458\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2536\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2537\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2538\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"Placeholder:0\", dtype=int32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_controller\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m   3644\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3645\u001b[0;31m       \u001b[0;32myield\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3646\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-5bcf9d88ead6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"evaluating...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0msentences_to_conll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marc_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dev_pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predictions made, checking...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c948f56c7d0c>\u001b[0m in \u001b[0;36msentences_to_conll\u001b[0;34m(sentences, arc_dict, file_name)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mbuff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0msentence_parse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingle_sentence_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_parse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mall_sentences_listed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_parse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c948f56c7d0c>\u001b[0m in \u001b[0;36msingle_sentence_parse\u001b[0;34m(stack, buff, sentence_parse, action_dict, w2v_matrix)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mShift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msingle_sentence_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_parse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c948f56c7d0c>\u001b[0m in \u001b[0;36msingle_sentence_parse\u001b[0;34m(stack, buff, sentence_parse, action_dict, w2v_matrix)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mShift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msingle_sentence_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_parse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c948f56c7d0c>\u001b[0m in \u001b[0;36msingle_sentence_parse\u001b[0;34m(stack, buff, sentence_parse, action_dict, w2v_matrix)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_action_decision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c948f56c7d0c>\u001b[0m in \u001b[0;36mmodel_action_decision\u001b[0;34m(w2v_matrix, s1, s2, b1, emptybuffer)\u001b[0m\n\u001b[1;32m     62\u001b[0m                   [np.array([[s1,s2,b1]])]} # feed_dict without labels\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0memptybuffer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    871\u001b[0m             raise TypeError('Cannot interpret feed_dict key as Tensor: '\n\u001b[0;32m--> 872\u001b[0;31m                             + e.args[0])\n\u001b[0m\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", dtype=int32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-5bcf9d88ead6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predictions made, checking...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mval_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"perl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eval.pl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-g\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dev_true.conll\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dev_pred.conll\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-q\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exec_type, exec_value, exec_tb)\u001b[0m\n\u001b[1;32m   1158\u001b[0m     self._default_session_context_manager.__exit__(\n\u001b[1;32m   1159\u001b[0m         exec_type, exec_value, exec_tb)\n\u001b[0;32m-> 1160\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexec_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexec_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexec_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_session_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't stop after throw()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_controller\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m   3646\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3647\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enforce_nesting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3648\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3649\u001b[0m           raise AssertionError(\n\u001b[1;32m   3650\u001b[0m               \u001b[0;34m\"Nesting violated for default stack of %s objects\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    tf.reset_default_graph()\n",
    "    print(\"graph reset\")\n",
    "    # placeholders\n",
    "    sentence_length = tf.placeholder(tf.int32)\n",
    "    lstm_x = tf.placeholder(tf.float64, [1, None, word2vec_length])\n",
    "    parse_indices = tf.placeholder(tf.int64, [1, None, 3])\n",
    "    labels = tf.placeholder(tf.int64, [None])\n",
    "    print(\"made new placeholders\")\n",
    "\n",
    "    # directly use word2vec as output per word (so lstm_input = lstm_output)\n",
    "    output_lstm = lstm_x\n",
    "    \n",
    "    # zero-padding of LSTM-output (sentence gets a \"dummy word\" in front of it)\n",
    "    zero_padding = tf.zeros([1, 1, n_input], tf.float64)\n",
    "    output_lstm = tf.concat(1, [zero_padding, output_lstm])\n",
    "   \n",
    "    # mlp_x: make a matrix with all corresponding word vector 3-tuples, CONCATENATED (up to the no of parse steps)\n",
    "    mlp_x = tf.nn.embedding_lookup(output_lstm[0,:,:], parse_indices[0,:,:])\n",
    "    dims = tf.shape(mlp_x)\n",
    "    mlp_x = tf.reshape(mlp_x, [dims[0], dims[1]*dims[2]])\n",
    "\n",
    "#     output_mlp = mlp(mlp_x, weights, biases)\n",
    "\n",
    "    print(\"now initializing variables\")\n",
    "    \n",
    "#     init = tf.initialize_all_variables()\n",
    "    \n",
    "#     print(\"running session with init...\")\n",
    "#     session.run(init)\n",
    "    \n",
    "    print(\"importing graph\")\n",
    "    saver = tf.train.import_meta_graph('tf_models_nolstm/model-7.meta')\n",
    "    saver.restore(session, 'tf_models_nolstm/model-7')\n",
    "    print(\"session restored\")\n",
    "    all_vars = tf.trainable_variables()\n",
    "    \n",
    "    for v in all_vars:\n",
    "        print(v.name)\n",
    "    \n",
    "    weights = {\n",
    "        'h': all_vars[0],\n",
    "        'out': all_vars[1]\n",
    "    }\n",
    "    biases = {\n",
    "        'b': all_vars[2],\n",
    "        'out': all_vars[3]\n",
    "    }\n",
    "    output_mlp = mlp(mlp_x, weights, biases)\n",
    "    \n",
    "    cost = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(output_mlp, labels))\n",
    "    cost_indication = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(output_mlp, labels))\n",
    "\n",
    "    train_op = tf.train.AdadeltaOptimizer(rho=rho, epsilon=epsilon, learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "#     session.run(all_vars)\n",
    "#         print(session.run(tf.all_variables()))\n",
    "\n",
    "    print(\"evaluating...\")\n",
    "    sentences_to_conll(train_sentences[:10], arc_dict, \"dev_pred\")\n",
    "    print(\"predictions made, checking...\")\n",
    "    val_output = subprocess.check_output([\"perl\", \"eval.pl\", \"-g\", \"dev_true.conll\", \"-s\", \"dev_pred.conll\", \"-q\"])\n",
    "    print(val_output.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
