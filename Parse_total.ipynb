{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sander/anaconda/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "# import modules\n",
    "##############################\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the file, \n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as myfile:\n",
    "        f = myfile.readlines()\n",
    "        s_num = 0\n",
    "        i =0\n",
    "        sentence_s = []\n",
    "        tag_s = []\n",
    "        dep_s = []\n",
    "        s  = []   # sentence\n",
    "        p = []    # tag\n",
    "        d = []    # dependency\n",
    "        for l in f:\n",
    "            \n",
    "            v = l.replace('\\n','').split(\"\\t\")\n",
    "            v.append(s_num)\n",
    "            if len(l) != 1:\n",
    "                data.append(v)\n",
    "                dep = v[6] + '_' + v[7]\n",
    "                word = v[1].lower()\n",
    "                if any(char.isdigit() for char in word):\n",
    "                    word = 'NUM'       # replace numbers with NUM\n",
    "                s.append(word)\n",
    "                p.append(v[3])\n",
    "                d.append(dep)\n",
    "                i +=1\n",
    "            else:\n",
    "                sentence_s.append(s)\n",
    "                tag_s.append(p)\n",
    "                dep_s.append(d)\n",
    "                s_num +=1\n",
    "                s  = []\n",
    "                p = []\n",
    "                d = []\n",
    "        \n",
    "    return data, sentence_s, tag_s, dep_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(dataname):\n",
    "    #reads in files, produces data structure with all actions\n",
    "        #does so by applying produce_rule_list to every sentence.\n",
    "        #for loop that sets actions to empty, calls p_r_l giving it\n",
    "        #the stack and buffer, actions and correct_parse, adds finished action list\n",
    "        #to new data file, for each sentence in the input data\n",
    "    #input: name of the data file with all parses. Run with data file in same directory.\n",
    "    #output: data file with all actions\n",
    "    file = open(dataname)\n",
    "    data = file.read()\n",
    "    correct_parses = correct_parse_list(data)\n",
    "    #gets rid of final whitespace\n",
    "    del correct_parses[len(correct_parses)-1]\n",
    "    \n",
    "    #iterates over all parses, producing action list for each\n",
    "    complete_rule_list = []\n",
    "    arc_dict = {'Shift':0,'L_root':1,'R_root':2}\n",
    "    for sentence_parse in correct_parses:\n",
    "        stack = []\n",
    "#         print(len(sentence_parse))\n",
    "        buff = list(range(1,len(sentence_parse)+1))\n",
    "        actions = []\n",
    "        rule_list, arc_dict = produce_rule_list(stack, buff, actions, sentence_parse, arc_dict)\n",
    "        complete_rule_list.append(np.array(rule_list))\n",
    "\n",
    "    \n",
    "    return complete_rule_list, arc_dict\n",
    "\n",
    "def correct_parse_list(data):\n",
    "    #Turns data into a list of lists of lists with relevant information\n",
    "    correct_parse = data.split(\"\\n\\n\")\n",
    "    for index, paragraph in enumerate(correct_parse):\n",
    "        correct_parse[index] = paragraph.split(\"\\n\")\n",
    "    for paragraph in correct_parse:\n",
    "        for index, line in enumerate(paragraph):\n",
    "            paragraph[index] = line.split(\"\\t\")\n",
    "    return correct_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_rule_list(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #recursive function that works through words in the sentence (stack/buffer)\n",
    "        #until only one word is left, creating the list of actions \n",
    "        #that was taken to parse it.\n",
    "    #input: stack, buffer, actions, correct parse\n",
    "    #output: actions with the actions taken for each buff/stack configuration\n",
    "    \n",
    "    #base case\n",
    "    if len(stack) == 1 and len(buff) == 0:\n",
    "        #actions.append([stack[:], \"empty\", \"R_arc\"])\n",
    "        actions.append([0,stack[0], 0, 2])\n",
    "        return actions, arc_dict\n",
    "\n",
    "    #If enough of the sentence is still left:\n",
    "    #If there is not enough material in the stack, shift:\n",
    "    if len(stack) == 0 :\n",
    "        #print('chose S - small stack')\n",
    "        actions.append([0,0,buff[0], 0])\n",
    "        stack.append(buff[0])\n",
    "        del buff[0]        \n",
    "        return produce_rule_list(stack,buff,actions,sentence_parse, arc_dict)\n",
    "    if len(stack) == 1:\n",
    "        actions.append([0,stack[-1],buff[0], 0])\n",
    "        stack.append(buff[0])\n",
    "        del buff[0]\n",
    "        return produce_rule_list(stack,buff,actions,sentence_parse, arc_dict)\n",
    "    #If there are 2 or more words in the stack, decide which action to perform and perform it\n",
    "    if len(stack) > 1:\n",
    "        action = rule_decision(stack,buff,sentence_parse)\n",
    "        stack, buff, actions, arc_dict = action(stack,buff,actions, sentence_parse, arc_dict)\n",
    "        return produce_rule_list(stack,buff,actions,sentence_parse, arc_dict)\n",
    "    \n",
    "\n",
    "def rule_decision(stack, buff, sentence_parse):\n",
    "    #determines which action to apply\n",
    "    #input: words on stack, words on buff, correct parse\n",
    "    #output: one of three methods, Shift(), L_arc(), R_arc()\n",
    "\n",
    "    #find ids/heads (index [6]) from stack and sentence_parse\n",
    "    s1 = stack[-2]\n",
    "    head_of_s1 = int(sentence_parse[s1-1][6])\n",
    "    s2 = stack[-1]\n",
    "    head_of_s2 = int(sentence_parse[s2-1][6])\n",
    "    \n",
    "    #L arcs can always be applied if possible\n",
    "    if head_of_s1 == s2:\n",
    "        action = L_arc\n",
    "        #print('chose L')\n",
    "    else:\n",
    "        #R arcs can only be applied if there is no word in the buffer which has the last word in the stack as a head\n",
    "        if head_of_s2 == s1:\n",
    "            buff_heads = [int(sentence_parse[x-1][6]) for x in buff]\n",
    "            if s2 in buff_heads:\n",
    "                action = Shift\n",
    "                #print('chose S - s2 in buffheads')\n",
    "            else:\n",
    "                action = R_arc\n",
    "                #print('chose R')\n",
    "        #if there is no match between s1 and s2, simply shift another word from the buffer\n",
    "        else:\n",
    "            action = Shift\n",
    "            #print('chose S - no matching s1s2')\n",
    "\n",
    "    return action\n",
    "\n",
    "#The following methods perform an arc or shift. These can be changed if more data is needed in the network.\n",
    "\n",
    "def L_arc(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #removes second to last item from stack, writes action to actions\n",
    "    #input: stack and actions\n",
    "    #output: new stack and actions with one L_arc line\n",
    "    #s1, s2, b1, action\n",
    "    s1 = int(stack[-2])\n",
    "    s2 = int(stack[-1])\n",
    "    if len(buff) == 0:\n",
    "        b1 = 0\n",
    "    else:\n",
    "        b1 = int(buff[0])\n",
    "    relation = \"L_\"+sentence_parse[s1-1][7]\n",
    "\n",
    "    if relation not in arc_dict:\n",
    "        maximum = max(arc_dict, key=arc_dict.get)\n",
    "        arc_dict['L_'+relation[2:]] = arc_dict[maximum]+1\n",
    "        arc_dict['R_'+relation[2:]] = arc_dict[maximum]+2\n",
    "    \n",
    "\n",
    "    actions.append([s1,s2,b1, arc_dict[relation]])\n",
    "    del stack[-2]\n",
    "    return stack, buff, actions, arc_dict\n",
    "\n",
    "\n",
    "\n",
    "def R_arc(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #removes last item from the stack, writes action to actions\n",
    "    #input: stack and actions\n",
    "    #output: new stack and actions with one R_arc line\n",
    "    #s1, s2, b1, action\n",
    "    s1 = int(stack[-2])\n",
    "    s2 = int(stack[-1])\n",
    "    if len(buff) == 0:\n",
    "        b1 = 0\n",
    "    else:\n",
    "        b1 = int(buff[0])\n",
    "        \n",
    "    relation = \"R_\"+sentence_parse[s2-1][7]\n",
    "\n",
    "    if relation not in arc_dict:\n",
    "        maximum = max(arc_dict, key=arc_dict.get)\n",
    "        arc_dict['L_'+relation[2:]] = arc_dict[maximum]+1\n",
    "        arc_dict['R_'+relation[2:]] = arc_dict[maximum]+2 \n",
    "    \n",
    "    actions.append([s1,s2,b1, arc_dict[relation]])\n",
    "    del stack[-1]\n",
    "    return stack, buff, actions, arc_dict\n",
    "\n",
    "\n",
    "\n",
    "def Shift(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #moves an item from the buff to the stack, writes action to actions\n",
    "    #input: stack, buff and actions\n",
    "    #output: new stack and actions with one extra shift line\n",
    "    #s1, s2, b1, action\n",
    "    s1 = int(stack[-2])\n",
    "    s2 = int(stack[-1])\n",
    "    b1 = int(buff[0])\n",
    "    #actions.append([stack[:], buff[:], \"Shift\"])\n",
    "    actions.append([s1,s2,b1, 0])\n",
    "    stack.append(buff[0])\n",
    "    del buff[0]\n",
    "    return stack, buff, actions, arc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  [['1', 'In', '_', 'IN', 'IN', '_', '45', 'prep', '_', '_', 0], ['2', 'an', '_', 'DT', 'DT', '_', '5', 'det', '_', '_', 0]]\n",
      "words sentences:  [['rolls-royce', 'motor', 'cars', 'inc.', 'said', 'it', 'expects', 'its', 'u.s.', 'sales', 'to', 'remain', 'steady', 'at', 'about', 'NUM', 'cars', 'in', 'NUM', '.'], ['the', 'luxury', 'auto', 'maker', 'last', 'year', 'sold', 'NUM', 'cars', 'in', 'the', 'u.s.']]\n",
      "tags sentences:  [['NNP', 'NNP', 'NNPS', 'NNP', 'VBD', 'PRP', 'VBZ', 'PRP$', 'NNP', 'NNS', 'TO', 'VB', 'JJ', 'IN', 'IN', 'CD', 'NNS', 'IN', 'CD', '.'], ['DT', 'NN', 'NN', 'NN', 'JJ', 'NN', 'VBD', 'CD', 'NNS', 'IN', 'DT', 'NNP']]\n",
      "dependencies:  [['4_nn', '4_nn', '4_nn', '5_nsubj', '0_root', '7_nsubj', '5_ccomp', '10_poss', '10_nn', '12_nsubj', '12_aux', '7_xcomp', '12_acomp', '12_prep', '16_quantmod', '17_num', '14_pobj', '12_prep', '18_pobj', '5_punct'], ['4_det', '4_nn', '4_nn', '7_nsubj', '6_amod', '7_tmod', '0_root', '9_num', '7_dobj', '7_prep', '12_det', '10_pobj']]\n"
     ]
    }
   ],
   "source": [
    "train_data, train_sentences, train_tags, train_dependencies = read_data('./data/train-stanford-raw.conll')\n",
    "dev_data, dev_sentences, dev_tags, dev_dependencies = read_data('./data/dev-stanford-raw.conll')\n",
    "test_data, test_sentences, test_tags, test_dependencies = read_data('./data/test-stanford-raw.conll')\n",
    "\n",
    "# create a full set of all the words in our train, test, and dev sets for word2vec model\n",
    "# in order to avoid unseen words during test and validation\n",
    "total_sentences = train_sentences + dev_sentences + test_sentences\n",
    "print('data: ', train_data[:2])\n",
    "print('words sentences: ', total_sentences[2:4])\n",
    "print('tags sentences: ', train_tags[2:4])\n",
    "print('dependencies: ', train_dependencies[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action_data, arc_dict = process_data('./data/train-stanford-raw.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R_npadvmod': 54, 'R_punct': 10, 'R_cop': 96, 'L_nsubj': 21, 'R_nsubjpass': 38, 'L_amod': 19, 'R_csubj': 74, 'L_det': 7, 'R_predet': 72, 'L_rel': 85, 'R_infmod': 58, 'L_dep': 29, 'L_csubjpass': 97, 'L_cop': 95, 'R_nn': 6, 'R_num': 4, 'R_dobj': 24, 'R_csubjpass': 98, 'L_rcmod': 61, 'R_purpcl': 90, 'L_root': 1, 'R_prt': 82, 'R_ccomp': 48, 'R_neg': 56, 'R_abbrev': 94, 'R_conj': 28, 'L_punct': 9, 'L_attr': 77, 'L_tmod': 49, 'L_complm': 63, 'R_root': 2, 'R_rcmod': 62, 'L_auxpass': 35, 'L_conj': 27, 'R_xcomp': 46, 'L_poss': 13, 'L_predet': 71, 'L_dobj': 23, 'R_mark': 68, 'R_complm': 64, 'R_attr': 78, 'L_expl': 87, 'R_number': 80, 'R_advcl': 70, 'L_npadvmod': 53, 'R_pobj': 16, 'R_cc': 26, 'L_quantmod': 43, 'L_prep': 17, 'L_appos': 51, 'R_acomp': 42, 'L_pobj': 15, 'L_neg': 55, 'L_parataxis': 65, 'L_mwe': 75, 'L_xcomp': 45, 'L_aux': 39, 'L_preconj': 91, 'R_amod': 20, 'R_appos': 52, 'L_csubj': 73, 'R_preconj': 92, 'L_purpcl': 89, 'L_cc': 25, 'L_nsubjpass': 37, 'R_det': 8, 'R_auxpass': 36, 'L_iobj': 83, 'R_rel': 86, 'R_mwe': 76, 'R_iobj': 84, 'L_advmod': 33, 'R_advmod': 34, 'R_possessive': 12, 'R_pcomp': 60, 'R_nsubj': 22, 'R_prep': 18, 'Shift': 0, 'R_partmod': 32, 'L_number': 79, 'L_nn': 5, 'L_ccomp': 47, 'L_partmod': 31, 'L_abbrev': 93, 'R_tmod': 50, 'L_pcomp': 59, 'L_prt': 81, 'L_possessive': 11, 'L_acomp': 41, 'R_poss': 14, 'R_parataxis': 66, 'R_aux': 40, 'L_mark': 67, 'L_advcl': 69, 'L_infmod': 57, 'R_dep': 30, 'L_num': 3, 'R_quantmod': 44, 'R_expl': 88}\n",
      "[0 0 1 0]\n",
      "[0 1 2 0]\n",
      "[1 2 3 0]\n",
      "[2 3 4 0]\n",
      "[3 4 5 0]\n",
      "[4 5 6 3]\n",
      "[3 5 6 5]\n",
      "[2 5 6 7]\n",
      "[1 5 6 0]\n",
      "[5 6 7 0]\n",
      "[6 7 8 0]\n",
      "[7 8 9 0]\n",
      "[ 8  9 10  7]\n",
      "[ 7  9 10  9]\n",
      "[ 6  9 10  0]\n",
      "[ 9 10 11 10]\n",
      "[ 6  9 11  0]\n",
      "[ 9 11 12  0]\n",
      "[11 12 13  0]\n",
      "[12 13 14 12]\n",
      "[11 12 14  0]\n",
      "[12 14 15  0]\n",
      "[14 15 16  5]\n",
      "[12 15 16 13]\n",
      "[11 15 16 16]\n",
      "[ 9 11 16 18]\n",
      "[ 6  9 16 16]\n",
      "[ 5  6 16 18]\n",
      "[ 1  5 16  0]\n",
      "[ 5 16 17  0]\n",
      "[16 17 18  0]\n",
      "[17 18 19  0]\n",
      "[18 19 20 19]\n",
      "[17 19 20  0]\n",
      "[19 20 21 21]\n",
      "[17 20 21  9]\n",
      "[16 20 21  9]\n",
      "[ 5 20 21  0]\n",
      "[20 21 22  0]\n",
      "[21 22 23  7]\n",
      "[20 22 23 24]\n",
      "[ 5 20 23  0]\n",
      "[20 23 24  0]\n",
      "[23 24 25  0]\n",
      "[24 25 26  5]\n",
      "[23 25 26 16]\n",
      "[20 23 26 18]\n",
      "[ 5 20 26  0]\n",
      "[20 26 27 10]\n",
      "[ 5 20 27  0]\n",
      "[20 27 28 10]\n",
      "[ 5 20 28  0]\n",
      "[20 28 29  0]\n",
      "[28 29 30 26]\n",
      "[20 28 30  0]\n",
      "[28 30 31 28]\n",
      "[20 28 31 30]\n",
      "[ 5 20 31  0]\n",
      "[20 31 32 10]\n",
      "[ 5 20 32 30]\n",
      "[ 1  5 32 16]\n",
      "[ 0  1 32  0]\n",
      "[ 1 32 33  0]\n",
      "[32 33 34  0]\n",
      "[33 34 35  7]\n",
      "[32 34 35  0]\n",
      "[34 35 36  0]\n",
      "[35 36 37 16]\n",
      "[34 35 37 18]\n",
      "[32 34 37  0]\n",
      "[34 37 38 10]\n",
      "[32 34 38  0]\n",
      "[34 38 39  0]\n",
      "[38 39 40  0]\n",
      "[39 40 41  0]\n",
      "[40 41 42  5]\n",
      "[39 41 42 16]\n",
      "[38 39 42 18]\n",
      "[34 38 42 32]\n",
      "[32 34 42  0]\n",
      "[34 42 43 10]\n",
      "[32 34 43  0]\n",
      "[34 43 44  0]\n",
      "[43 44 45  0]\n",
      "[44 45 46 33]\n",
      "[43 45 46 35]\n",
      "[34 45 46 37]\n",
      "[32 45 46  9]\n",
      "[ 1 45 46 17]\n",
      "[ 0 45 46  0]\n",
      "[45 46 47  0]\n",
      "[46 47 48  0]\n",
      "[47 48 49  5]\n",
      "[46 48 49 16]\n",
      "[45 46 49 18]\n",
      "[ 0 45 49  0]\n",
      "[45 49  0 10]\n",
      "[ 0 45  0  2]\n"
     ]
    }
   ],
   "source": [
    "print(arc_dict)\n",
    "for line in action_data[0]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# TF functions\n",
    "##############################\n",
    "\n",
    "def length(sequence):\n",
    "    \"\"\"\n",
    "    function that computes the real, unpadded lenghts for every sequence in batch\n",
    "    \"\"\"\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "\n",
    "def mlp(_X, _weights, _biases):\n",
    "    \"\"\"\n",
    "    function that defines a multilayer perceptron in the graph\n",
    "    input shape: parse_steps (=?) x filtered_words (=3) x lstm_output_length (=400)\n",
    "    output shape: parse_steps (=?) x num_classes\n",
    "    \"\"\"\n",
    "    # ReLU hidden layer (output shape: parse_steps x n_hidden)\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.einsum('ijk,kl->il', _X, _weights['h']), _biases['b'])) \n",
    "    # return output layer (output shape: parse_steps x n_classes)\n",
    "    return tf.add(tf.einsum('ik,kl->il', layer_1, _weights['out']), _biases['out'])\n",
    "\n",
    "def embedding_lookup(sentences, max_seq_length, vec_length):\n",
    "    \"\"\"\n",
    "    function that looks up embeddings.\n",
    "    input: list of sentences, length of sentences, length of word vectors\n",
    "    output: 3D array of word vectors per sentence \n",
    "                (dims #sentences x sentence_length x embedding_size)\n",
    "    \"\"\"\n",
    "    sentence_embeddings = np.empty((0,max_seq_length,vec_length))\n",
    "    for sentence in sentences:\n",
    "        word_embeddings = np.empty((0,vec_length))\n",
    "        for word in sentence:\n",
    "            word_embeddings = np.vstack([word_embeddings, model[word]])\n",
    "        if len(sentence) < max_seq_length:\n",
    "            zero_padding_length = max_seq_length - len(sentence)\n",
    "            word_embeddings = np.vstack([word_embeddings, np.zeros((zero_padding_length, vec_length))])\n",
    "        sentence_embeddings = np.append(sentence_embeddings, np.array([word_embeddings]), axis=0)\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model first\n",
    "model_name = \"dep_parser_word2vec_total\"\n",
    "model = word2vec.Word2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# create training data using parser\n",
    "# for each sentence, get embedded representation\n",
    "# and actions ONLY when needed\n",
    "##############################\n",
    "\n",
    "def create_sentence_embeddings(sentences):\n",
    "    embedded_train_sentences = []\n",
    "    for sentence in sentences:\n",
    "        embed = model[sentence]\n",
    "        embedded_train_sentences.append(embed)\n",
    "    return embedded_train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the word2vec embedding version for all trainings sentences\n",
    "embedded_train_sentences = create_sentence_embeddings(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_output_array(indices_and_actions):\n",
    "    max_len = 0\n",
    "    no_sent = 0\n",
    "    for sent in indices_and_actions:\n",
    "        no_sent +=1\n",
    "        if len(sent) > max_len:\n",
    "            max_len = len(sent)\n",
    "            print(max_len)\n",
    "    \n",
    "    input_indices = np.empty((no_sent,max_len,3),dtype=np.int64) # ,dtype=np.ndarray\n",
    "    input_indices.fill(-2)\n",
    "\n",
    "    output_actions = np.empty((no_sent,max_len),dtype=np.int64)\n",
    "    output_actions.fill(-1)\n",
    "    \n",
    "    sentence_count = 0\n",
    "    for sent in indices_and_actions:\n",
    "        action_pair_count = 0\n",
    "        for action_pair in sent:\n",
    "            output_actions[sentence_count,action_pair_count] = action_pair[-1]\n",
    "            index_count = 0\n",
    "            indices = action_pair[:-1]\n",
    "            for ind in indices:\n",
    "                input_indices[sentence_count,action_pair_count,index_count] = ind\n",
    "                index_count += 1\n",
    "            action_pair_count += 1\n",
    "        sentence_count += 1\n",
    "    return input_indices, output_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "100\n",
      "116\n",
      "118\n",
      "128\n",
      "136\n",
      "198\n",
      "216\n",
      "238\n",
      "282\n",
      "(39832, 282, 3)\n"
     ]
    }
   ],
   "source": [
    "# stack+buffer indices need to be put in a padded numpy array first\n",
    "input_indices, output_actions = get_output_array(action_data)\n",
    "print(input_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec vector length:  189\n",
      "max sentence length:  141\n",
      "no. of training sentences:  39832\n",
      "--------------------\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# create some variables for TF\n",
    "word2vec_length = model['a'].size\n",
    "seq_lengths = [len(sentence) for sentence in train_sentences]\n",
    "max_seq_length = max(seq_lengths)\n",
    "train_set_length = len(train_sentences)\n",
    "print(\"word2vec vector length: \", word2vec_length)\n",
    "print(\"max sentence length: \", max_seq_length)\n",
    "print(\"no. of training sentences: \", train_set_length)\n",
    "print(\"--------------------\")\n",
    "print(type(seq_lengths[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns a list with padded np arrays for every word2vec sentence\n",
    "def get_input_array(unpadded_word2vec_sentences):\n",
    "    padded_sentences = []\n",
    "    for unpad_sentence in unpadded_word2vec_sentences:\n",
    "        unpad_length = unpad_sentence.shape\n",
    "        pad_array = np.empty((max_seq_length - unpad_length[0], word2vec_length),dtype=np.float64)\n",
    "        pad_array.fill(-2.)\n",
    "        padded_sentences.append(np.concatenate((unpad_sentence,pad_array)))\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding word2vec sentences... this might take a while\n",
      "... DONE\n",
      "39832\n"
     ]
    }
   ],
   "source": [
    "print(\"padding word2vec sentences... this might take a while\")\n",
    "padded_word2vec_train = get_input_array(embedded_train_sentences)\n",
    "print(\"... DONE\")\n",
    "print(len(padded_word2vec_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass size:  (10, 141, 200)\n",
      "BiLSTM output shape:  (10, 141, 400)\n",
      "BiLSTM+padding output shape:  (10, 142, 400)\n",
      "Parse indices shape:  Tensor(\"Placeholder_2:0\", shape=(10, 282, 3), dtype=int64)\n",
      "MLP input shape:  (282, 3, 400)\n",
      "MLP output shape S{}:  (282, 99)\n",
      "(141, 189)\n",
      "(10, 282, 3)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[50,0] = -2 is not in [0, 142)\n\t [[Node: embedding_lookup_9 = Gather[Tindices=DT_INT64, Tparams=DT_DOUBLE, _class=[\"loc:@strided_slice_27\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](strided_slice_27, strided_slice_28)]]\n\nCaused by op 'embedding_lookup_9', defined at:\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-b9ff6dade28c>\", line 78, in <module>\n    mlp_x = tf.nn.embedding_lookup(output_lstm[i,:,:], parse_indices[i,:,:])\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py\", line 87, in embedding_lookup\n    validate_indices=validate_indices)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1010, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n    op_def=op_def)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[50,0] = -2 is not in [0, 142)\n\t [[Node: embedding_lookup_9 = Gather[Tindices=DT_INT64, Tparams=DT_DOUBLE, _class=[\"loc:@strided_slice_27\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](strided_slice_27, strided_slice_28)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Lucas/anaconda/envs/python3/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/errors.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    462\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    464\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[50,0] = -2 is not in [0, 142)\n\t [[Node: embedding_lookup_9 = Gather[Tindices=DT_INT64, Tparams=DT_DOUBLE, _class=[\"loc:@strided_slice_27\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](strided_slice_27, strided_slice_28)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b9ff6dade28c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mfeed_dict_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0msentence_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpadded_word2vec_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mparse_indices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[50,0] = -2 is not in [0, 142)\n\t [[Node: embedding_lookup_9 = Gather[Tindices=DT_INT64, Tparams=DT_DOUBLE, _class=[\"loc:@strided_slice_27\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](strided_slice_27, strided_slice_28)]]\n\nCaused by op 'embedding_lookup_9', defined at:\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-b9ff6dade28c>\", line 78, in <module>\n    mlp_x = tf.nn.embedding_lookup(output_lstm[i,:,:], parse_indices[i,:,:])\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py\", line 87, in embedding_lookup\n    validate_indices=validate_indices)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1010, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n    op_def=op_def)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[50,0] = -2 is not in [0, 142)\n\t [[Node: embedding_lookup_9 = Gather[Tindices=DT_INT64, Tparams=DT_DOUBLE, _class=[\"loc:@strided_slice_27\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](strided_slice_27, strided_slice_28)]]\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # hyperparameters (from Cross & Huang, 2016)\n",
    "    batch_size = 10\n",
    "    n_input = 400\n",
    "    n_hidden = 200\n",
    "    n_classes = 99 # there are 99 possible actions to take\n",
    "    lstm_units = 200\n",
    "    num_epochs = 10\n",
    "    dropout = 0.5\n",
    "    L2_penalty = 0.\n",
    "    rho = 0.99\n",
    "    epsilon = 1e-07\n",
    "    learn_rate = 0.01\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h': tf.Variable(tf.random_normal([n_input, n_hidden], dtype=tf.float64)),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], dtype=tf.float64))\n",
    "    }\n",
    "    biases = {\n",
    "        'b': tf.Variable(tf.random_normal([n_hidden], dtype=tf.float64)),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes], dtype=tf.float64))\n",
    "    }\n",
    "    \n",
    "    # placeholders for input (input tensor uses None as variable sized dimension, \n",
    "    # or can use rows of train set length, or batch length might be better)\n",
    "    lstm_x = tf.placeholder(tf.float64, [batch_size, max_seq_length, word2vec_length])\n",
    "    # lstm output (1 pass, max sentence length, lstm output vector length)\n",
    "#     lstm_y = tf.placeholder(\"float\", [None, max_seq_length, n_hidden])\n",
    "    # mlp input (3 words, 2xlstm output)\n",
    "#     mlp_x = tf.placeholder(\"float\", [3, 2*n_hidden])\n",
    "    # mlp output (number of action classes)\n",
    "#     mlp_y = tf.placeholder(\"float\", [n_classes])\n",
    "    \n",
    "    # placeholder for sentence length per batch (one int for each sentence)\n",
    "    sentence_lengths = tf.placeholder(tf.int64, [batch_size])\n",
    "    \n",
    "    # placeholder for indices (batch size, max sentence length(=2*max sentence length), 3 indices)\n",
    "    parse_indices = tf.placeholder(tf.int64, [batch_size, 2*max_seq_length, 3])\n",
    "    \n",
    "    # placeholder for labels (batch size, max parse length(=2*max sentence length))\n",
    "    labels = tf.placeholder(tf.int64, [batch_size, 2*max_seq_length])\n",
    "    \n",
    "    # define LSTM cell + dropout wrapper (like Cross & Huang)\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=lstm_units, state_is_tuple=True)\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=dropout)\n",
    "\n",
    "    # define bidirectional architecture\n",
    "    outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=cell,\n",
    "        cell_bw=cell,\n",
    "        dtype=tf.float64,\n",
    "        sequence_length=sentence_lengths,\n",
    "        inputs=lstm_x\n",
    "    )\n",
    "\n",
    "    # fw/bw output (num_sequences x max_seq_length x lstm_units)\n",
    "    output_fw, output_bw = outputs\n",
    "    print(\"Forward pass size: \", output_fw.get_shape())\n",
    "\n",
    "    # concatenate forward & backward outputs per word\n",
    "    output_lstm = tf.concat(2, outputs)\n",
    "    print(\"BiLSTM output shape: \", output_lstm.get_shape())\n",
    "    \n",
    "    # zero-padding of LSTM-output (every sentence gets a \"dummy word\" in front of it)\n",
    "    zero_padding = tf.zeros([batch_size, 1, n_input], tf.float64)\n",
    "    output_lstm = tf.concat(1, [zero_padding, output_lstm])\n",
    "    print(\"BiLSTM+padding output shape: \", output_lstm.get_shape())\n",
    "    \n",
    "    print(\"Parse indices shape: \", parse_indices)\n",
    "    \n",
    "    batch_cost = 0.\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        mlp_x = tf.nn.embedding_lookup(output_lstm[i,:,:], parse_indices[i,:,:])\n",
    "        output_mlp = mlp(mlp_x, weights, biases)\n",
    "        ### TODO: ###\n",
    "        # opsplitsen in zinnen van correcte lengte (padding weg)\n",
    "        # voor elke plek in elke zin output mlp berekenen\n",
    "        # outputs in tensor/array als resultaat (kan dit wel?)\n",
    "        ### TODO: ###\n",
    "        \n",
    "        if i < 1:\n",
    "            print(\"MLP input shape: \", mlp_x.get_shape())\n",
    "            print(\"MLP output shape S{}: \", output_mlp.get_shape())\n",
    "        \n",
    "        cost = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(output_mlp, labels[i]))\n",
    "        batch_cost += cost\n",
    "    \n",
    "    batch_cost /= batch_size\n",
    "        \n",
    "        \n",
    "    # now use a feed_dict\n",
    "\n",
    "    print(padded_word2vec_train[0:batch_size][0].shape)\n",
    "    ### TODO: ###\n",
    "    # juiste acties zijn nu ints, moeten one-hot vectoren worden met een 1 op de juiste index\n",
    "    # dat wordt gepassd aan session.run\n",
    "    \n",
    "    # dit is de oplossing:\n",
    "    # output_actions_onehot = tf.one_hot(output_actions, depth=99, axis=-1)\n",
    "    # ...maar het is handiger om de ints te laten staan vanwege memory efficiency!\n",
    "    # tf.nn.sparse_softmax_cross_entropy_with_logits() laat ints ipv one-hot-vectors to als labels :D \n",
    "    \n",
    "\n",
    "    with tf.Session() as session:\n",
    "        init = tf.initialize_all_variables()\n",
    "        session.run(init)\n",
    "        \n",
    "        print(input_indices[0:10].shape)\n",
    "        feed_dict_batch = {sentence_lengths: seq_lengths[0:batch_size], lstm_x: padded_word2vec_train[0:batch_size],  parse_indices: input_indices[0:batch_size], labels: output_actions[0:batch_size]}\n",
    "        result = session.run(batch_cost, feed_dict_batch)\n",
    "        print(result[0])\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 189)\n",
      "(39832, 282)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder_1:0\", shape=(10,), dtype=int64) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    868\u001b[0m             subfeed_t = self.graph.as_graph_element(subfeed, allow_tensor=True,\n\u001b[0;32m--> 869\u001b[0;31m                                                     allow_operation=False)\n\u001b[0m\u001b[1;32m    870\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2458\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2536\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2537\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2538\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"Placeholder_1:0\", shape=(10,), dtype=int64) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f68b832f4405>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mfeed_dict_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlstm_sent_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtemp_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpadded_word2vec_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtemp_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mparse_indices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtemp_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    870\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             raise TypeError('Cannot interpret feed_dict key as Tensor: '\n\u001b[0;32m--> 872\u001b[0;31m                             + e.args[0])\n\u001b[0m\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder_1:0\", shape=(10,), dtype=int64) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "# now use a feed_dict\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "print(padded_word2vec_train[0:batch_size][0].shape)\n",
    "print(output_actions.shape)\n",
    "### TODO: ###\n",
    "# juiste acties zijn nu ints, moeten one-hot vectoren worden met een 1 op de juiste index\n",
    "# dat wordt gepassd aan session.run\n",
    "# dit is de oplossing:\n",
    "# output_onehot = tf.one_hot(output_actions, depth=1)\n",
    "# ...maar het is handiger om de ints te laten staan vanwege memory efficiency!\n",
    "# tf.nn.sparse_softmax_cross_entropy_with_logits() laat ints ipv one-hot-vectors to als labels :D \n",
    "\n",
    "\n",
    "with tf.Session() as session:\n",
    "    feed_dict_batch = {sentence_lengths: seq_lengths[0:batch_size], lstm_x: padded_word2vec_train[0:batch_size],  parse_indices: input_indices[0:10]}\n",
    "    result = session.run(output_actions[0:batch_size], feed_dict_batch)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check_index = 1\n",
    "print(embedded_train_sentences[check_index].shape)\n",
    "print(len(embedded_train_sentences[check_index]))\n",
    "# print(embedded_train_sentences[check_index])\n",
    "print(type(embedded_train_sentences[check_index][0][0]))\n",
    "for word in train_sentences[check_index]:\n",
    "    print(word)\n",
    "# for emb in embedded_train_sentences[check_index]:\n",
    "#     print(emb)\n",
    "# for action in action_data[check_index]:\n",
    "#     print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### OUD OUD OUD\n",
    "\n",
    "##############################\n",
    "# build graph\n",
    "##############################\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # hyperparameters (from Cross & Huang, 2016)\n",
    "    batch_size = 10\n",
    "    n_input = 400\n",
    "    n_hidden = 200\n",
    "    n_classes = 99 # there are 99 possible actions to take\n",
    "    lstm_units = 200\n",
    "    num_epochs = 10\n",
    "    dropout = 0.5\n",
    "    L2_penalty = 0.\n",
    "    rho = 0.99\n",
    "    epsilon = 1e-07\n",
    "    learn_rate = 0.01\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h': tf.Variable(tf.random_normal([n_input, n_hidden], dtype=tf.float64)),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], dtype=tf.float64))\n",
    "    }\n",
    "    biases = {\n",
    "        'b': tf.Variable(tf.random_normal([n_hidden], dtype=tf.float64)),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes], dtype=tf.float64))\n",
    "    }\n",
    "\n",
    "    # load word2vec model\n",
    "    model_name = \"dep_parser_word2vec_total\"\n",
    "    if not 'model' in locals():\n",
    "        model = models.word2vec.Word2Vec.load(model_name)\n",
    "        print(\"Model loaded from disk\")\n",
    "    else:\n",
    "        print(\"Model was already loaded\")\n",
    "\n",
    "    # placeholders for input (input tensor uses None as variable sized dimension, \n",
    "    # or can use rows of train set length, or batch length might be better)\n",
    "    lstm_x = tf.placeholder(\"float\", [None, max_seq_length, word2vec_length])\n",
    "    # lstm output (1 pass)\n",
    "    lstm_y = tf.placeholder(\"float\", [None, max_seq_length, n_hidden])\n",
    "    # mlp input (3 words)\n",
    "    mlp_x = tf.placeholder(\"float\", [3, 2*n_hidden])\n",
    "    # mlp output\n",
    "    mlp_y = tf.placeholder(\"float\", [n_classes])\n",
    "    \n",
    "    \n",
    "    # dummy data:\n",
    "    # toy sentences (= list of lists of words)\n",
    "#     sentences = [[\"the\", \"by\", \"an\", \"on\", \"the\", \"in\", \"an\"], [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"ground\"]]\n",
    "#     toy_sentences = train_sentences[0:30]\n",
    "#     toy_parses = action_data[0:30]\n",
    "    # toy parses (= array with an action and word indices)\n",
    "#     parses = [np.array([[1,0,1,2], \n",
    "#                         [0,1,2,3],\n",
    "#                         [0,1,2,3],\n",
    "#                         [1,0,1,2]]),\n",
    "#               np.array([[1,0,1,2], \n",
    "#                         [0,1,2,3], \n",
    "#                         [3,2,3,4]])]\n",
    "    \n",
    "    # look up embeddings of words in sentence\n",
    "#     toy_embeddings = np.array(embedded_train_sentences[0:30])\n",
    "#     embeddings = embedding_lookup(toy_sentences, max_seq_length, vec_length)\n",
    "#     embeddings = embedded_train_sentences[0:30]\n",
    "#     print(\"Sentence embedding shape (np-array): \", embeddings.shape)\n",
    "\n",
    "    # define LSTM cell + dropout wrapper (like Cross & Huang)\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=lstm_units, state_is_tuple=True)\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=0.5)\n",
    "\n",
    "    # define bidirectional architecture\n",
    "    outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=cell,\n",
    "        cell_bw=cell,\n",
    "        dtype=tf.float64,\n",
    "        sequence_length=seq_lengths,\n",
    "        inputs=lstm_x\n",
    "    )\n",
    "\n",
    "    # fw/bw output (num_sequences x max_seq_length x lstm_units)\n",
    "    output_fw, output_bw = outputs\n",
    "    print(\"Forward pass size: \", output_fw.get_shape())\n",
    "\n",
    "    # concatenate forward & backward outputs per word\n",
    "    output_lstm = tf.concat(2, outputs)\n",
    "    print(\"BiLSTM output shape: \", output_lstm.get_shape())\n",
    "\n",
    "\n",
    "    # MLP layer filtering LSTM outputs based on parse steps\n",
    "    batch_cost = 0\n",
    "    for i in range(0, len(toy_sentences)):\n",
    "        action = action_data[i]\n",
    "#         sentence_input_indices = action[0:5]\n",
    "        print(input_indices)\n",
    "        output_action_index = action[-1]\n",
    "        print(output_action_index)\n",
    "        sentence = toy_sentences[i]\n",
    "#         print(len(sentence))\n",
    "        emb = embeddings[i]\n",
    "#         print(emb.shape())\n",
    "        parse = toy_parses[i]\n",
    "        # input: parse_steps (=?) x filtered_words (=3) x lstm_output_length (=400)\n",
    "        # output: parse_steps (=?) x num_classes\n",
    "        output_mlp = mlp(tf.gather(emb, parse), weights, biases)\n",
    "        print(\"MLP output shape S{}: \".format(i), output_mlp.get_shape())\n",
    "        cost = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(output_mlp, parse[:,0]))\n",
    "        batch_cost += cost\n",
    "    batch_cost /= len(sentences)\n",
    "\n",
    "    # define optimizer\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=True)\n",
    "    optimizer = tf.train.AdadeltaOptimizer(rho=rho, epsilon=epsilon).minimize(batch_cost)\n",
    "#     train_op = optimizer.train(loss, global_step=global_step)\n",
    "    correct = tf.nn.in_top_k(embeddings[0], parses[0], 1)\n",
    "    print(correct)\n",
    "\n",
    "    \n",
    "    ##############################\n",
    "    # start session in graph\n",
    "    ##############################\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
