{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# import modules\n",
    "##############################\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the file, \n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as myfile:\n",
    "        f = myfile.readlines()\n",
    "        s_num = 0\n",
    "        i =0\n",
    "        sentence_s = []\n",
    "        tag_s = []\n",
    "        dep_s = []\n",
    "        s  = []   # sentence\n",
    "        p = []    # tag\n",
    "        d = []    # dependency\n",
    "        for l in f:\n",
    "            \n",
    "            v = l.replace('\\n','').split(\"\\t\")\n",
    "            v.append(s_num)\n",
    "            if len(l) != 1:\n",
    "                data.append(v)\n",
    "                dep = v[6] + '_' + v[7]\n",
    "                word = v[1].lower()\n",
    "                if any(char.isdigit() for char in word):\n",
    "                    word = 'NUM'       # replace numbers with NUM\n",
    "                s.append(word)\n",
    "                p.append(v[3])\n",
    "                d.append(dep)\n",
    "                i +=1\n",
    "            else:\n",
    "                sentence_s.append(s)\n",
    "                tag_s.append(p)\n",
    "                dep_s.append(d)\n",
    "                s_num +=1\n",
    "                s  = []\n",
    "                p = []\n",
    "                d = []\n",
    "        \n",
    "    return data, sentence_s, tag_s, dep_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(dataname):\n",
    "    #reads in files, produces data structure with all actions\n",
    "        #does so by applying produce_rule_list to every sentence.\n",
    "        #for loop that sets actions to empty, calls p_r_l giving it\n",
    "        #the stack and buffer, actions and correct_parse, adds finished action list\n",
    "        #to new data file, for each sentence in the input data\n",
    "    #input: name of the data file with all parses. Run with data file in same directory.\n",
    "    #output: data file with all actions\n",
    "    file = open(dataname)\n",
    "    data = file.read()\n",
    "    correct_parses = correct_parse_list(data)\n",
    "    #gets rid of final whitespace\n",
    "    del correct_parses[len(correct_parses)-1]\n",
    "    \n",
    "    #iterates over all parses, producing action list for each\n",
    "    complete_rule_list = []\n",
    "    arc_dict = {'Shift':0,'L_root':1,'R_root':2}\n",
    "    for sentence_parse in correct_parses:\n",
    "        stack = []\n",
    "#         print(len(sentence_parse))\n",
    "        buff = list(range(1,len(sentence_parse)+1))\n",
    "        actions = []\n",
    "        rule_list, arc_dict = produce_rule_list(stack, buff, actions, sentence_parse, arc_dict)\n",
    "        complete_rule_list.append(np.array(rule_list))\n",
    "\n",
    "    \n",
    "    return complete_rule_list, arc_dict\n",
    "\n",
    "def correct_parse_list(data):\n",
    "    #Turns data into a list of lists of lists with relevant information\n",
    "    correct_parse = data.split(\"\\n\\n\")\n",
    "    for index, paragraph in enumerate(correct_parse):\n",
    "        correct_parse[index] = paragraph.split(\"\\n\")\n",
    "    for paragraph in correct_parse:\n",
    "        for index, line in enumerate(paragraph):\n",
    "            paragraph[index] = line.split(\"\\t\")\n",
    "    return correct_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_rule_list(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #recursive function that works through words in the sentence (stack/buffer)\n",
    "        #until only one word is left, creating the list of actions \n",
    "        #that was taken to parse it.\n",
    "    #input: stack, buffer, actions, correct parse\n",
    "    #output: actions with the actions taken for each buff/stack configuration\n",
    "    \n",
    "    #base case\n",
    "    if len(stack) == 1 and len(buff) == 0:\n",
    "        #actions.append([stack[:], \"empty\", \"R_arc\"])\n",
    "        actions.append([0,stack[0], 0, 2])\n",
    "        return actions, arc_dict\n",
    "\n",
    "    #If enough of the sentence is still left:\n",
    "    #If there is not enough material in the stack, shift:\n",
    "    if len(stack) == 0 :\n",
    "        #print('chose S - small stack')\n",
    "        actions.append([0,0,buff[0], 0])\n",
    "        stack.append(buff[0])\n",
    "        del buff[0]        \n",
    "        return produce_rule_list(stack,buff,actions,sentence_parse, arc_dict)\n",
    "    if len(stack) == 1:\n",
    "        actions.append([0,stack[-1],buff[0], 0])\n",
    "        stack.append(buff[0])\n",
    "        del buff[0]\n",
    "        return produce_rule_list(stack,buff,actions,sentence_parse, arc_dict)\n",
    "    #If there are 2 or more words in the stack, decide which action to perform and perform it\n",
    "    if len(stack) > 1:\n",
    "        action = rule_decision(stack,buff,sentence_parse)\n",
    "        stack, buff, actions, arc_dict = action(stack,buff,actions, sentence_parse, arc_dict)\n",
    "        return produce_rule_list(stack,buff,actions,sentence_parse, arc_dict)\n",
    "    \n",
    "\n",
    "def rule_decision(stack, buff, sentence_parse):\n",
    "    #determines which action to apply\n",
    "    #input: words on stack, words on buff, correct parse\n",
    "    #output: one of three methods, Shift(), L_arc(), R_arc()\n",
    "\n",
    "    #find ids/heads (index [6]) from stack and sentence_parse\n",
    "    s1 = stack[-2]\n",
    "    head_of_s1 = int(sentence_parse[s1-1][6])\n",
    "    s2 = stack[-1]\n",
    "    head_of_s2 = int(sentence_parse[s2-1][6])\n",
    "    \n",
    "    #L arcs can always be applied if possible\n",
    "    if head_of_s1 == s2:\n",
    "        action = L_arc\n",
    "        #print('chose L')\n",
    "    else:\n",
    "        #R arcs can only be applied if there is no word in the buffer which has the last word in the stack as a head\n",
    "        if head_of_s2 == s1:\n",
    "            buff_heads = [int(sentence_parse[x-1][6]) for x in buff]\n",
    "            if s2 in buff_heads:\n",
    "                action = Shift\n",
    "                #print('chose S - s2 in buffheads')\n",
    "            else:\n",
    "                action = R_arc\n",
    "                #print('chose R')\n",
    "        #if there is no match between s1 and s2, simply shift another word from the buffer\n",
    "        else:\n",
    "            action = Shift\n",
    "            #print('chose S - no matching s1s2')\n",
    "\n",
    "    return action\n",
    "\n",
    "#The following methods perform an arc or shift. These can be changed if more data is needed in the network.\n",
    "\n",
    "def L_arc(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #removes second to last item from stack, writes action to actions\n",
    "    #input: stack and actions\n",
    "    #output: new stack and actions with one L_arc line\n",
    "    #s1, s2, b1, action\n",
    "    s1 = int(stack[-2])\n",
    "    s2 = int(stack[-1])\n",
    "    if len(buff) == 0:\n",
    "        b1 = 0\n",
    "    else:\n",
    "        b1 = int(buff[0])\n",
    "    relation = \"L_\"+sentence_parse[s1-1][7]\n",
    "\n",
    "    if relation not in arc_dict:\n",
    "        maximum = max(arc_dict, key=arc_dict.get)\n",
    "        arc_dict['L_'+relation[2:]] = arc_dict[maximum]+1\n",
    "        arc_dict['R_'+relation[2:]] = arc_dict[maximum]+2\n",
    "    \n",
    "\n",
    "    actions.append([s1,s2,b1, arc_dict[relation]])\n",
    "    del stack[-2]\n",
    "    return stack, buff, actions, arc_dict\n",
    "\n",
    "\n",
    "\n",
    "def R_arc(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #removes last item from the stack, writes action to actions\n",
    "    #input: stack and actions\n",
    "    #output: new stack and actions with one R_arc line\n",
    "    #s1, s2, b1, action\n",
    "    s1 = int(stack[-2])\n",
    "    s2 = int(stack[-1])\n",
    "    if len(buff) == 0:\n",
    "        b1 = 0\n",
    "    else:\n",
    "        b1 = int(buff[0])\n",
    "        \n",
    "    relation = \"R_\"+sentence_parse[s2-1][7]\n",
    "\n",
    "    if relation not in arc_dict:\n",
    "        maximum = max(arc_dict, key=arc_dict.get)\n",
    "        arc_dict['L_'+relation[2:]] = arc_dict[maximum]+1\n",
    "        arc_dict['R_'+relation[2:]] = arc_dict[maximum]+2 \n",
    "    \n",
    "    actions.append([s1,s2,b1, arc_dict[relation]])\n",
    "    del stack[-1]\n",
    "    return stack, buff, actions, arc_dict\n",
    "\n",
    "\n",
    "\n",
    "def Shift(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #moves an item from the buff to the stack, writes action to actions\n",
    "    #input: stack, buff and actions\n",
    "    #output: new stack and actions with one extra shift line\n",
    "    #s1, s2, b1, action\n",
    "    s1 = int(stack[-2])\n",
    "    s2 = int(stack[-1])\n",
    "    b1 = int(buff[0])\n",
    "    #actions.append([stack[:], buff[:], \"Shift\"])\n",
    "    actions.append([s1,s2,b1, 0])\n",
    "    stack.append(buff[0])\n",
    "    del buff[0]\n",
    "    return stack, buff, actions, arc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  [['1', 'In', '_', 'IN', 'IN', '_', '45', 'prep', '_', '_', 0], ['2', 'an', '_', 'DT', 'DT', '_', '5', 'det', '_', '_', 0]]\n",
      "words sentences:  [['rolls-royce', 'motor', 'cars', 'inc.', 'said', 'it', 'expects', 'its', 'u.s.', 'sales', 'to', 'remain', 'steady', 'at', 'about', 'NUM', 'cars', 'in', 'NUM', '.'], ['the', 'luxury', 'auto', 'maker', 'last', 'year', 'sold', 'NUM', 'cars', 'in', 'the', 'u.s.']]\n",
      "tags sentences:  [['NNP', 'NNP', 'NNPS', 'NNP', 'VBD', 'PRP', 'VBZ', 'PRP$', 'NNP', 'NNS', 'TO', 'VB', 'JJ', 'IN', 'IN', 'CD', 'NNS', 'IN', 'CD', '.'], ['DT', 'NN', 'NN', 'NN', 'JJ', 'NN', 'VBD', 'CD', 'NNS', 'IN', 'DT', 'NNP']]\n",
      "dependencies:  [['4_nn', '4_nn', '4_nn', '5_nsubj', '0_root', '7_nsubj', '5_ccomp', '10_poss', '10_nn', '12_nsubj', '12_aux', '7_xcomp', '12_acomp', '12_prep', '16_quantmod', '17_num', '14_pobj', '12_prep', '18_pobj', '5_punct'], ['4_det', '4_nn', '4_nn', '7_nsubj', '6_amod', '7_tmod', '0_root', '9_num', '7_dobj', '7_prep', '12_det', '10_pobj']]\n"
     ]
    }
   ],
   "source": [
    "train_data, train_sentences, train_tags, train_dependencies = read_data('./data/train-stanford-raw.conll')\n",
    "dev_data, dev_sentences, dev_tags, dev_dependencies = read_data('./data/dev-stanford-raw.conll')\n",
    "test_data, test_sentences, test_tags, test_dependencies = read_data('./data/test-stanford-raw.conll')\n",
    "\n",
    "# create a full set of all the words in our train, test, and dev sets for word2vec model\n",
    "# in order to avoid unseen words during test and validation\n",
    "total_sentences = train_sentences + dev_sentences + test_sentences\n",
    "print('data: ', train_data[:2])\n",
    "print('words sentences: ', total_sentences[2:4])\n",
    "print('tags sentences: ', train_tags[2:4])\n",
    "print('dependencies: ', train_dependencies[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action_data, arc_dict = process_data('./data/train-stanford-raw.conll')\n",
    "\n",
    "# print(arc_dict)\n",
    "# for line in action_data[0]:\n",
    "#     print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# TF functions\n",
    "##############################\n",
    "\n",
    "def length(sequence):\n",
    "    \"\"\"\n",
    "    function that computes the real, unpadded lenghts for every sequence in batch\n",
    "    \"\"\"\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "\n",
    "def mlp(_X, _weights, _biases):\n",
    "    \"\"\"\n",
    "    function that defines a multilayer perceptron in the graph\n",
    "    input shape: parse_steps (=?) x filtered_words (=3) x lstm_output_length (=400)\n",
    "    output shape: parse_steps (=?) x num_classes\n",
    "    \"\"\"\n",
    "    # ReLU hidden layer (output shape: parse_steps x n_hidden)\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.einsum('ijk,kl->il', _X, _weights['h']), _biases['b'])) \n",
    "    # return output layer (output shape: parse_steps x n_classes)\n",
    "    return tf.add(tf.einsum('ik,kl->il', layer_1, _weights['out']), _biases['out'])\n",
    "\n",
    "def embedding_lookup(sentences, max_seq_length, vec_length):\n",
    "    \"\"\"\n",
    "    function that looks up embeddings.\n",
    "    input: list of sentences, length of sentences, length of word vectors\n",
    "    output: 3D array of word vectors per sentence \n",
    "                (dims #sentences x sentence_length x embedding_size)\n",
    "    \"\"\"\n",
    "    sentence_embeddings = np.empty((0,max_seq_length,vec_length))\n",
    "    for sentence in sentences:\n",
    "        word_embeddings = np.empty((0,vec_length))\n",
    "        for word in sentence:\n",
    "            word_embeddings = np.vstack([word_embeddings, model[word]])\n",
    "        if len(sentence) < max_seq_length:\n",
    "            zero_padding_length = max_seq_length - len(sentence)\n",
    "            word_embeddings = np.vstack([word_embeddings, np.zeros((zero_padding_length, vec_length))])\n",
    "        sentence_embeddings = np.append(sentence_embeddings, np.array([word_embeddings]), axis=0)\n",
    "    return sentence_embeddings\n",
    "\n",
    "        \n",
    "def data_iterator():\n",
    "    \"\"\"\n",
    "    function that iterates over the data and creates batches\n",
    "    \"\"\"\n",
    "    batch_idx = 0\n",
    "    while True:\n",
    "        idxs = np.arange(0, len(train_sentences))\n",
    "        for batch_idx in range(0, len(train_sentences), batch_size):\n",
    "            sentence_batch = train_sentences[batch_idx:batch_idx+batch_size]\n",
    "            parse_batch = action_data[batch_idx:batch_idx+batch_size]\n",
    "            yield sentence_batch, parse_batch\n",
    "\n",
    "def create_sentence_embeddings(sentences):\n",
    "    \"\"\"\n",
    "    for each sentence, get embedded representation\n",
    "    \"\"\"\n",
    "    embedded_train_sentences = []\n",
    "    for sentence in sentences:\n",
    "        embed = model[sentence]\n",
    "        embedded_train_sentences.append(embed)\n",
    "    return embedded_train_sentences\n",
    "\n",
    "def get_output_array(indices_and_actions):\n",
    "    max_len = 0\n",
    "    no_sent = 0\n",
    "    for sent in indices_and_actions:\n",
    "        no_sent +=1\n",
    "        if len(sent) > max_len:\n",
    "            max_len = len(sent)\n",
    "    \n",
    "    input_indices = np.empty((no_sent,max_len,3),dtype=np.int64) # ,dtype=np.ndarray\n",
    "    input_indices.fill(0)\n",
    "\n",
    "    output_actions = np.empty((no_sent,max_len),dtype=np.int64)\n",
    "    output_actions.fill(0)\n",
    "    \n",
    "    sentence_count = 0\n",
    "    for sent in indices_and_actions:\n",
    "        action_pair_count = 0\n",
    "        for action_pair in sent:\n",
    "            output_actions[sentence_count,action_pair_count] = action_pair[-1]\n",
    "            index_count = 0\n",
    "            indices = action_pair[:-1]\n",
    "            for ind in indices:\n",
    "                input_indices[sentence_count,action_pair_count,index_count] = ind\n",
    "                index_count += 1\n",
    "            action_pair_count += 1\n",
    "        sentence_count += 1\n",
    "    return input_indices, output_actions\n",
    "\n",
    "def get_input_array(unpadded_word2vec_sentences):\n",
    "    \"\"\"\n",
    "    returns a list with padded np arrays for every word2vec sentence\n",
    "    \"\"\"\n",
    "    padded_sentences = []\n",
    "    for unpad_sentence in unpadded_word2vec_sentences:\n",
    "        unpad_length = unpad_sentence.shape\n",
    "        pad_array = np.empty((max_seq_length - unpad_length[0], word2vec_length),dtype=np.float64)\n",
    "        pad_array.fill(0)\n",
    "        padded_sentences.append(np.concatenate((unpad_sentence,pad_array)))\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# load word2vec model\n",
    "##############################\n",
    "\n",
    "model_name = \"dep_parser_word2vec_total\"\n",
    "model = word2vec.Word2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass size:  (10, ?, 200)\n",
      "BiLSTM output shape:  (10, ?, 400)\n",
      "BiLSTM+padding output shape:  (10, ?, 400)\n",
      "Parse indices shape:  Tensor(\"Placeholder_2:0\", shape=(10, ?, 3), dtype=int64)\n",
      "MLP input shape:  (?, 3, 400)\n",
      "MLP output shape S{}:  (?, 99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lucas/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length in batch:    49\n",
      "max no of parse steps in batch:  98\n",
      "input_indices shape:  (10, 98, 3)\n",
      "current loss:  10868.6296756\n",
      "max sentence length in batch:    48\n",
      "max no of parse steps in batch:  96\n",
      "input_indices shape:  (10, 96, 3)\n",
      "current loss:  16441.712323\n",
      "max sentence length in batch:    50\n",
      "max no of parse steps in batch:  100\n",
      "input_indices shape:  (10, 100, 3)\n",
      "current loss:  13568.1771227\n",
      "max sentence length in batch:    40\n",
      "max no of parse steps in batch:  80\n",
      "input_indices shape:  (10, 80, 3)\n",
      "current loss:  12784.2630363\n",
      "max sentence length in batch:    36\n",
      "max no of parse steps in batch:  72\n",
      "input_indices shape:  (10, 72, 3)\n",
      "current loss:  13145.098107\n",
      "max sentence length in batch:    58\n",
      "max no of parse steps in batch:  116\n",
      "input_indices shape:  (10, 116, 3)\n",
      "current loss:  13595.5391693\n",
      "max sentence length in batch:    58\n",
      "max no of parse steps in batch:  116\n",
      "input_indices shape:  (10, 116, 3)\n",
      "current loss:  16577.5476863\n",
      "max sentence length in batch:    53\n",
      "max no of parse steps in batch:  106\n",
      "input_indices shape:  (10, 106, 3)\n",
      "current loss:  12493.5927823\n",
      "max sentence length in batch:    49\n",
      "max no of parse steps in batch:  98\n",
      "input_indices shape:  (10, 98, 3)\n",
      "current loss:  9957.11847313\n",
      "max sentence length in batch:    47\n",
      "max no of parse steps in batch:  94\n",
      "input_indices shape:  (10, 94, 3)\n",
      "current loss:  11456.52639\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "# TensorFlow model\n",
    "##############################\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # hyperparameters (from Cross & Huang, 2016)\n",
    "    word2vec_length = model['a'].size\n",
    "    batch_size = 10\n",
    "    n_input = 400\n",
    "    n_hidden = 200\n",
    "    n_classes = 99 # there are 99 possible actions to take\n",
    "    lstm_units = 200\n",
    "    num_epochs = 1 # TODO: should be 10, is 1 only for testing\n",
    "    dropout = 0.5\n",
    "    L2_penalty = 0.\n",
    "    rho = 0.99\n",
    "    epsilon = 1e-07\n",
    "    learn_rate = 0.01\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h': tf.Variable(tf.random_normal([n_input, n_hidden], dtype=tf.float64)),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], dtype=tf.float64))\n",
    "    }\n",
    "    biases = {\n",
    "        'b': tf.Variable(tf.random_normal([n_hidden], dtype=tf.float64)),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes], dtype=tf.float64))\n",
    "    }\n",
    "\n",
    "    \n",
    "    # placeholders\n",
    "    sentence_lengths = tf.placeholder(tf.int64, [batch_size])\n",
    "    lstm_x = tf.placeholder(tf.float64, [batch_size, None, word2vec_length])\n",
    "    parse_indices = tf.placeholder(tf.int64, [batch_size, None, 3])\n",
    "    labels = tf.placeholder(tf.int64, [batch_size, None])\n",
    "    \n",
    "    \n",
    "    # define LSTM cell + dropout wrapper (like Cross & Huang)\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=lstm_units, state_is_tuple=True)\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=dropout)\n",
    "\n",
    "    # define bidirectional architecture\n",
    "    outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=cell,\n",
    "        cell_bw=cell,\n",
    "        dtype=tf.float64,\n",
    "        sequence_length=sentence_lengths,\n",
    "        inputs=lstm_x\n",
    "    )\n",
    "\n",
    "    # fw/bw output (num_sequences x max_seq_length x lstm_units)\n",
    "    output_fw, output_bw = outputs\n",
    "    print(\"Forward pass size: \", output_fw.get_shape())\n",
    "\n",
    "    # concatenate forward & backward outputs per word\n",
    "    output_lstm = tf.concat(2, outputs)\n",
    "    print(\"BiLSTM output shape: \", output_lstm.get_shape())\n",
    "    \n",
    "    # zero-padding of LSTM-output (every sentence gets a \"dummy word\" in front of it)\n",
    "    zero_padding = tf.zeros([batch_size, 1, n_input], tf.float64)\n",
    "    output_lstm = tf.concat(1, [zero_padding, output_lstm])\n",
    "    print(\"BiLSTM+padding output shape: \", output_lstm.get_shape())\n",
    "    \n",
    "    print(\"Parse indices shape: \", parse_indices)\n",
    "    \n",
    "    # compute the batch cost\n",
    "    batch_cost = 0.\n",
    "    for i in range(batch_size):\n",
    "        mlp_x = tf.nn.embedding_lookup(output_lstm[i,:,:], parse_indices[i,:,:])\n",
    "        output_mlp = mlp(mlp_x, weights, biases)\n",
    "        \n",
    "        if i < 1:\n",
    "            print(\"MLP input shape: \", mlp_x.get_shape())\n",
    "            print(\"MLP output shape S{}: \", output_mlp.get_shape())\n",
    "        \n",
    "        cost = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(output_mlp, labels[i]))\n",
    "        batch_cost += cost\n",
    "    batch_cost /= batch_size\n",
    "    \n",
    "    train_op = tf.train.AdadeltaOptimizer(rho=rho, epsilon=epsilon).minimize(batch_cost)\n",
    "\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        init = tf.initialize_all_variables()\n",
    "        session.run(init)\n",
    "        \n",
    "        # TODO: this iterator might cause trouble at the end (going \n",
    "        # out of bounds or skipping the last cases)\n",
    "        iter_ = data_iterator()\n",
    "\n",
    "        for epoch in range(0,num_epochs): \n",
    "            for i in range(0,10): # TODO: this should be a batch range over all sentences; use \"while True\"?\n",
    "        #     while True:\n",
    "                \n",
    "                # get a batch of the data\n",
    "                sentence_batch, parse_batch = iter_.__next__()\n",
    "\n",
    "                # get the word2vec embedding version for all trainings sentences\n",
    "                embedded_train_sentences = create_sentence_embeddings(sentence_batch)\n",
    "\n",
    "                # stack+buffer indices need to be put in a padded numpy array first\n",
    "                input_indices, output_actions = get_output_array(parse_batch)\n",
    "\n",
    "                # important batch variables: \n",
    "                seq_lengths = [len(sentence) for sentence in sentence_batch]\n",
    "                max_seq_length = max(seq_lengths)\n",
    "                print(\"max sentence length in batch:   \", max_seq_length)\n",
    "                print(\"max no of parse steps in batch: \", 2*max_seq_length)\n",
    "\n",
    "                padded_word2vec_train = get_input_array(embedded_train_sentences)\n",
    "\n",
    "                print(\"input_indices shape: \", input_indices[0:10].shape)\n",
    "                feed_dict_batch = {sentence_lengths: seq_lengths, lstm_x: padded_word2vec_train,  parse_indices: input_indices, labels: output_actions}\n",
    "                result = session.run([train_op, batch_cost], feed_dict_batch)\n",
    "                print(\"current loss: \", result[1])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check_index = 1\n",
    "print(embedded_train_sentences[check_index].shape)\n",
    "print(len(embedded_train_sentences[check_index]))\n",
    "# print(embedded_train_sentences[check_index])\n",
    "print(type(embedded_train_sentences[check_index][0][0]))\n",
    "for word in train_sentences[check_index]:\n",
    "    print(word)\n",
    "# for emb in embedded_train_sentences[check_index]:\n",
    "#     print(emb)\n",
    "# for action in action_data[check_index]:\n",
    "#     print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### OUD OUD OUD\n",
    "\n",
    "##############################\n",
    "# build graph\n",
    "##############################\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # hyperparameters (from Cross & Huang, 2016)\n",
    "    batch_size = 10\n",
    "    n_input = 400\n",
    "    n_hidden = 200\n",
    "    n_classes = 99 # there are 99 possible actions to take\n",
    "    lstm_units = 200\n",
    "    num_epochs = 10\n",
    "    dropout = 0.5\n",
    "    L2_penalty = 0.\n",
    "    rho = 0.99\n",
    "    epsilon = 1e-07\n",
    "    learn_rate = 0.01\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h': tf.Variable(tf.random_normal([n_input, n_hidden], dtype=tf.float64)),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], dtype=tf.float64))\n",
    "    }\n",
    "    biases = {\n",
    "        'b': tf.Variable(tf.random_normal([n_hidden], dtype=tf.float64)),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes], dtype=tf.float64))\n",
    "    }\n",
    "\n",
    "    # load word2vec model\n",
    "    model_name = \"dep_parser_word2vec_total\"\n",
    "    if not 'model' in locals():\n",
    "        model = models.word2vec.Word2Vec.load(model_name)\n",
    "        print(\"Model loaded from disk\")\n",
    "    else:\n",
    "        print(\"Model was already loaded\")\n",
    "\n",
    "    # placeholders for input (input tensor uses None as variable sized dimension, \n",
    "    # or can use rows of train set length, or batch length might be better)\n",
    "    lstm_x = tf.placeholder(\"float\", [None, max_seq_length, word2vec_length])\n",
    "    # lstm output (1 pass)\n",
    "    lstm_y = tf.placeholder(\"float\", [None, max_seq_length, n_hidden])\n",
    "    # mlp input (3 words)\n",
    "    mlp_x = tf.placeholder(\"float\", [3, 2*n_hidden])\n",
    "    # mlp output\n",
    "    mlp_y = tf.placeholder(\"float\", [n_classes])\n",
    "    \n",
    "    \n",
    "    # dummy data:\n",
    "    # toy sentences (= list of lists of words)\n",
    "#     sentences = [[\"the\", \"by\", \"an\", \"on\", \"the\", \"in\", \"an\"], [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"ground\"]]\n",
    "#     toy_sentences = train_sentences[0:30]\n",
    "#     toy_parses = action_data[0:30]\n",
    "    # toy parses (= array with an action and word indices)\n",
    "#     parses = [np.array([[1,0,1,2], \n",
    "#                         [0,1,2,3],\n",
    "#                         [0,1,2,3],\n",
    "#                         [1,0,1,2]]),\n",
    "#               np.array([[1,0,1,2], \n",
    "#                         [0,1,2,3], \n",
    "#                         [3,2,3,4]])]\n",
    "    \n",
    "    # look up embeddings of words in sentence\n",
    "#     toy_embeddings = np.array(embedded_train_sentences[0:30])\n",
    "#     embeddings = embedding_lookup(toy_sentences, max_seq_length, vec_length)\n",
    "#     embeddings = embedded_train_sentences[0:30]\n",
    "#     print(\"Sentence embedding shape (np-array): \", embeddings.shape)\n",
    "\n",
    "    # define LSTM cell + dropout wrapper (like Cross & Huang)\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=lstm_units, state_is_tuple=True)\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=0.5)\n",
    "\n",
    "    # define bidirectional architecture\n",
    "    outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=cell,\n",
    "        cell_bw=cell,\n",
    "        dtype=tf.float64,\n",
    "        sequence_length=seq_lengths,\n",
    "        inputs=lstm_x\n",
    "    )\n",
    "\n",
    "    # fw/bw output (num_sequences x max_seq_length x lstm_units)\n",
    "    output_fw, output_bw = outputs\n",
    "    print(\"Forward pass size: \", output_fw.get_shape())\n",
    "\n",
    "    # concatenate forward & backward outputs per word\n",
    "    output_lstm = tf.concat(2, outputs)\n",
    "    print(\"BiLSTM output shape: \", output_lstm.get_shape())\n",
    "\n",
    "\n",
    "    # MLP layer filtering LSTM outputs based on parse steps\n",
    "    batch_cost = 0\n",
    "    for i in range(0, len(toy_sentences)):\n",
    "        action = action_data[i]\n",
    "#         sentence_input_indices = action[0:5]\n",
    "        print(input_indices)\n",
    "        output_action_index = action[-1]\n",
    "        print(output_action_index)\n",
    "        sentence = toy_sentences[i]\n",
    "#         print(len(sentence))\n",
    "        emb = embeddings[i]\n",
    "#         print(emb.shape())\n",
    "        parse = toy_parses[i]\n",
    "        # input: parse_steps (=?) x filtered_words (=3) x lstm_output_length (=400)\n",
    "        # output: parse_steps (=?) x num_classes\n",
    "        output_mlp = mlp(tf.gather(emb, parse), weights, biases)\n",
    "        print(\"MLP output shape S{}: \".format(i), output_mlp.get_shape())\n",
    "        cost = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(output_mlp, parse[:,0]))\n",
    "        batch_cost += cost\n",
    "    batch_cost /= len(sentences)\n",
    "\n",
    "    # define optimizer\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=True)\n",
    "    optimizer = tf.train.AdadeltaOptimizer(rho=rho, epsilon=epsilon).minimize(batch_cost)\n",
    "#     train_op = optimizer.train(loss, global_step=global_step)\n",
    "    correct = tf.nn.in_top_k(embeddings[0], parses[0], 1)\n",
    "    print(correct)\n",
    "\n",
    "    \n",
    "    ##############################\n",
    "    # start session in graph\n",
    "    ##############################\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
