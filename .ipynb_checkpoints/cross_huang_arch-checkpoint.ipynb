{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "# import modules\n",
    "##############################\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# define functions\n",
    "##############################\n",
    "\n",
    "def length(sequence):\n",
    "    \"\"\"\n",
    "    function that computes the real, unpadded lenghts for every sequence in batch\n",
    "    \"\"\"\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "\n",
    "def mlp(_X, _weights, _biases):\n",
    "    \"\"\"\n",
    "    function that defines a multilayer perceptron in the graph\n",
    "    input shape: parse_steps (=?) x filtered_words (=3) x lstm_output_length (=400)\n",
    "    output shape: parse_steps (=?) x num_classes\n",
    "    \"\"\"\n",
    "    # ReLU hidden layer (output shape: parse_steps x n_hidden)\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.einsum('ijk,kl->il', _X, _weights['h']), _biases['b'])) \n",
    "    # return output layer (output shape: parse_steps x n_classes)\n",
    "    return tf.add(tf.einsum('ik,kl->il', layer_1, _weights['out']), _biases['out'])\n",
    "\n",
    "def embedding_lookup(sentences, max_seq_length, vec_length):\n",
    "    \"\"\"\n",
    "    function that looks up embeddings.\n",
    "    input: list of sentences, length of sentences, length of word vectors\n",
    "    output: 3D array of word vectors per sentence \n",
    "                (dims #sentences x sentence_length x embedding_size)\n",
    "    \"\"\"\n",
    "    sentence_embeddings = np.empty((0,max_seq_length,vec_length))\n",
    "    for sentence in sentences:\n",
    "        word_embeddings = np.empty((0,vec_length))\n",
    "        for word in sentence:\n",
    "            word_embeddings = np.vstack([word_embeddings, model[word]])\n",
    "        if len(sentence) < max_seq_length:\n",
    "            zero_padding_length = max_seq_length - len(sentence)\n",
    "            word_embeddings = np.vstack([word_embeddings, np.zeros((zero_padding_length, vec_length))])\n",
    "        sentence_embeddings = np.append(sentence_embeddings, np.array([word_embeddings]), axis=0)\n",
    "    return sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was already loaded\n",
      "Sentence embedding shape (np-array):  (2, 7, 189)\n",
      "BiLSTM output shape:  (2, 7, 400)\n",
      "MLP output shape S0:  (4, 50)\n",
      "MLP output shape S1:  (3, 50)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 1 but is rank 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, debug_python_shape_fn)\u001b[0m\n\u001b[1;32m    593\u001b[0m                                                              \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                                                              status)\n\u001b[0m\u001b[1;32m    595\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/errors.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    462\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    464\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 1 but is rank 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2c2806e0b856>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdadeltaOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m#     train_op = optimizer.train(loss, global_step=global_step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_top_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36min_top_k\u001b[0;34m(predictions, targets, k, name)\u001b[0m\n\u001b[1;32m   1288\u001b[0m   \"\"\"\n\u001b[1;32m   1289\u001b[0m   result = _op_def_lib.apply_op(\"InTopK\", predictions=predictions,\n\u001b[0;32m-> 1290\u001b[0;31m                                 targets=targets, k=k, name=name)\n\u001b[0m\u001b[1;32m   1291\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    747\u001b[0m           op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    748\u001b[0m                            \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m                            op_def=op_def)\n\u001b[0m\u001b[1;32m    750\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m           return _Restructure(ops.convert_n_to_tensor(outputs),\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2380\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2383\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1781\u001b[0m       raise RuntimeError(\"No shape function registered for standard op: %s\"\n\u001b[1;32m   1782\u001b[0m                          % op.type)\n\u001b[0;32m-> 1783\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, debug_python_shape_fn)\u001b[0m\n\u001b[1;32m    594\u001b[0m                                                              status)\n\u001b[1;32m    595\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m   \u001b[0;31m# Convert TensorShapeProto values in output_shapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 1 but is rank 2"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ik (Lucas) staar me er een beetje blind op nu, dus Niels, zou jij \n",
    "kunnen nadenken over onderstaande punten en sowieso even naar de \n",
    "code kunnen kijken?\n",
    "\n",
    "TODO:\n",
    "\n",
    "- Ik ga er nu vanuit dat ik een lijst van zinnen en een lijst van parses\n",
    "terugkrijg. Ik weet niet goed hoe ik die moet opdelen in batches voor\n",
    "training. De dummy data die ik nu gebruik moet een batch voorstellen.\n",
    "\n",
    "- Tweede, gerelateerde issue: het model gaat nu er basically vanuit dat\n",
    "het een batch ontvangt. Ik denk dus dat het het beste is om een groot\n",
    "deel van de onderstaande code waarin ik de graph define (dus voordat\n",
    "de session begint) te wrappen in een functie (de \"train_op\") met \n",
    "tf.placeholders die via een feed_dict worden gevuld met de huidige batch.\n",
    "\n",
    "- Als ik vectoren probeer op te halen uit het model, maar dan voor \n",
    "woorden die er niet inzitten, dan krijg ik een error. Dit moeten we \n",
    "op een of andere manier zien te fixen.\n",
    "\n",
    "- Ik zit even vast en weet niet goed hoe ik de uiteindelijke session\n",
    "(= voornamelijk de training loop) die de graph runt, moet opbouwen. Er\n",
    "staan voorbeelden van online, maar ik kan ze nog niet concreet toepassen\n",
    "omdat ik nog geen concrete input data heb, en omdat ik nog niet weet hoe\n",
    "ik batches maak/afhandel. DIT IS EEN BELANGRIJK PUNT NOG!!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "##############################\n",
    "# build graph\n",
    "##############################\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # hyperparameters (from Cross & Huang, 2016)\n",
    "    batch_size = 10\n",
    "    n_input = 400\n",
    "    n_hidden = 200\n",
    "    n_classes = 50 # TODO: how many classes for the actual data?\n",
    "    lstm_units = 200\n",
    "    num_epochs = 10\n",
    "    dropout = 0.5\n",
    "    L2_penalty = 0.\n",
    "    rho = 0.99\n",
    "    epsilon = 1e-07\n",
    "    learn_rate = 0.01\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h': tf.Variable(tf.random_normal([n_input, n_hidden], dtype=tf.float64)),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], dtype=tf.float64))\n",
    "    }\n",
    "    biases = {\n",
    "        'b': tf.Variable(tf.random_normal([n_hidden], dtype=tf.float64)),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes], dtype=tf.float64))\n",
    "    }\n",
    "\n",
    "    # load word2vec model\n",
    "    model_name = \"dep_parser_word2vec_total\"\n",
    "    if not 'model' in locals():\n",
    "        model = models.word2vec.Word2Vec.load(model_name)\n",
    "        print(\"Model loaded from disk\")\n",
    "    else:\n",
    "        print(\"Model was already loaded\")\n",
    "\n",
    "    \n",
    "    # dummy data:\n",
    "    # toy sentences (= list of lists of words)\n",
    "    sentences = [[\"the\", \"by\", \"an\", \"on\", \"the\", \"in\", \"an\"], [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"ground\"]]\n",
    "    \n",
    "    # toy parses (= array with an action and word indices)\n",
    "    parses = [np.array([[1,0,1,2], \n",
    "                        [0,1,2,3],\n",
    "                        [0,1,2,3],\n",
    "                        [1,0,1,2]]),\n",
    "              np.array([[1,0,1,2], \n",
    "                        [0,1,2,3], \n",
    "                        [3,2,3,4]])]\n",
    "\n",
    "    # variables from sentences\n",
    "    vec_length = model['a'].size\n",
    "    seq_lengths = [len(sentence) for sentence in sentences]\n",
    "    max_seq_length = max(seq_lengths)\n",
    "    \n",
    "\n",
    "    # look up embeddings of words in sentence\n",
    "    embeddings = embedding_lookup(sentences, max_seq_length, vec_length)\n",
    "    print(\"Sentence embedding shape (np-array): \", embeddings.shape)\n",
    "\n",
    "    # define LSTM cell + dropout wrapper (like Cross & Huang)\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=lstm_units, state_is_tuple=True)\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=0.5)\n",
    "\n",
    "    # define bidirectional architecture\n",
    "    outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=cell,\n",
    "        cell_bw=cell,\n",
    "        dtype=tf.float64,\n",
    "        sequence_length=seq_lengths,\n",
    "        inputs=embeddings\n",
    "    )\n",
    "\n",
    "    # fw/bw output (num_sequences x max_seq_length x lstm_units)\n",
    "    output_fw, output_bw = outputs\n",
    "\n",
    "    # concatenate forward & backward outputs per word\n",
    "    output_lstm = tf.concat(2, outputs)\n",
    "    print(\"BiLSTM output shape: \", output_lstm.get_shape())\n",
    "\n",
    "\n",
    "    # MLP layer filtering LSTM outputs based on parse steps\n",
    "    batch_cost = 0\n",
    "    for i in range(0, len(sentences)):\n",
    "        sentence, parse = output_lstm[i], parses[i]\n",
    "        # input: parse_steps (=?) x filtered_words (=3) x lstm_output_length (=400)\n",
    "        # output: parse_steps (=?) x num_classes\n",
    "        output_mlp = mlp(tf.gather(sentence, parse[:,1:]), weights, biases)\n",
    "        print(\"MLP output shape S{}: \".format(i), output_mlp.get_shape())\n",
    "        cost = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(output_mlp, parse[:,0]))\n",
    "        batch_cost += cost\n",
    "    batch_cost /= len(sentences)\n",
    "\n",
    "    # define optimizer\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=True)\n",
    "    optimizer = tf.train.AdadeltaOptimizer(rho=rho, epsilon=epsilon).minimize(batch_cost)\n",
    "#     train_op = optimizer.train(loss, global_step=global_step)\n",
    "    correct = tf.nn.in_top_k(embeddings[0], parses[0], 1)\n",
    "    print(correct)\n",
    "\n",
    "    \n",
    "    ##############################\n",
    "    # start session in graph\n",
    "    ##############################\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        # TODO: HOE WERKT TRAINING?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
