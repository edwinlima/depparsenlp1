{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "# import modules\n",
    "##############################\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the file, \n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as myfile:\n",
    "        f = myfile.readlines()\n",
    "        s_num = 0\n",
    "        i =0\n",
    "        sentence_s = []\n",
    "        tag_s = []\n",
    "        dep_s = []\n",
    "        s  = []   # sentence\n",
    "        p = []    # tag\n",
    "        d = []    # dependency\n",
    "        for l in f:\n",
    "            \n",
    "            v = l.replace('\\n','').split(\"\\t\")\n",
    "            v.append(s_num)\n",
    "            if len(l) != 1:\n",
    "                data.append(v)\n",
    "                dep = v[6] + '_' + v[7]\n",
    "                word = v[1].lower()\n",
    "                if any(char.isdigit() for char in word):\n",
    "                    word = 'NUM'       # replace numbers with NUM\n",
    "                s.append(word)\n",
    "                p.append(v[3])\n",
    "                d.append(dep)\n",
    "                i +=1\n",
    "            else:\n",
    "                sentence_s.append(s)\n",
    "                tag_s.append(p)\n",
    "                dep_s.append(d)\n",
    "                s_num +=1\n",
    "                s  = []\n",
    "                p = []\n",
    "                d = []\n",
    "        \n",
    "    return data, sentence_s, tag_s, dep_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(dataname):\n",
    "    #reads in files, produces data structure with all actions\n",
    "        #does so by applying produce_rule_list to every sentence.\n",
    "        #for loop that sets actions to empty, calls p_r_l giving it\n",
    "        #the stack and buffer, actions and correct_parse, adds finished action list\n",
    "        #to new data file, for each sentence in the input data\n",
    "    #input: name of the data file with all parses. Run with data file in same directory.\n",
    "    #output: data file with all actions\n",
    "    file = open(dataname)\n",
    "    data = file.read()\n",
    "    correct_parses = correct_parse_list(data)\n",
    "    #gets rid of final whitespace\n",
    "    del correct_parses[len(correct_parses)-1]\n",
    "    \n",
    "    #iterates over all parses, producing action list for each\n",
    "    complete_rule_list = []\n",
    "    arc_dict = {'Shift':0,'L_root':1,'R_root':2}\n",
    "    for sentence_parse in correct_parses:\n",
    "        stack = []\n",
    "#         print(len(sentence_parse))\n",
    "        buff = list(range(1,len(sentence_parse)+1))\n",
    "        actions = []\n",
    "        rule_list, arc_dict = produce_rule_list(stack, buff, actions, sentence_parse, arc_dict)\n",
    "        complete_rule_list.append(np.array(rule_list))\n",
    "\n",
    "    \n",
    "    return complete_rule_list, arc_dict\n",
    "\n",
    "def correct_parse_list(data):\n",
    "    #Turns data into a list of lists of lists with relevant information\n",
    "    correct_parse = data.split(\"\\n\\n\")\n",
    "    for index, paragraph in enumerate(correct_parse):\n",
    "        correct_parse[index] = paragraph.split(\"\\n\")\n",
    "    for paragraph in correct_parse:\n",
    "        for index, line in enumerate(paragraph):\n",
    "            paragraph[index] = line.split(\"\\t\")\n",
    "    return correct_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_rule_list(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #recursive function that works through words in the sentence (stack/buffer)\n",
    "        #until only one word is left, creating the list of actions \n",
    "        #that was taken to parse it.\n",
    "    #input: stack, buffer, actions, correct parse\n",
    "    #output: actions with the actions taken for each buff/stack configuration\n",
    "    \n",
    "    #base case\n",
    "    if len(stack) == 1 and len(buff) == 0:\n",
    "        #actions.append([stack[:], \"empty\", \"R_arc\"])\n",
    "        actions.append([-1,stack[0], -1, 2])\n",
    "        return actions, arc_dict\n",
    "\n",
    "    #If enough of the sentence is still left:\n",
    "    #If there is not enough material in the stack, shift:\n",
    "    if len(stack) == 0 :\n",
    "        #print('chose S - small stack')\n",
    "        actions.append([-1,-1,buff[0], 0])\n",
    "        stack.append(buff[0])\n",
    "        del buff[0]        \n",
    "        return produce_rule_list(stack,buff,actions,sentence_parse, arc_dict)\n",
    "    if len(stack) == 1:\n",
    "        actions.append([-1,stack[-1],buff[0], 0])\n",
    "        stack.append(buff[0])\n",
    "        del buff[0]\n",
    "        return produce_rule_list(stack,buff,actions,sentence_parse, arc_dict)\n",
    "    #If there are 2 or more words in the stack, decide which action to perform and perform it\n",
    "    if len(stack) > 1:\n",
    "        action = rule_decision(stack,buff,sentence_parse)\n",
    "        stack, buff, actions, arc_dict = action(stack,buff,actions, sentence_parse, arc_dict)\n",
    "        return produce_rule_list(stack,buff,actions,sentence_parse, arc_dict)\n",
    "    \n",
    "\n",
    "def rule_decision(stack, buff, sentence_parse):\n",
    "    #determines which action to apply\n",
    "    #input: words on stack, words on buff, correct parse\n",
    "    #output: one of three methods, Shift(), L_arc(), R_arc()\n",
    "\n",
    "    #TODO: find ids/heads (index [6]) from stack and sentence_parse\n",
    "    s1 = stack[-2]\n",
    "    head_of_s1 = int(sentence_parse[s1-1][6])\n",
    "    s2 = stack[-1]\n",
    "    head_of_s2 = int(sentence_parse[s2-1][6])\n",
    "    \n",
    "    #L arcs can always be applied if possible\n",
    "    if head_of_s1 == s2:\n",
    "        action = L_arc\n",
    "        #print('chose L')\n",
    "    else:\n",
    "        #R arcs can only be applied if there is no word in the buffer which has the last word in the stack as a head\n",
    "        if head_of_s2 == s1:\n",
    "            buff_heads = [int(sentence_parse[x-1][6]) for x in buff]\n",
    "            if s2 in buff_heads:\n",
    "                action = Shift\n",
    "                #print('chose S - s2 in buffheads')\n",
    "            else:\n",
    "                action = R_arc\n",
    "                #print('chose R')\n",
    "        #if there is no match between s1 and s2, simply shift another word from the buffer\n",
    "        else:\n",
    "            action = Shift\n",
    "            #print('chose S - no matching s1s2')\n",
    "\n",
    "    return action\n",
    "\n",
    "#The following methods perform an arc or shift. These can be changed if more data is needed in the network.\n",
    "\n",
    "def L_arc(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #removes second to last item from stack, writes action to actions\n",
    "    #input: stack and actions\n",
    "    #output: new stack and actions with one L_arc line\n",
    "    #s1, s2, b1, action\n",
    "    s1 = int(stack[-2])\n",
    "    s2 = int(stack[-1])\n",
    "    b1 = int(stack[0])\n",
    "    relation = \"L_\"+sentence_parse[s1-1][7]\n",
    "\n",
    "    if relation not in arc_dict:\n",
    "        maximum = max(arc_dict, key=arc_dict.get)\n",
    "        arc_dict['L_'+relation[2:]] = arc_dict[maximum]+1\n",
    "        arc_dict['R_'+relation[2:]] = arc_dict[maximum]+2\n",
    "    \n",
    "\n",
    "    actions.append([s1,s2,b1, arc_dict[relation]])\n",
    "    del stack[-2]\n",
    "    return stack, buff, actions, arc_dict\n",
    "\n",
    "\n",
    "\n",
    "def R_arc(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #removes last item from the stack, writes action to actions\n",
    "    #input: stack and actions\n",
    "    #output: new stack and actions with one R_arc line\n",
    "    #s1, s2, b1, action\n",
    "    s1 = int(stack[-2])\n",
    "    s2 = int(stack[-1])\n",
    "    b1 = int(stack[0])\n",
    "    relation = \"R_\"+sentence_parse[s2-1][7]\n",
    "\n",
    "    if relation not in arc_dict:\n",
    "        maximum = max(arc_dict, key=arc_dict.get)\n",
    "        arc_dict['L_'+relation[2:]] = arc_dict[maximum]+1\n",
    "        arc_dict['R_'+relation[2:]] = arc_dict[maximum]+2 \n",
    "    \n",
    "    actions.append([s1,s2,b1, arc_dict[relation]])\n",
    "    del stack[-1]\n",
    "    return stack, buff, actions, arc_dict\n",
    "\n",
    "\n",
    "\n",
    "def Shift(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #moves an item from the buff to the stack, writes action to actions\n",
    "    #input: stack, buff and actions\n",
    "    #output: new stack and actions with one extra shift line\n",
    "    #s1, s2, b1, action\n",
    "    s1 = int(stack[-2])\n",
    "    s2 = int(stack[-1])\n",
    "    b1 = int(stack[0])\n",
    "    #actions.append([stack[:], buff[:], \"Shift\"])\n",
    "    actions.append([s1,s2,b1, 0])\n",
    "    stack.append(buff[0])\n",
    "    del buff[0]\n",
    "    return stack, buff, actions, arc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  [['1', 'In', '_', 'IN', 'IN', '_', '45', 'prep', '_', '_', 0], ['2', 'an', '_', 'DT', 'DT', '_', '5', 'det', '_', '_', 0]]\n",
      "words sentences:  [['rolls-royce', 'motor', 'cars', 'inc.', 'said', 'it', 'expects', 'its', 'u.s.', 'sales', 'to', 'remain', 'steady', 'at', 'about', 'NUM', 'cars', 'in', 'NUM', '.'], ['the', 'luxury', 'auto', 'maker', 'last', 'year', 'sold', 'NUM', 'cars', 'in', 'the', 'u.s.']]\n",
      "tags sentences:  [['NNP', 'NNP', 'NNPS', 'NNP', 'VBD', 'PRP', 'VBZ', 'PRP$', 'NNP', 'NNS', 'TO', 'VB', 'JJ', 'IN', 'IN', 'CD', 'NNS', 'IN', 'CD', '.'], ['DT', 'NN', 'NN', 'NN', 'JJ', 'NN', 'VBD', 'CD', 'NNS', 'IN', 'DT', 'NNP']]\n",
      "dependencies:  [['4_nn', '4_nn', '4_nn', '5_nsubj', '0_root', '7_nsubj', '5_ccomp', '10_poss', '10_nn', '12_nsubj', '12_aux', '7_xcomp', '12_acomp', '12_prep', '16_quantmod', '17_num', '14_pobj', '12_prep', '18_pobj', '5_punct'], ['4_det', '4_nn', '4_nn', '7_nsubj', '6_amod', '7_tmod', '0_root', '9_num', '7_dobj', '7_prep', '12_det', '10_pobj']]\n"
     ]
    }
   ],
   "source": [
    "train_data, train_sentences, train_tags, train_dependencies = read_data('./data/train-stanford-raw.conll')\n",
    "dev_data, dev_sentences, dev_tags, dev_dependencies = read_data('./data/dev-stanford-raw.conll')\n",
    "test_data, test_sentences, test_tags, test_dependencies = read_data('./data/test-stanford-raw.conll')\n",
    "\n",
    "# create a full set of all the words in our train, test, and dev sets for word2vec model\n",
    "# in order to avoid unseen words during test and validation\n",
    "total_sentences = train_sentences + dev_sentences + test_sentences\n",
    "print('data: ', train_data[:2])\n",
    "print('words sentences: ', total_sentences[2:4])\n",
    "print('tags sentences: ', train_tags[2:4])\n",
    "print('dependencies: ', train_dependencies[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action_data, arc_dict = process_data('./data/train-stanford-raw.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R_conj': 28, 'R_num': 4, 'R_acomp': 42, 'L_number': 79, 'L_num': 3, 'R_pcomp': 60, 'R_poss': 14, 'R_abbrev': 94, 'R_predet': 72, 'R_auxpass': 36, 'R_complm': 64, 'R_dobj': 24, 'L_rcmod': 61, 'R_advmod': 34, 'L_acomp': 41, 'L_conj': 27, 'L_purpcl': 89, 'L_cc': 25, 'R_advcl': 70, 'R_rcmod': 62, 'R_det': 8, 'R_parataxis': 66, 'L_amod': 19, 'L_possessive': 11, 'R_purpcl': 90, 'R_rel': 86, 'R_nsubj': 22, 'R_number': 80, 'L_abbrev': 93, 'L_nsubj': 21, 'L_mark': 67, 'L_xcomp': 45, 'R_pobj': 16, 'R_csubj': 74, 'L_parataxis': 65, 'R_possessive': 12, 'R_appos': 52, 'L_poss': 13, 'R_partmod': 32, 'L_rel': 85, 'L_csubj': 73, 'R_mark': 68, 'R_punct': 10, 'L_partmod': 31, 'L_complm': 63, 'R_ccomp': 48, 'L_auxpass': 35, 'R_attr': 78, 'L_appos': 51, 'L_iobj': 83, 'L_punct': 9, 'L_quantmod': 43, 'R_nn': 6, 'L_cop': 95, 'L_csubjpass': 97, 'L_det': 7, 'R_infmod': 58, 'L_pobj': 15, 'L_advcl': 69, 'R_mwe': 76, 'R_dep': 30, 'Shift': 0, 'L_expl': 87, 'L_advmod': 33, 'L_attr': 77, 'L_aux': 39, 'R_xcomp': 46, 'R_preconj': 92, 'R_amod': 20, 'R_npadvmod': 54, 'L_prep': 17, 'L_dobj': 23, 'R_cc': 26, 'R_aux': 40, 'R_expl': 88, 'R_neg': 56, 'L_root': 1, 'R_iobj': 84, 'L_prt': 81, 'R_nsubjpass': 38, 'R_quantmod': 44, 'L_predet': 71, 'L_infmod': 57, 'R_tmod': 50, 'L_nn': 5, 'L_pcomp': 59, 'L_neg': 55, 'R_csubjpass': 98, 'L_dep': 29, 'L_preconj': 91, 'R_prep': 18, 'L_ccomp': 47, 'L_nsubjpass': 37, 'R_root': 2, 'R_cop': 96, 'L_mwe': 75, 'L_tmod': 49, 'L_npadvmod': 53, 'R_prt': 82}\n",
      "[-1 -1  1  0]\n",
      "[-1  1  2  0]\n",
      "[1 2 1 5]\n",
      "[-1  2  3  0]\n",
      "[ 2  3  2 21]\n",
      "[-1  3  4  0]\n",
      "[ 3  4  3 24]\n",
      "[-1  3  5  0]\n",
      "[ 3  5  3 10]\n",
      "[-1  3 -1  2]\n"
     ]
    }
   ],
   "source": [
    "print(arc_dict)\n",
    "for line in action_data[1]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# TF functions\n",
    "##############################\n",
    "\n",
    "def length(sequence):\n",
    "    \"\"\"\n",
    "    function that computes the real, unpadded lenghts for every sequence in batch\n",
    "    \"\"\"\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "\n",
    "def mlp(_X, _weights, _biases):\n",
    "    \"\"\"\n",
    "    function that defines a multilayer perceptron in the graph\n",
    "    input shape: parse_steps (=?) x filtered_words (=3) x lstm_output_length (=400)\n",
    "    output shape: parse_steps (=?) x num_classes\n",
    "    \"\"\"\n",
    "    # ReLU hidden layer (output shape: parse_steps x n_hidden)\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.einsum('ijk,kl->il', _X, _weights['h']), _biases['b'])) \n",
    "    # return output layer (output shape: parse_steps x n_classes)\n",
    "    return tf.add(tf.einsum('ik,kl->il', layer_1, _weights['out']), _biases['out'])\n",
    "\n",
    "def embedding_lookup(sentences, max_seq_length, vec_length):\n",
    "    \"\"\"\n",
    "    function that looks up embeddings.\n",
    "    input: list of sentences, length of sentences, length of word vectors\n",
    "    output: 3D array of word vectors per sentence \n",
    "                (dims #sentences x sentence_length x embedding_size)\n",
    "    \"\"\"\n",
    "    sentence_embeddings = np.empty((0,max_seq_length,vec_length))\n",
    "    for sentence in sentences:\n",
    "        word_embeddings = np.empty((0,vec_length))\n",
    "        for word in sentence:\n",
    "            word_embeddings = np.vstack([word_embeddings, model[word]])\n",
    "        if len(sentence) < max_seq_length:\n",
    "            zero_padding_length = max_seq_length - len(sentence)\n",
    "            word_embeddings = np.vstack([word_embeddings, np.zeros((zero_padding_length, vec_length))])\n",
    "        sentence_embeddings = np.append(sentence_embeddings, np.array([word_embeddings]), axis=0)\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model first\n",
    "model_name = \"dep_parser_word2vec_total\"\n",
    "model = word2vec.Word2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# create training data using parser\n",
    "# for each sentence, get embedded representation\n",
    "# and actions ONLY when needed\n",
    "##############################\n",
    "\n",
    "def create_sentence_embeddings(sentences):\n",
    "    embedded_train_sentences = []\n",
    "    for sentence in sentences:\n",
    "        embed = model[sentence]\n",
    "#         print(embed)\n",
    "        embedded_train_sentences.append(embed)\n",
    "    return embedded_train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the word2vec embedding version for all trainings sentences\n",
    "embedded_train_sentences = create_sentence_embeddings(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_output_array(indices_and_actions):\n",
    "    max_len = 0\n",
    "    no_sent = 0\n",
    "    for sent in indices_and_actions:\n",
    "        no_sent +=1\n",
    "        if len(sent) > max_len:\n",
    "            max_len = len(sent)\n",
    "            print(max_len)\n",
    "    \n",
    "    input_indices = np.empty((no_sent,max_len,3),dtype=np.int64) # ,dtype=np.ndarray\n",
    "    input_indices.fill(-2)\n",
    "\n",
    "    output_actions = np.empty((no_sent,max_len),dtype=np.int64)\n",
    "    output_actions.fill(-1)\n",
    "    \n",
    "    sentence_count = 0\n",
    "    for sent in indices_and_actions:\n",
    "        action_pair_count = 0\n",
    "        for action_pair in sent:\n",
    "            output_actions[sentence_count,action_pair_count] = action_pair[-1]\n",
    "            index_count = 0\n",
    "            indices = action_pair[:-1]\n",
    "            for ind in indices:\n",
    "                input_indices[sentence_count,action_pair_count,index_count] = ind\n",
    "                index_count += 1\n",
    "            action_pair_count += 1\n",
    "        sentence_count += 1\n",
    "    return input_indices, output_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "98\n",
      "100\n",
      "116\n",
      "118\n",
      "128\n",
      "136\n",
      "198\n",
      "216\n",
      "238\n",
      "282\n",
      "(39832, 282)\n",
      "(39832, 282, 3)\n"
     ]
    }
   ],
   "source": [
    "# convert data to tensors\n",
    "# stack+buffer indices need to be put in a padded numpy array first\n",
    "input_indices, output_actions = get_output_array(action_data)\n",
    "\n",
    "# print(input_indices[1])\n",
    "\n",
    "# now convert to tensor\n",
    "output_actions_tensor = tf.convert_to_tensor(output_actions)\n",
    "input_indices_tensor = tf.convert_to_tensor(input_indices)\n",
    "print(output_actions_tensor.get_shape())\n",
    "print(input_indices_tensor.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec vector length:  189\n",
      "max sentence length:  141\n",
      "no. of training sentences:  39832\n",
      "--------------------\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# create some variables for TF\n",
    "word2vec_length = model['a'].size\n",
    "seq_lengths = [len(sentence) for sentence in train_sentences]\n",
    "max_seq_length = max(seq_lengths)\n",
    "train_set_length = len(train_sentences)\n",
    "print(\"word2vec vector length: \", word2vec_length)\n",
    "print(\"max sentence length: \", max_seq_length)\n",
    "print(\"no. of training sentences: \", train_set_length)\n",
    "print(\"--------------------\")\n",
    "print(type(seq_lengths[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns a list with padded np arrays for every word2vec sentence\n",
    "def get_input_array(unpadded_word2vec_sentences):\n",
    "    padded_sentences = []\n",
    "    for unpad_sentence in unpadded_word2vec_sentences:\n",
    "        unpad_length = unpad_sentence.shape\n",
    "        pad_array = np.empty((max_seq_length - unpad_length[0], word2vec_length),dtype=np.float64)\n",
    "        pad_array.fill(-2.)\n",
    "        padded_sentences.append(np.concatenate((unpad_sentence,pad_array)))\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding word2vec sentences... this might take a while\n",
      "... DONE\n",
      "39832\n"
     ]
    }
   ],
   "source": [
    "print(\"padding word2vec sentences... this might take a while\")\n",
    "padded_word2vec_train = get_input_array(embedded_train_sentences)\n",
    "print(\"... DONE\")\n",
    "print(len(padded_word2vec_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass size:  (10, 141, 200)\n",
      "BiLSTM output shape:  (10, 141, 400)\n",
      "Parse indices shape:  Tensor(\"Placeholder_2:0\", shape=(10, 141, 3), dtype=int64)\n",
      "MLP input shape:  (141, 3, 400)\n",
      "MLP output shape S{}:  (141, 99)\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # hyperparameters (from Cross & Huang, 2016)\n",
    "    batch_size = 10\n",
    "    n_input = 400\n",
    "    n_hidden = 200\n",
    "    n_classes = 99 # there are 99 possible actions to take\n",
    "    lstm_units = 200\n",
    "    num_epochs = 10\n",
    "    dropout = 0.5\n",
    "    L2_penalty = 0.\n",
    "    rho = 0.99\n",
    "    epsilon = 1e-07\n",
    "    learn_rate = 0.01\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h': tf.Variable(tf.random_normal([n_input, n_hidden], dtype=tf.float64)),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], dtype=tf.float64))\n",
    "    }\n",
    "    biases = {\n",
    "        'b': tf.Variable(tf.random_normal([n_hidden], dtype=tf.float64)),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes], dtype=tf.float64))\n",
    "    }\n",
    "    \n",
    "    # placeholders for input (input tensor uses None as variable sized dimension, \n",
    "    # or can use rows of train set length, or batch length might be better)\n",
    "    lstm_x = tf.placeholder(tf.float64, [batch_size, max_seq_length, word2vec_length])\n",
    "    # lstm output (1 pass, max sentence length, lstm output vector length)\n",
    "#     lstm_y = tf.placeholder(\"float\", [None, max_seq_length, n_hidden])\n",
    "    # mlp input (3 words, 2xlstm output)\n",
    "#     mlp_x = tf.placeholder(\"float\", [3, 2*n_hidden])\n",
    "    # mlp output (number of action classes)\n",
    "#     mlp_y = tf.placeholder(\"float\", [n_classes])\n",
    "    \n",
    "    # placeholder for sentence length per batch (one int for each sentence)\n",
    "    lstm_sent_lengths = tf.placeholder(tf.int64, [batch_size])\n",
    "    \n",
    "    # placeholder for indices (batch size, max sentence length, 3 indices)\n",
    "    parse_indices = tf.placeholder(tf.int64, [batch_size, max_seq_length, 3])\n",
    "    \n",
    "    # sentence lengths\n",
    "    sentence_lengths = tf.placeholder(tf.int64, [batch_size])\n",
    "    \n",
    "    # define LSTM cell + dropout wrapper (like Cross & Huang)\n",
    "    lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=lstm_units, state_is_tuple=True)\n",
    "    lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=lstm_units, state_is_tuple=True)\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell=lstm_fw_cell, output_keep_prob=0.5)\n",
    "\n",
    "    # define bidirectional architecture\n",
    "    outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=lstm_fw_cell,\n",
    "        cell_bw=lstm_bw_cell,\n",
    "        dtype=tf.float64,\n",
    "        sequence_length=lstm_sent_lengths,\n",
    "        inputs=lstm_x\n",
    "    )\n",
    "\n",
    "    # fw/bw output (num_sequences x max_seq_length x lstm_units)\n",
    "    output_fw, output_bw = outputs\n",
    "    print(\"Forward pass size: \", output_fw.get_shape())\n",
    "\n",
    "    # concatenate forward & backward outputs per word\n",
    "    output_lstm = tf.concat(2, outputs)\n",
    "    print(\"BiLSTM output shape: \", output_lstm.get_shape())\n",
    "    \n",
    "#     for i in range(batch_size):\n",
    "#         mlp_input_\n",
    "    print(\"Parse indices shape: \", parse_indices)\n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        mlp_x = tf.nn.embedding_lookup(output_lstm[i,:,:], parse_indices[i,:,:])\n",
    "\n",
    "#         for j in range(sentence_lengths[i]):\n",
    "        output_mlp = mlp(mlp_x, weights, biases)\n",
    "        ### TODO: ###\n",
    "        # opsplitsen in zinnen van correcte lengte (padding weg)\n",
    "        # voor elke plek in elke zin output mlp berekenen\n",
    "        # outputs in tensor/array als resultaat (kan dit wel?)\n",
    "        ### TODO: ###\n",
    "        \n",
    "        if i < 1:\n",
    "            print(\"MLP input shape: \", mlp_x.get_shape())\n",
    "            print(\"MLP output shape S{}: \", output_mlp.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 189)\n",
      "(39832, 282)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder_1:0\", shape=(10,), dtype=int64) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    868\u001b[0m             subfeed_t = self.graph.as_graph_element(subfeed, allow_tensor=True,\n\u001b[0;32m--> 869\u001b[0;31m                                                     allow_operation=False)\n\u001b[0m\u001b[1;32m    870\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2458\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2536\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2537\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2538\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"Placeholder_1:0\", shape=(10,), dtype=int64) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-88316185c968>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mfeed_dict_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlstm_sent_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtemp_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpadded_word2vec_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtemp_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mparse_indices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtemp_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    870\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             raise TypeError('Cannot interpret feed_dict key as Tensor: '\n\u001b[0;32m--> 872\u001b[0;31m                             + e.args[0])\n\u001b[0m\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder_1:0\", shape=(10,), dtype=int64) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "# now use a feed_dict\n",
    "\n",
    "temp_batch_size = 10\n",
    "\n",
    "print(padded_word2vec_train[0:temp_batch_size][0].shape)\n",
    "print(output_actions.shape)\n",
    "### TODO: ###\n",
    "# juiste acties zijn nu ints, moeten one-hot vectoren worden met een 1 op de juiste index\n",
    "# dat wordt gepassd aan session.run\n",
    "\n",
    "with tf.Session() as session:\n",
    "    feed_dict_batch = {lstm_sent_lengths: seq_lengths[0:temp_batch_size], lstm_x: padded_word2vec_train[0:temp_batch_size],  parse_indices: input_indices[0:10]}\n",
    "    result = session.run(output_actions[0:temp_batch_size], feed_dict_batch)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check_index = 1\n",
    "print(embedded_train_sentences[check_index].shape)\n",
    "print(len(embedded_train_sentences[check_index]))\n",
    "# print(embedded_train_sentences[check_index])\n",
    "print(type(embedded_train_sentences[check_index][0][0]))\n",
    "for word in train_sentences[check_index]:\n",
    "    print(word)\n",
    "# for emb in embedded_train_sentences[check_index]:\n",
    "#     print(emb)\n",
    "# for action in action_data[check_index]:\n",
    "#     print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### OUD OUD OUD\n",
    "\n",
    "##############################\n",
    "# build graph\n",
    "##############################\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # hyperparameters (from Cross & Huang, 2016)\n",
    "    batch_size = 10\n",
    "    n_input = 400\n",
    "    n_hidden = 200\n",
    "    n_classes = 99 # there are 99 possible actions to take\n",
    "    lstm_units = 200\n",
    "    num_epochs = 10\n",
    "    dropout = 0.5\n",
    "    L2_penalty = 0.\n",
    "    rho = 0.99\n",
    "    epsilon = 1e-07\n",
    "    learn_rate = 0.01\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h': tf.Variable(tf.random_normal([n_input, n_hidden], dtype=tf.float64)),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], dtype=tf.float64))\n",
    "    }\n",
    "    biases = {\n",
    "        'b': tf.Variable(tf.random_normal([n_hidden], dtype=tf.float64)),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes], dtype=tf.float64))\n",
    "    }\n",
    "\n",
    "    # load word2vec model\n",
    "    model_name = \"dep_parser_word2vec_total\"\n",
    "    if not 'model' in locals():\n",
    "        model = models.word2vec.Word2Vec.load(model_name)\n",
    "        print(\"Model loaded from disk\")\n",
    "    else:\n",
    "        print(\"Model was already loaded\")\n",
    "\n",
    "    # placeholders for input (input tensor uses None as variable sized dimension, \n",
    "    # or can use rows of train set length, or batch length might be better)\n",
    "    lstm_x = tf.placeholder(\"float\", [None, max_seq_length, word2vec_length])\n",
    "    # lstm output (1 pass)\n",
    "    lstm_y = tf.placeholder(\"float\", [None, max_seq_length, n_hidden])\n",
    "    # mlp input (3 words)\n",
    "    mlp_x = tf.placeholder(\"float\", [3, 2*n_hidden])\n",
    "    # mlp output\n",
    "    mlp_y = tf.placeholder(\"float\", [n_classes])\n",
    "    \n",
    "    \n",
    "    # dummy data:\n",
    "    # toy sentences (= list of lists of words)\n",
    "#     sentences = [[\"the\", \"by\", \"an\", \"on\", \"the\", \"in\", \"an\"], [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"ground\"]]\n",
    "#     toy_sentences = train_sentences[0:30]\n",
    "#     toy_parses = action_data[0:30]\n",
    "    # toy parses (= array with an action and word indices)\n",
    "#     parses = [np.array([[1,0,1,2], \n",
    "#                         [0,1,2,3],\n",
    "#                         [0,1,2,3],\n",
    "#                         [1,0,1,2]]),\n",
    "#               np.array([[1,0,1,2], \n",
    "#                         [0,1,2,3], \n",
    "#                         [3,2,3,4]])]\n",
    "    \n",
    "    # look up embeddings of words in sentence\n",
    "#     toy_embeddings = np.array(embedded_train_sentences[0:30])\n",
    "#     embeddings = embedding_lookup(toy_sentences, max_seq_length, vec_length)\n",
    "#     embeddings = embedded_train_sentences[0:30]\n",
    "#     print(\"Sentence embedding shape (np-array): \", embeddings.shape)\n",
    "\n",
    "    # define LSTM cell + dropout wrapper (like Cross & Huang)\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=lstm_units, state_is_tuple=True)\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=0.5)\n",
    "\n",
    "    # define bidirectional architecture\n",
    "    outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=cell,\n",
    "        cell_bw=cell,\n",
    "        dtype=tf.float64,\n",
    "        sequence_length=seq_lengths,\n",
    "        inputs=lstm_x\n",
    "    )\n",
    "\n",
    "    # fw/bw output (num_sequences x max_seq_length x lstm_units)\n",
    "    output_fw, output_bw = outputs\n",
    "    print(\"Forward pass size: \", output_fw.get_shape())\n",
    "\n",
    "    # concatenate forward & backward outputs per word\n",
    "    output_lstm = tf.concat(2, outputs)\n",
    "    print(\"BiLSTM output shape: \", output_lstm.get_shape())\n",
    "\n",
    "\n",
    "    # MLP layer filtering LSTM outputs based on parse steps\n",
    "    batch_cost = 0\n",
    "    for i in range(0, len(toy_sentences)):\n",
    "        action = action_data[i]\n",
    "#         sentence_input_indices = action[0:5]\n",
    "        print(input_indices)\n",
    "        output_action_index = action[-1]\n",
    "        print(output_action_index)\n",
    "        sentence = toy_sentences[i]\n",
    "#         print(len(sentence))\n",
    "        emb = embeddings[i]\n",
    "#         print(emb.shape())\n",
    "        parse = toy_parses[i]\n",
    "        # input: parse_steps (=?) x filtered_words (=3) x lstm_output_length (=400)\n",
    "        # output: parse_steps (=?) x num_classes\n",
    "        output_mlp = mlp(tf.gather(emb, parse), weights, biases)\n",
    "        print(\"MLP output shape S{}: \".format(i), output_mlp.get_shape())\n",
    "        cost = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(output_mlp, parse[:,0]))\n",
    "        batch_cost += cost\n",
    "    batch_cost /= len(sentences)\n",
    "\n",
    "    # define optimizer\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=True)\n",
    "    optimizer = tf.train.AdadeltaOptimizer(rho=rho, epsilon=epsilon).minimize(batch_cost)\n",
    "#     train_op = optimizer.train(loss, global_step=global_step)\n",
    "    correct = tf.nn.in_top_k(embeddings[0], parses[0], 1)\n",
    "    print(correct)\n",
    "\n",
    "    \n",
    "    ##############################\n",
    "    # start session in graph\n",
    "    ##############################\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
