{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "# import modules\n",
    "##############################\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim import models\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the file, \n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as myfile:\n",
    "        f = myfile.readlines()\n",
    "        s_num = 0\n",
    "        i =0\n",
    "        sentence_s = []\n",
    "        tag_s = []\n",
    "        dep_s = []\n",
    "        s  = []   # sentence\n",
    "        p = []    # tag\n",
    "        d = []    # dependency\n",
    "        for l in f:\n",
    "            \n",
    "            v = l.replace('\\n','').split(\"\\t\")\n",
    "            v.append(s_num)\n",
    "            if len(l) != 1:\n",
    "                data.append(v)\n",
    "                dep = v[6] + '_' + v[7]\n",
    "                word = v[1].lower()\n",
    "                if any(char.isdigit() for char in word):\n",
    "                    word = 'NUM'       # replace numbers with NUM\n",
    "                s.append(word)\n",
    "                p.append(v[3])\n",
    "                d.append(dep)\n",
    "                i +=1\n",
    "            else:\n",
    "                sentence_s.append(s)\n",
    "                tag_s.append(p)\n",
    "                dep_s.append(d)\n",
    "                s_num +=1\n",
    "                s  = []\n",
    "                p = []\n",
    "                d = []\n",
    "        \n",
    "    return data, sentence_s, tag_s, dep_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(dataname):\n",
    "    #reads in files, produces data structure with all actions\n",
    "        #does so by applying produce_rule_list to every sentence.\n",
    "        #for loop that sets actions to empty, calls p_r_l giving it\n",
    "        #the stack and buffer, actions and correct_parse, adds finished action list\n",
    "        #to new data file, for each sentence in the input data\n",
    "    #input: name of the data file with all parses. Run with data file in same directory.\n",
    "    #output: data file with all actions\n",
    "    file = open(dataname)\n",
    "    data = file.read()\n",
    "    correct_parses = correct_parse_list(data)\n",
    "    #gets rid of final whitespace\n",
    "    del correct_parses[len(correct_parses)-1]\n",
    "    \n",
    "    #iterates over all parses, producing action list for each\n",
    "    complete_rule_list = []\n",
    "    arc_dict = {'Shift':0,'L_root':1,'R_root':2}\n",
    "    for sentence_parse in correct_parses:\n",
    "        stack = []\n",
    "#         print(len(sentence_parse))\n",
    "        buff = list(range(1,len(sentence_parse)+1))\n",
    "        actions = []\n",
    "        rule_list, arc_dict = produce_rule_list(stack, buff, actions, sentence_parse, arc_dict)\n",
    "        complete_rule_list.append(np.array(rule_list))\n",
    "\n",
    "    \n",
    "    return complete_rule_list, arc_dict\n",
    "\n",
    "def correct_parse_list(data):\n",
    "    #Turns data into a list of lists of lists with relevant information\n",
    "    correct_parse = data.split(\"\\n\\n\")\n",
    "    for index, paragraph in enumerate(correct_parse):\n",
    "        correct_parse[index] = paragraph.split(\"\\n\")\n",
    "    for paragraph in correct_parse:\n",
    "        for index, line in enumerate(paragraph):\n",
    "            paragraph[index] = line.split(\"\\t\")\n",
    "    return correct_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_rule_list(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #recursive function that works through words in the sentence (stack/buffer)\n",
    "        #until only one word is left, creating the list of actions \n",
    "        #that was taken to parse it.\n",
    "    #input: stack, buffer, actions, correct parse\n",
    "    #output: actions with the actions taken for each buff/stack configuration\n",
    "    \n",
    "    #base case\n",
    "    if len(stack) == 1 and len(buff) == 0:\n",
    "        #actions.append([stack[:], \"empty\", \"R_arc\"])\n",
    "        actions.append([0,stack[0], 0, 2])\n",
    "        return actions, arc_dict\n",
    "\n",
    "    #If enough of the sentence is still left:\n",
    "    #If there is not enough material in the stack, shift:\n",
    "    if len(stack) == 0 :\n",
    "        #print('chose S - small stack')\n",
    "        actions.append([0,0,buff[0], 0])\n",
    "        stack.append(buff[0])\n",
    "        del buff[0]        \n",
    "        return produce_rule_list(stack,buff,actions,sentence_parse, arc_dict)\n",
    "    if len(stack) == 1:\n",
    "        actions.append([0,stack[-1],buff[0], 0])\n",
    "        stack.append(buff[0])\n",
    "        del buff[0]\n",
    "        return produce_rule_list(stack,buff,actions,sentence_parse, arc_dict)\n",
    "    #If there are 2 or more words in the stack, decide which action to perform and perform it\n",
    "    if len(stack) > 1:\n",
    "        action = rule_decision(stack,buff,sentence_parse)\n",
    "        stack, buff, actions, arc_dict = action(stack,buff,actions, sentence_parse, arc_dict)\n",
    "        return produce_rule_list(stack,buff,actions,sentence_parse, arc_dict)\n",
    "    \n",
    "\n",
    "def rule_decision(stack, buff, sentence_parse):\n",
    "    #determines which action to apply\n",
    "    #input: words on stack, words on buff, correct parse\n",
    "    #output: one of three methods, Shift(), L_arc(), R_arc()\n",
    "\n",
    "    #find ids/heads (index [6]) from stack and sentence_parse\n",
    "    s1 = stack[-2]\n",
    "    head_of_s1 = int(sentence_parse[s1-1][6])\n",
    "    s2 = stack[-1]\n",
    "    head_of_s2 = int(sentence_parse[s2-1][6])\n",
    "    \n",
    "    #L arcs can always be applied if possible\n",
    "    if head_of_s1 == s2:\n",
    "        action = L_arc\n",
    "        #print('chose L')\n",
    "    else:\n",
    "        #R arcs can only be applied if there is no word in the buffer which has the last word in the stack as a head\n",
    "        if head_of_s2 == s1:\n",
    "            buff_heads = [int(sentence_parse[x-1][6]) for x in buff]\n",
    "            if s2 in buff_heads:\n",
    "                action = Shift\n",
    "                #print('chose S - s2 in buffheads')\n",
    "            else:\n",
    "                action = R_arc\n",
    "                #print('chose R')\n",
    "        #if there is no match between s1 and s2, simply shift another word from the buffer\n",
    "        else:\n",
    "            action = Shift\n",
    "            #print('chose S - no matching s1s2')\n",
    "\n",
    "    return action\n",
    "\n",
    "#The following methods perform an arc or shift. These can be changed if more data is needed in the network.\n",
    "\n",
    "def L_arc(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #removes second to last item from stack, writes action to actions\n",
    "    #input: stack and actions\n",
    "    #output: new stack and actions with one L_arc line\n",
    "    #s1, s2, b1, action\n",
    "    s1 = int(stack[-2])\n",
    "    s2 = int(stack[-1])\n",
    "    if len(buff) == 0:\n",
    "        b1 = 0\n",
    "    else:\n",
    "        b1 = int(buff[0])\n",
    "    relation = \"L_\"+sentence_parse[s1-1][7]\n",
    "\n",
    "    if relation not in arc_dict:\n",
    "        maximum = max(arc_dict, key=arc_dict.get)\n",
    "        arc_dict['L_'+relation[2:]] = arc_dict[maximum]+1\n",
    "        arc_dict['R_'+relation[2:]] = arc_dict[maximum]+2\n",
    "    \n",
    "\n",
    "    actions.append([s1,s2,b1, arc_dict[relation]])\n",
    "    del stack[-2]\n",
    "    return stack, buff, actions, arc_dict\n",
    "\n",
    "\n",
    "\n",
    "def R_arc(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #removes last item from the stack, writes action to actions\n",
    "    #input: stack and actions\n",
    "    #output: new stack and actions with one R_arc line\n",
    "    #s1, s2, b1, action\n",
    "    s1 = int(stack[-2])\n",
    "    s2 = int(stack[-1])\n",
    "    if len(buff) == 0:\n",
    "        b1 = 0\n",
    "    else:\n",
    "        b1 = int(buff[0])\n",
    "        \n",
    "    relation = \"R_\"+sentence_parse[s2-1][7]\n",
    "\n",
    "    if relation not in arc_dict:\n",
    "        maximum = max(arc_dict, key=arc_dict.get)\n",
    "        arc_dict['L_'+relation[2:]] = arc_dict[maximum]+1\n",
    "        arc_dict['R_'+relation[2:]] = arc_dict[maximum]+2 \n",
    "    \n",
    "    actions.append([s1,s2,b1, arc_dict[relation]])\n",
    "    del stack[-1]\n",
    "    return stack, buff, actions, arc_dict\n",
    "\n",
    "\n",
    "\n",
    "def Shift(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #moves an item from the buff to the stack, writes action to actions\n",
    "    #input: stack, buff and actions\n",
    "    #output: new stack and actions with one extra shift line\n",
    "    #s1, s2, b1, action\n",
    "    s1 = int(stack[-2])\n",
    "    s2 = int(stack[-1])\n",
    "    b1 = int(buff[0])\n",
    "    #actions.append([stack[:], buff[:], \"Shift\"])\n",
    "    actions.append([s1,s2,b1, 0])\n",
    "    stack.append(buff[0])\n",
    "    del buff[0]\n",
    "    return stack, buff, actions, arc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  [['1', 'In', '_', 'IN', 'IN', '_', '45', 'prep', '_', '_', 0], ['2', 'an', '_', 'DT', 'DT', '_', '5', 'det', '_', '_', 0]]\n",
      "words sentences:  [['rolls-royce', 'motor', 'cars', 'inc.', 'said', 'it', 'expects', 'its', 'u.s.', 'sales', 'to', 'remain', 'steady', 'at', 'about', 'NUM', 'cars', 'in', 'NUM', '.'], ['the', 'luxury', 'auto', 'maker', 'last', 'year', 'sold', 'NUM', 'cars', 'in', 'the', 'u.s.']]\n",
      "tags sentences:  [['NNP', 'NNP', 'NNPS', 'NNP', 'VBD', 'PRP', 'VBZ', 'PRP$', 'NNP', 'NNS', 'TO', 'VB', 'JJ', 'IN', 'IN', 'CD', 'NNS', 'IN', 'CD', '.'], ['DT', 'NN', 'NN', 'NN', 'JJ', 'NN', 'VBD', 'CD', 'NNS', 'IN', 'DT', 'NNP']]\n",
      "dependencies:  [['4_nn', '4_nn', '4_nn', '5_nsubj', '0_root', '7_nsubj', '5_ccomp', '10_poss', '10_nn', '12_nsubj', '12_aux', '7_xcomp', '12_acomp', '12_prep', '16_quantmod', '17_num', '14_pobj', '12_prep', '18_pobj', '5_punct'], ['4_det', '4_nn', '4_nn', '7_nsubj', '6_amod', '7_tmod', '0_root', '9_num', '7_dobj', '7_prep', '12_det', '10_pobj']]\n"
     ]
    }
   ],
   "source": [
    "train_data, train_sentences, train_tags, train_dependencies = read_data('./data/train-stanford-raw.conll')\n",
    "dev_data, dev_sentences, dev_tags, dev_dependencies = read_data('./data/dev-stanford-raw.conll')\n",
    "test_data, test_sentences, test_tags, test_dependencies = read_data('./data/test-stanford-raw.conll')\n",
    "\n",
    "# create a full set of all the words in our train, test, and dev sets for word2vec model\n",
    "# in order to avoid unseen words during test and validation\n",
    "total_sentences = train_sentences + dev_sentences + test_sentences\n",
    "print('data: ', train_data[:2])\n",
    "print('words sentences: ', total_sentences[2:4])\n",
    "print('tags sentences: ', train_tags[2:4])\n",
    "print('dependencies: ', train_dependencies[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_data, arc_dict = process_data('./data/train-stanford-raw.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Interactive parser and evaluation functions #\n",
    "###############################################\n",
    "\n",
    "def sentences_to_conll(sentences, arc_dict, file_name):\n",
    "    action_dict = {v:k for k,v in arc_dict.items()}\n",
    "    all_sentences_listed = []\n",
    "    for sentence in sentences:\n",
    "        sentence_parse = [[i+1,word,'_','_','_','_','_','_','_','_'] \n",
    "                          for i, word in enumerate(sentence)]\n",
    "        w2v_matrix = create_sentence_embeddings([sentence])[0]\n",
    "        stack = []\n",
    "        buff = list(range(1,len(sentence)+1))\n",
    "        sentence_parse = single_sentence_parse(stack, buff, sentence_parse, action_dict, w2v_matrix)\n",
    "        all_sentences_listed.append(sentence_parse)\n",
    "    convert_to_conll(all_sentences_listed, file_name)        \n",
    "    return\n",
    "\n",
    "def single_sentence_parse(stack, buff, sentence_parse, action_dict, w2v_matrix):\n",
    "    #If there are 2 or more words in the stack, decide which action to perform and perform it\n",
    "    if len(stack) > 1:\n",
    "        s1 = int(stack[-2])\n",
    "        s2 = int(stack[-1])\n",
    "        #checks whether buffer contains words\n",
    "        if len(buff) > 0:\n",
    "            b1 = int(buff[0])\n",
    "            action = model_action_decision(w2v_matrix,s1,s2,b1,False)\n",
    "        else:\n",
    "            b1 = 0\n",
    "            action = model_action_decision(w2v_matrix,s1,s2,b1,True)\n",
    "        \n",
    "        if action == 0:\n",
    "            # perform a shift\n",
    "            stack, buff = Shift(stack, buff)\n",
    "        elif action%2 == 1:\n",
    "            # left-arc. All left tags are odd in the dictionary\n",
    "            stack, sentence_parse = L_arc(stack,s1,s2, sentence_parse, action_dict, action)\n",
    "        else:\n",
    "            # right-arc. All right tags are even in the dictionary\n",
    "            stack, sentence_parse = R_arc(stack,s1,s2, sentence_parse, action_dict, action)\n",
    "        return single_sentence_parse(stack, buff, sentence_parse, action_dict, w2v_matrix)\n",
    "    \n",
    "    #base case (R_arc): if only one word is left, perform the last right arc with root.\n",
    "    if len(stack) == 1 and len(buff) == 0:\n",
    "        sentence_parse[stack[0]-1][6] = 0\n",
    "        sentence_parse[stack[0]-1][7] = 'root'\n",
    "        return sentence_parse    \n",
    "\n",
    "    #If there is not enough material in the stack, shift:\n",
    "    if len(stack) == 0 :\n",
    "        #print('chose S - small stack')\n",
    "        stack, buff = Shift(stack, buff)       \n",
    "        return single_sentence_parse(stack, buff, sentence_parse, action_dict, w2v_matrix)\n",
    "    if len(stack) == 1:\n",
    "        stack, buff = Shift(stack, buff)\n",
    "        return single_sentence_parse(stack, buff, sentence_parse, action_dict, w2v_matrix)\n",
    "    \n",
    "def model_action_decision(w2v_matrix,s1,s2,b1,emptybuffer):\n",
    "    #if emptybuffer is true, exclude option 0 (shift).\n",
    "    pred_input = {sentence_length: [len(w2v_matrix)], \n",
    "                  lstm_x: [w2v_matrix],  parse_indices: \n",
    "                  [np.array([[s1,s2,b1]])]} # feed_dict without labels\n",
    "    prediction = session.run(output_mlp, pred_input)[0]\n",
    "    if emptybuffer == False:\n",
    "        action = np.argmax(prediction)\n",
    "    else:\n",
    "        pred = np.delete(prediction,[0])\n",
    "        action = np.argmax(pred)+1\n",
    "    return action\n",
    "        \n",
    "    \n",
    "def L_arc(stack,s1,s2, sentence_parse, action_dict, action):\n",
    "    #removes second to last item from stack, sends info to sentence_parse\n",
    "\n",
    "    action_type = action_dict[action]\n",
    "    \n",
    "    #update head and relation for s1\n",
    "    sentence_parse[s1-1][6] = s2\n",
    "    sentence_parse[s1-1][7] = action_type[2:]\n",
    "    \n",
    "    del stack[-2]\n",
    "    return stack, sentence_parse\n",
    "\n",
    "\n",
    "def R_arc(stack,s1,s2, sentence_parse, action_dict, action):\n",
    "    #removes last item from the stack, sends info to sentence_parse\n",
    "    \n",
    "    action_type = action_dict[action]\n",
    "\n",
    "    #update head and relation for s2\n",
    "    sentence_parse[s2-1][6] = s1\n",
    "    sentence_parse[s2-1][7] = action_type[2:]\n",
    "    \n",
    "    del stack[-1]\n",
    "    return stack, sentence_parse\n",
    "\n",
    "\n",
    "def Shift(stack, buff):\n",
    "    #moves an item from the buff to the stack\n",
    "    #input: stack, buff\n",
    "    #output: new stack and buff\n",
    "    stack.append(buff[0])\n",
    "    del buff[0]\n",
    "    return stack, buff\n",
    "\n",
    "def convert_to_conll(sentences, file_name):\n",
    "    content = \"\\n\\n\".join([\"\\n\".join([\"\\t\".join([str(var) \n",
    "                                                 for var in word]) \n",
    "                                      for word in sentence]) \n",
    "                           for sentence in sentences]) + \"\\n\"\n",
    "    with open(file_name+\".conll\", \"w\") as text_file:\n",
    "        text_file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# TF functions\n",
    "##############################\n",
    "\n",
    "def mlp(_X, _weights, _biases):\n",
    "    \"\"\"\n",
    "    function that defines a multilayer perceptron in the graph\n",
    "    input shape: parse_steps (=?) x filtered_words (=3) x lstm_output_length (=400)\n",
    "    output shape: parse_steps (=?) x num_classes\n",
    "    \"\"\"\n",
    "    # ReLU hidden layer (output shape: parse_steps x n_hidden)\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h']), _biases['b'])) \n",
    "    # return output layer (output shape: parse_steps x n_classes)\n",
    "    return tf.add(tf.matmul(layer_1, _weights['out']), _biases['out'])\n",
    "\n",
    "\n",
    "def create_sentence_embeddings(sentences):\n",
    "    \"\"\"\n",
    "    for each sentence, get embedded representation\n",
    "    \"\"\"\n",
    "    embedded_train_sentences = []\n",
    "    for sentence in sentences:\n",
    "        embed = model[sentence]\n",
    "        embedded_train_sentences.append(embed)\n",
    "    return embedded_train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# load word2vec model & input data\n",
    "##############################\n",
    "\n",
    "model_name = \"dep_parser_word2vec_total\"\n",
    "model = word2vec.Word2Vec.load(model_name)\n",
    "\n",
    "# embeddings for all sentences\n",
    "sentence_embeddings = create_sentence_embeddings(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_h:0\n",
      "Tensor(\"Shape_1:0\", shape=(1,), dtype=int32)\n",
      "weights_out:0\n",
      "Tensor(\"Shape_2:0\", shape=(1,), dtype=int32)\n",
      "biases_b:0\n",
      "Tensor(\"Shape_3:0\", shape=(0,), dtype=int32)\n",
      "biases_out:0\n",
      "Tensor(\"Shape_4:0\", shape=(0,), dtype=int32)\n",
      "BiRNN/FW/BasicLSTMCell/Linear/Matrix:0\n",
      "Tensor(\"Shape_5:0\", shape=(1,), dtype=int32)\n",
      "BiRNN/FW/BasicLSTMCell/Linear/Bias:0\n",
      "Tensor(\"Shape_6:0\", shape=(0,), dtype=int32)\n",
      "BiRNN/BW/BasicLSTMCell/Linear/Matrix:0\n",
      "Tensor(\"Shape_7:0\", shape=(1,), dtype=int32)\n",
      "BiRNN/BW/BasicLSTMCell/Linear/Bias:0\n",
      "Tensor(\"Shape_8:0\", shape=(0,), dtype=int32)\n",
      "<tensorflow.python.ops.variables.Variable object at 0x1394e1da0>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'DropoutWrapper' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-1aa6205d7e1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_is_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropoutWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_keep_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"defining bi-lstm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DropoutWrapper' object does not support indexing"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "# TensorFlow model\n",
    "##############################\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session() as session:\n",
    "        saver = tf.train.import_meta_graph('tf_models/model-8-30000.meta')\n",
    "        saver.restore(session, 'tf_models/model-8-30000')\n",
    "        \n",
    "        # get all vars\n",
    "        all_vars = tf.trainable_variables()\n",
    "        \n",
    "        for v in all_vars:\n",
    "            print(v.name)\n",
    "            print(tf.shape(session.run(v)[0]))\n",
    "        print(tf.Variable('BiRNN/FW/BasicLSTMCell/Linear/Matrix'))\n",
    "\n",
    "#         print(tf.get_collection(tf.GraphKeys.VARIABLES))\n",
    "\n",
    "#         restore trained weights & biases\n",
    "        weights = {\n",
    "            'h': all_vars[0],\n",
    "            'out': all_vars[1]\n",
    "        }\n",
    "        biases = {\n",
    "            'b': all_vars[2],\n",
    "            'out': all_vars[3]\n",
    "        }\n",
    "    \n",
    "#         tf.initialize_variables(all_vars[4:8])\n",
    "    \n",
    "        # hyperparameters (from Cross & Huang, 2016)\n",
    "        word2vec_length = model['a'].size\n",
    "        n_input = 400\n",
    "        n_hidden = 200\n",
    "        n_classes = 99 # there are 99 possible actions to take\n",
    "        lstm_units = 200\n",
    "        num_epochs = 10 # at least 1 for now\n",
    "        dropout = 0.5\n",
    "        L2_penalty = 0.\n",
    "        rho = 0.99\n",
    "        epsilon = 1e-07\n",
    "        learning_rate = 0.02 # default is 0.001, Cross & Huang do not specify learning rate\n",
    "        \n",
    "        # placeholders\n",
    "        sentence_length = tf.placeholder(tf.int32)\n",
    "        lstm_x = tf.placeholder(tf.float64, [1, None, word2vec_length])\n",
    "        parse_indices = tf.placeholder(tf.int64, [1, None, 3])\n",
    "        labels = tf.placeholder(tf.int64, [None])\n",
    "        \n",
    "        # load LSTM cell + dropout wrapper \n",
    "        load_cell_fw = {\n",
    "            all_vars[4],\n",
    "            all_vars[5]\n",
    "        }\n",
    "        load_cell_bw = {\n",
    "            all_vars[6],\n",
    "            all_vars[7]\n",
    "        }\n",
    "#         load_cell_fw = all_vars[5]\n",
    "#         load_cell_bw = all_vars[6]\n",
    "#         load_cell_bw = all_vars[7]\n",
    "\n",
    "#         fw_mat = tf.reshape(tf.Variable(all_vars[4]), [189])\n",
    "#         fw_bias = tf.reshape(tf.Variable(all_vars[5]), [189])\n",
    "\n",
    "#         fw_mat = tf.Variable(all_vars[4])\n",
    "#         fw_bias = tf.Variable(all_vars[5])\n",
    "        \n",
    "#         fw_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=lstm_units, state_is_tuple=True)\n",
    "#         fw_cell = tf.nn.rnn_cell.DropoutWrapper(cell=fw_cell, output_keep_prob=dropout)\n",
    "#         fw_cell([fw_mat, fw_bias])\n",
    "        \n",
    "#         bw_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=lstm_units, state_is_tuple=True)\n",
    "#         bw_cell = tf.nn.rnn_cell.DropoutWrapper(cell=bw_cell, output_keep_prob=dropout)\n",
    "#         bw_cell(bw_mat, bw_bias)\n",
    "\n",
    "#         cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=lstm_units, state_is_tuple=True)\n",
    "#         cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=dropout)\n",
    "#         cell = tf.get_variable('BiRNN/FW/BasicLSTMCell/Linear/Matrix')\n",
    "\n",
    "#         fw_cell(all_vars[4])\n",
    "#         bw_cell = tf.Variable(initial_value = all_vars[6])\n",
    "\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=lstm_units, state_is_tuple=True)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=dropout)\n",
    "        print(cell[0])\n",
    "\n",
    "        print(\"defining bi-lstm\")\n",
    "        # define bidirectional architecture\n",
    "        outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw=cell,\n",
    "            cell_bw=cell,\n",
    "#             initial_state_fw=tf.Variable('BiRNN/FW/BasicLSTMCell/Linear/Matrix'),\n",
    "#             initial_state_bw=tf.Variable('BiRNN/BW/BasicLSTMCell/Linear/Matrix'),\n",
    "            dtype=tf.float64,\n",
    "            sequence_length=sentence_length,\n",
    "            inputs=lstm_x\n",
    "        )\n",
    "                \n",
    "        # fw/bw output\n",
    "        output_fw, output_bw = outputs\n",
    "\n",
    "        # concatenate forward & backward outputs per word\n",
    "        output_lstm = tf.concat(2, outputs)\n",
    "\n",
    "        # zero-padding of LSTM-output (sentence gets a \"dummy word\" in front of it)\n",
    "        zero_padding = tf.zeros([1, 1, n_input], tf.float64)\n",
    "        # concatenate forward & backward outputs per word\n",
    "        output_lstm = tf.concat(1, [zero_padding, output_lstm])\n",
    "        \n",
    "        # put vars back in mlp\n",
    "        mlp_x = tf.nn.embedding_lookup(output_lstm[0,:,:], parse_indices[0,:,:])\n",
    "        dims = tf.shape(mlp_x)\n",
    "        mlp_x = tf.reshape(mlp_x, [dims[0], dims[1]*dims[2]])\n",
    "\n",
    "        output_mlp = mlp(mlp_x, weights, biases)\n",
    "        \n",
    "        \n",
    "        print(\"evaluating..\")\n",
    "        sentences_to_conll(train_sentences[:10], arc_dict, \"dev_pred\")\n",
    "        val_output = subprocess.check_output([\"perl\", \"eval.pl\", \"-g\", \"dev_true.conll\", \"-s\", \"dev_pred.conll\", \"-q\"])\n",
    "        print(val_output.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4567, 6, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "a = [3,5,6,678,4567,6,4,3,34,5,0]\n",
    "print(a[4:8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
