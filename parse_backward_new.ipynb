{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edwinlima/anaconda/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "# import modules\n",
    "##############################\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim import models\n",
    "import subprocess\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the file, \n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as myfile:\n",
    "        f = myfile.readlines()\n",
    "        s_num = 0\n",
    "        i =0\n",
    "        sentence_s = []\n",
    "        tag_s = []\n",
    "        dep_s = []\n",
    "        s  = []   # sentence\n",
    "        p = []    # tag\n",
    "        d = []    # dependency\n",
    "        for l in f:\n",
    "            \n",
    "            v = l.replace('\\n','').split(\"\\t\")\n",
    "            v.append(s_num)\n",
    "            if len(l) != 1:\n",
    "                data.append(v)\n",
    "                dep = v[6] + '_' + v[7]\n",
    "                word = v[1].lower()\n",
    "                if any(char.isdigit() for char in word):\n",
    "                    word = 'NUM'       # replace numbers with NUM\n",
    "                s.append(word)\n",
    "                p.append(v[3])\n",
    "                d.append(dep)\n",
    "                i +=1\n",
    "            else:\n",
    "                sentence_s.append(s)\n",
    "                tag_s.append(p)\n",
    "                dep_s.append(d)\n",
    "                s_num +=1\n",
    "                s  = []\n",
    "                p = []\n",
    "                d = []\n",
    "        \n",
    "    return data, sentence_s, tag_s, dep_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(dataname):\n",
    "    #reads in files, produces data structure with all actions\n",
    "        #does so by applying produce_rule_list to every sentence.\n",
    "        #for loop that sets actions to empty, calls p_r_l giving it\n",
    "        #the stack and buffer, actions and correct_parse, adds finished action list\n",
    "        #to new data file, for each sentence in the input data\n",
    "    #input: name of the data file with all parses. Run with data file in same directory.\n",
    "    #output: data file with all actions\n",
    "    file = open(dataname)\n",
    "    data = file.read()\n",
    "    correct_parses = correct_parse_list(data)\n",
    "    #gets rid of final whitespace\n",
    "    del correct_parses[len(correct_parses)-1]\n",
    "    \n",
    "    #iterates over all parses, producing action list for each\n",
    "    complete_rule_list = []\n",
    "    arc_dict = {'Shift':0,'L_root':1,'R_root':2}\n",
    "    for sentence_parse in correct_parses:\n",
    "        stack = []\n",
    "#         print(len(sentence_parse))\n",
    "        buff = list(range(1,len(sentence_parse)+1))\n",
    "        actions = []\n",
    "        rule_list, arc_dict = produce_rule_list(stack, buff, actions, sentence_parse, arc_dict)\n",
    "        complete_rule_list.append(np.array(rule_list))\n",
    "\n",
    "    \n",
    "    return complete_rule_list, arc_dict\n",
    "\n",
    "def correct_parse_list(data):\n",
    "    #Turns data into a list of lists of lists with relevant information\n",
    "    correct_parse = data.split(\"\\n\\n\")\n",
    "    for index, paragraph in enumerate(correct_parse):\n",
    "        correct_parse[index] = paragraph.split(\"\\n\")\n",
    "    for paragraph in correct_parse:\n",
    "        for index, line in enumerate(paragraph):\n",
    "            paragraph[index] = line.split(\"\\t\")\n",
    "    return correct_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_rule_list(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #recursive function that works through words in the sentence (stack/buffer)\n",
    "        #until only one word is left, creating the list of actions \n",
    "        #that was taken to parse it.\n",
    "    #input: stack, buffer, actions, correct parse\n",
    "    #output: actions with the actions taken for each buff/stack configuration\n",
    "    \n",
    "    #base case\n",
    "    if len(stack) == 1 and len(buff) == 0:\n",
    "        #actions.append([stack[:], \"empty\", \"R_arc\"])\n",
    "        actions.append([0,stack[0], 0, 2])\n",
    "        return actions, arc_dict\n",
    "\n",
    "    #If enough of the sentence is still left:\n",
    "    #If there is not enough material in the stack, shift:\n",
    "    if len(stack) == 0 :\n",
    "        #print('chose S - small stack')\n",
    "        actions.append([0,0,buff[0], 0])\n",
    "        stack.append(buff[0])\n",
    "        del buff[0]        \n",
    "        return produce_rule_list(stack,buff,actions,sentence_parse, arc_dict)\n",
    "    if len(stack) == 1:\n",
    "        actions.append([0,stack[-1],buff[0], 0])\n",
    "        stack.append(buff[0])\n",
    "        del buff[0]\n",
    "        return produce_rule_list(stack,buff,actions,sentence_parse, arc_dict)\n",
    "    #If there are 2 or more words in the stack, decide which action to perform and perform it\n",
    "    if len(stack) > 1:\n",
    "        action = rule_decision(stack,buff,sentence_parse)\n",
    "        stack, buff, actions, arc_dict = action(stack,buff,actions, sentence_parse, arc_dict)\n",
    "        return produce_rule_list(stack,buff,actions,sentence_parse, arc_dict)\n",
    "    \n",
    "\n",
    "def rule_decision(stack, buff, sentence_parse):\n",
    "    #determines which action to apply\n",
    "    #input: words on stack, words on buff, correct parse\n",
    "    #output: one of three methods, Shift(), L_arc(), R_arc()\n",
    "\n",
    "    #find ids/heads (index [6]) from stack and sentence_parse\n",
    "    s1 = stack[-2]\n",
    "    head_of_s1 = int(sentence_parse[s1-1][6])\n",
    "    s2 = stack[-1]\n",
    "    head_of_s2 = int(sentence_parse[s2-1][6])\n",
    "    \n",
    "    #L arcs can always be applied if possible\n",
    "    if head_of_s1 == s2:\n",
    "        action = L_arc\n",
    "        #print('chose L')\n",
    "    else:\n",
    "        #R arcs can only be applied if there is no word in the buffer which has the last word in the stack as a head\n",
    "        if head_of_s2 == s1:\n",
    "            buff_heads = [int(sentence_parse[x-1][6]) for x in buff]\n",
    "            if s2 in buff_heads:\n",
    "                action = Shift\n",
    "                #print('chose S - s2 in buffheads')\n",
    "            else:\n",
    "                action = R_arc\n",
    "                #print('chose R')\n",
    "        #if there is no match between s1 and s2, simply shift another word from the buffer\n",
    "        else:\n",
    "            action = Shift\n",
    "            #print('chose S - no matching s1s2')\n",
    "\n",
    "    return action\n",
    "\n",
    "#The following methods perform an arc or shift. These can be changed if more data is needed in the network.\n",
    "\n",
    "def L_arc(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #removes second to last item from stack, writes action to actions\n",
    "    #input: stack and actions\n",
    "    #output: new stack and actions with one L_arc line\n",
    "    #s1, s2, b1, action\n",
    "    s1 = int(stack[-2])\n",
    "    s2 = int(stack[-1])\n",
    "    if len(buff) == 0:\n",
    "        b1 = 0\n",
    "    else:\n",
    "        b1 = int(buff[0])\n",
    "    relation = \"L_\"+sentence_parse[s1-1][7]\n",
    "\n",
    "    if relation not in arc_dict:\n",
    "        maximum = max(arc_dict, key=arc_dict.get)\n",
    "        arc_dict['L_'+relation[2:]] = arc_dict[maximum]+1\n",
    "        arc_dict['R_'+relation[2:]] = arc_dict[maximum]+2\n",
    "    \n",
    "\n",
    "    actions.append([s1,s2,b1, arc_dict[relation]])\n",
    "    del stack[-2]\n",
    "    return stack, buff, actions, arc_dict\n",
    "\n",
    "\n",
    "\n",
    "def R_arc(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #removes last item from the stack, writes action to actions\n",
    "    #input: stack and actions\n",
    "    #output: new stack and actions with one R_arc line\n",
    "    #s1, s2, b1, action\n",
    "    s1 = int(stack[-2])\n",
    "    s2 = int(stack[-1])\n",
    "    if len(buff) == 0:\n",
    "        b1 = 0\n",
    "    else:\n",
    "        b1 = int(buff[0])\n",
    "        \n",
    "    relation = \"R_\"+sentence_parse[s2-1][7]\n",
    "\n",
    "    if relation not in arc_dict:\n",
    "        maximum = max(arc_dict, key=arc_dict.get)\n",
    "        arc_dict['L_'+relation[2:]] = arc_dict[maximum]+1\n",
    "        arc_dict['R_'+relation[2:]] = arc_dict[maximum]+2 \n",
    "    \n",
    "    actions.append([s1,s2,b1, arc_dict[relation]])\n",
    "    del stack[-1]\n",
    "    return stack, buff, actions, arc_dict\n",
    "\n",
    "\n",
    "\n",
    "def Shift(stack, buff, actions, sentence_parse, arc_dict):\n",
    "    #moves an item from the buff to the stack, writes action to actions\n",
    "    #input: stack, buff and actions\n",
    "    #output: new stack and actions with one extra shift line\n",
    "    #s1, s2, b1, action\n",
    "    s1 = int(stack[-2])\n",
    "    s2 = int(stack[-1])\n",
    "    b1 = int(buff[0])\n",
    "    #actions.append([stack[:], buff[:], \"Shift\"])\n",
    "    actions.append([s1,s2,b1, 0])\n",
    "    stack.append(buff[0])\n",
    "    del buff[0]\n",
    "    return stack, buff, actions, arc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  [['1', 'In', '_', 'IN', 'IN', '_', '45', 'prep', '_', '_', 0], ['2', 'an', '_', 'DT', 'DT', '_', '5', 'det', '_', '_', 0]]\n",
      "words sentences:  [['rolls-royce', 'motor', 'cars', 'inc.', 'said', 'it', 'expects', 'its', 'u.s.', 'sales', 'to', 'remain', 'steady', 'at', 'about', 'NUM', 'cars', 'in', 'NUM', '.'], ['the', 'luxury', 'auto', 'maker', 'last', 'year', 'sold', 'NUM', 'cars', 'in', 'the', 'u.s.']]\n",
      "tags sentences:  [['NNP', 'NNP', 'NNPS', 'NNP', 'VBD', 'PRP', 'VBZ', 'PRP$', 'NNP', 'NNS', 'TO', 'VB', 'JJ', 'IN', 'IN', 'CD', 'NNS', 'IN', 'CD', '.'], ['DT', 'NN', 'NN', 'NN', 'JJ', 'NN', 'VBD', 'CD', 'NNS', 'IN', 'DT', 'NNP']]\n",
      "dependencies:  [['4_nn', '4_nn', '4_nn', '5_nsubj', '0_root', '7_nsubj', '5_ccomp', '10_poss', '10_nn', '12_nsubj', '12_aux', '7_xcomp', '12_acomp', '12_prep', '16_quantmod', '17_num', '14_pobj', '12_prep', '18_pobj', '5_punct'], ['4_det', '4_nn', '4_nn', '7_nsubj', '6_amod', '7_tmod', '0_root', '9_num', '7_dobj', '7_prep', '12_det', '10_pobj']]\n"
     ]
    }
   ],
   "source": [
    "train_data, train_sentences, train_tags, train_dependencies = read_data('./data/train-stanford-raw.conll')\n",
    "dev_data, dev_sentences, dev_tags, dev_dependencies = read_data('./data/dev-stanford-raw.conll')\n",
    "test_data, test_sentences, test_tags, test_dependencies = read_data('./data/test-stanford-raw.conll')\n",
    "\n",
    "# create a full set of all the words in our train, test, and dev sets for word2vec model\n",
    "# in order to avoid unseen words during test and validation\n",
    "total_sentences = train_sentences + dev_sentences + test_sentences\n",
    "print('data: ', train_data[:2])\n",
    "print('words sentences: ', total_sentences[2:4])\n",
    "print('tags sentences: ', train_tags[2:4])\n",
    "print('dependencies: ', train_dependencies[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action_data, arc_dict = process_data('./data/train-stanford-raw.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Interactive parser and evaluation functions #\n",
    "###############################################\n",
    "\n",
    "def sentences_to_conll(sentences, arc_dict, file_name):\n",
    "    action_dict = {v:k for k,v in arc_dict.items()}\n",
    "    all_sentences_listed = []\n",
    "    for sentence in sentences:\n",
    "        sentence_parse = [[i+1,word,'_','_','_','_','_','_','_','_'] \n",
    "                          for i, word in enumerate(sentence)]\n",
    "        w2v_matrix = create_sentence_embeddings([sentence])[0]\n",
    "        stack = []\n",
    "        buff = list(range(1,len(sentence)+1))\n",
    "        sentence_parse = single_sentence_parse(stack, buff, sentence_parse, action_dict, w2v_matrix)\n",
    "        all_sentences_listed.append(sentence_parse)\n",
    "    convert_to_conll(all_sentences_listed, file_name)        \n",
    "    return\n",
    "\n",
    "def single_sentence_parse(stack, buff, sentence_parse, action_dict, w2v_matrix):\n",
    "    #If there are 2 or more words in the stack, decide which action to perform and perform it\n",
    "    if len(stack) > 1:\n",
    "        s1 = int(stack[-2])\n",
    "        s2 = int(stack[-1])\n",
    "        #checks whether buffer contains words\n",
    "        if len(buff) > 0:\n",
    "            b1 = int(buff[0])\n",
    "            action = model_action_decision(w2v_matrix,s1,s2,b1,False)\n",
    "        else:\n",
    "            b1 = 0\n",
    "            action = model_action_decision(w2v_matrix,s1,s2,b1,True)\n",
    "        \n",
    "        if action == 0:\n",
    "            # perform a shift\n",
    "            stack, buff = Shift(stack, buff)\n",
    "        elif action%2 == 1:\n",
    "            # left-arc. All left tags are odd in the dictionary\n",
    "            stack, sentence_parse = L_arc(stack,s1,s2, sentence_parse, action_dict, action)\n",
    "        else:\n",
    "            # right-arc. All right tags are even in the dictionary\n",
    "            stack, sentence_parse = R_arc(stack,s1,s2, sentence_parse, action_dict, action)\n",
    "        return single_sentence_parse(stack, buff, sentence_parse, action_dict, w2v_matrix)\n",
    "    \n",
    "    #base case (R_arc): if only one word is left, perform the last right arc with root.\n",
    "    if len(stack) == 1 and len(buff) == 0:\n",
    "        sentence_parse[stack[0]-1][6] = 0\n",
    "        sentence_parse[stack[0]-1][7] = 'root'\n",
    "        return sentence_parse    \n",
    "\n",
    "    #If there is not enough material in the stack, shift:\n",
    "    if len(stack) == 0 :\n",
    "        #print('chose S - small stack')\n",
    "        stack, buff = Shift(stack, buff)       \n",
    "        return single_sentence_parse(stack, buff, sentence_parse, action_dict, w2v_matrix)\n",
    "    if len(stack) == 1:\n",
    "        stack, buff = Shift(stack, buff)\n",
    "        return single_sentence_parse(stack, buff, sentence_parse, action_dict, w2v_matrix)\n",
    "    \n",
    "def model_action_decision(w2v_matrix,s1,s2,b1,emptybuffer):\n",
    "    #if emptybuffer is true, exclude option 0 (shift).\n",
    "    pred_input = {sentence_length: [len(w2v_matrix)], \n",
    "                  lstm_x: [w2v_matrix],  parse_indices: \n",
    "                  [np.array([[s1,s2,b1]])]} # feed_dict without labels\n",
    "    prediction = session.run(output_mlp, pred_input)[0]\n",
    "    if emptybuffer == False:\n",
    "        action = np.argmax(prediction)\n",
    "    else:\n",
    "        pred = np.delete(prediction,[0])\n",
    "        action = np.argmax(pred)+1\n",
    "    return action\n",
    "        \n",
    "    \n",
    "def L_arc(stack,s1,s2, sentence_parse, action_dict, action):\n",
    "    #removes second to last item from stack, sends info to sentence_parse\n",
    "\n",
    "    action_type = action_dict[action]\n",
    "    \n",
    "    #update head and relation for s1\n",
    "    sentence_parse[s1-1][6] = s2\n",
    "    sentence_parse[s1-1][7] = action_type[2:]\n",
    "    \n",
    "    del stack[-2]\n",
    "    return stack, sentence_parse\n",
    "\n",
    "\n",
    "def R_arc(stack,s1,s2, sentence_parse, action_dict, action):\n",
    "    #removes last item from the stack, sends info to sentence_parse\n",
    "    \n",
    "    action_type = action_dict[action]\n",
    "\n",
    "    #update head and relation for s2\n",
    "    sentence_parse[s2-1][6] = s1\n",
    "    sentence_parse[s2-1][7] = action_type[2:]\n",
    "    \n",
    "    del stack[-1]\n",
    "    return stack, sentence_parse\n",
    "\n",
    "\n",
    "def Shift(stack, buff):\n",
    "    #moves an item from the buff to the stack\n",
    "    #input: stack, buff\n",
    "    #output: new stack and buff\n",
    "    stack.append(buff[0])\n",
    "    del buff[0]\n",
    "    return stack, buff\n",
    "\n",
    "def convert_to_conll(sentences, file_name):\n",
    "    content = \"\\n\\n\".join([\"\\n\".join([\"\\t\".join([str(var) \n",
    "                                                 for var in word]) \n",
    "                                      for word in sentence]) \n",
    "                           for sentence in sentences]) + \"\\n\"\n",
    "    with open(file_name+\".conll\", \"w\") as text_file:\n",
    "        text_file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# TF functions\n",
    "##############################\n",
    "\n",
    "def mlp(_X, _weights, _biases):\n",
    "    \"\"\"\n",
    "    function that defines a multilayer perceptron in the graph\n",
    "    input shape: parse_steps (=?) x filtered_words (=3) x lstm_output_length (=400)\n",
    "    output shape: parse_steps (=?) x num_classes\n",
    "    \"\"\"\n",
    "    # ReLU hidden layer (output shape: parse_steps x n_hidden)\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h']), _biases['b'])) \n",
    "    # return output layer (output shape: parse_steps x n_classes)\n",
    "    return tf.add(tf.matmul(layer_1, _weights['out']), _biases['out'])\n",
    "\n",
    "\n",
    "def create_sentence_embeddings(sentences):\n",
    "    \"\"\"\n",
    "    for each sentence, get embedded representation\n",
    "    \"\"\"\n",
    "    embedded_train_sentences = []\n",
    "    for sentence in sentences:\n",
    "        embed = model[sentence]\n",
    "        embedded_train_sentences.append(embed)\n",
    "    return embedded_train_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# load word2vec model & input data\n",
    "##############################\n",
    "\n",
    "model_name = \"dep_parser_word2vec_total\"\n",
    "model = word2vec.Word2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edwinlima/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 , iteration  0 \n",
      " current average loss:  137.784109221\n",
      "evaluating..\n",
      "  Labeled   attachment score: 0 / 500 * 100 = 0.00 %\n",
      "  Unlabeled attachment score: 78 / 500 * 100 = 15.60 %\n",
      "  Label accuracy score:       14 / 500 * 100 = 2.80 %\n",
      "\n",
      "epoch  0 , iteration  1000 \n",
      " current average loss:  24.5390372005\n",
      "epoch  0 , iteration  2000 \n",
      " current average loss:  9.11041217374\n",
      "epoch  0 , iteration  3000 \n",
      " current average loss:  11.3919953638\n",
      "epoch  0 , iteration  4000 \n",
      " current average loss:  11.3627595086\n",
      "epoch  0 , iteration  5000 \n",
      " current average loss:  7.7920270269\n",
      "evaluating..\n",
      "  Labeled   attachment score: 45 / 500 * 100 = 9.00 %\n",
      "  Unlabeled attachment score: 108 / 500 * 100 = 21.60 %\n",
      "  Label accuracy score:       101 / 500 * 100 = 20.20 %\n",
      "\n",
      "epoch  0 , iteration  6000 \n",
      " current average loss:  6.30634793563\n",
      "epoch  0 , iteration  7000 \n",
      " current average loss:  7.1912482718\n",
      "epoch  0 , iteration  8000 \n",
      " current average loss:  6.05371742988\n",
      "epoch  0 , iteration  9000 \n",
      " current average loss:  3.27041025179\n",
      "epoch  0 , iteration  10000 \n",
      " current average loss:  4.27974198739\n",
      "evaluating..\n",
      "  Labeled   attachment score: 84 / 500 * 100 = 16.80 %\n",
      "  Unlabeled attachment score: 134 / 500 * 100 = 26.80 %\n",
      "  Label accuracy score:       146 / 500 * 100 = 29.20 %\n",
      "\n",
      "epoch  0 , iteration  11000 \n",
      " current average loss:  4.13795441023\n",
      "epoch  0 , iteration  12000 \n",
      " current average loss:  3.22432128263\n",
      "epoch  0 , iteration  13000 \n",
      " current average loss:  2.79618858169\n",
      "epoch  0 , iteration  14000 \n",
      " current average loss:  3.79206661327\n",
      "epoch  0 , iteration  15000 \n",
      " current average loss:  1.5177089057\n",
      "evaluating..\n",
      "  Labeled   attachment score: 121 / 500 * 100 = 24.20 %\n",
      "  Unlabeled attachment score: 177 / 500 * 100 = 35.40 %\n",
      "  Label accuracy score:       192 / 500 * 100 = 38.40 %\n",
      "\n",
      "epoch  0 , iteration  16000 \n",
      " current average loss:  2.37517674005\n",
      "epoch  0 , iteration  17000 \n",
      " current average loss:  1.9236493226\n",
      "epoch  0 , iteration  18000 \n",
      " current average loss:  2.61026152043\n",
      "epoch  0 , iteration  19000 \n",
      " current average loss:  2.35137146278\n",
      "epoch  0 , iteration  20000 \n",
      " current average loss:  2.56804753189\n",
      "evaluating..\n",
      "  Labeled   attachment score: 130 / 500 * 100 = 26.00 %\n",
      "  Unlabeled attachment score: 187 / 500 * 100 = 37.40 %\n",
      "  Label accuracy score:       210 / 500 * 100 = 42.00 %\n",
      "\n",
      "epoch  0 , iteration  21000 \n",
      " current average loss:  1.54548674133\n",
      "epoch  0 , iteration  22000 \n",
      " current average loss:  1.66042854314\n",
      "epoch  0 , iteration  23000 \n",
      " current average loss:  1.18184700378\n",
      "epoch  0 , iteration  24000 \n",
      " current average loss:  2.13606406011\n",
      "epoch  0 , iteration  25000 \n",
      " current average loss:  0.873004376924\n",
      "evaluating..\n",
      "  Labeled   attachment score: 170 / 500 * 100 = 34.00 %\n",
      "  Unlabeled attachment score: 220 / 500 * 100 = 44.00 %\n",
      "  Label accuracy score:       236 / 500 * 100 = 47.20 %\n",
      "\n",
      "epoch  0 , iteration  26000 \n",
      " current average loss:  1.49061963869\n",
      "epoch  0 , iteration  27000 \n",
      " current average loss:  1.7917919539\n",
      "epoch  0 , iteration  28000 \n",
      " current average loss:  1.04863949446\n",
      "epoch  0 , iteration  29000 \n",
      " current average loss:  1.58435006126\n",
      "epoch  0 , iteration  30000 \n",
      " current average loss:  1.16410089272\n",
      "evaluating..\n",
      "  Labeled   attachment score: 185 / 500 * 100 = 37.00 %\n",
      "  Unlabeled attachment score: 238 / 500 * 100 = 47.60 %\n",
      "  Label accuracy score:       241 / 500 * 100 = 48.20 %\n",
      "\n",
      "epoch  0 , iteration  31000 \n",
      " current average loss:  1.1394505041\n",
      "epoch  0 , iteration  32000 \n",
      " current average loss:  1.12416009364\n",
      "epoch  0 , iteration  33000 \n",
      " current average loss:  1.53941834094\n",
      "epoch  0 , iteration  34000 \n",
      " current average loss:  1.21564019658\n",
      "epoch  0 , iteration  35000 \n",
      " current average loss:  0.439058885512\n",
      "evaluating..\n",
      "  Labeled   attachment score: 184 / 500 * 100 = 36.80 %\n",
      "  Unlabeled attachment score: 230 / 500 * 100 = 46.00 %\n",
      "  Label accuracy score:       259 / 500 * 100 = 51.80 %\n",
      "\n",
      "epoch  0 , iteration  36000 \n",
      " current average loss:  0.749856981365\n",
      "epoch  0 , iteration  37000 \n",
      " current average loss:  1.25794981413\n",
      "epoch  0 , iteration  38000 \n",
      " current average loss:  2.3700283231\n",
      "epoch  0 , iteration  39000 \n",
      " current average loss:  1.34263589652\n",
      "evaluating on test set..\n",
      "  Labeled   attachment score: 784 / 2033 * 100 = 38.56 %\n",
      "  Unlabeled attachment score: 994 / 2033 * 100 = 48.89 %\n",
      "  Label accuracy score:       1067 / 2033 * 100 = 52.48 %\n",
      "\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "# TensorFlow model\n",
    "##############################\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# embeddings for all sentences\n",
    "sentence_embeddings = create_sentence_embeddings(train_sentences)\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "        \n",
    "    # hyperparameters (from Cross & Huang, 2016)\n",
    "    word2vec_length = model['a'].size\n",
    "    n_input = 200\n",
    "    n_hidden = 200\n",
    "    n_classes = 99 # there are 99 possible actions to take\n",
    "    lstm_units = 200\n",
    "    num_epochs = 1 # at least 1 for now\n",
    "    dropout = 0.5\n",
    "    L2_penalty = 0.\n",
    "    rho = 0.99\n",
    "    epsilon = 1e-07\n",
    "    learning_rate = 0.02 # default is 0.001, Cross & Huang do not specify learning rate\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h': tf.Variable(tf.random_normal([3*n_input, n_hidden], dtype=tf.float64), name='weights_h'),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], dtype=tf.float64), name='weights_out')\n",
    "    }\n",
    "    biases = {\n",
    "        'b': tf.Variable(tf.random_normal([n_hidden], dtype=tf.float64), name='biases_b'),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes], dtype=tf.float64), name='biases_out')\n",
    "    }\n",
    "    \n",
    "    # placeholders\n",
    "    sentence_length = tf.placeholder(tf.int32)\n",
    "    lstm_x = tf.placeholder(tf.float64, [1, None, word2vec_length])\n",
    "    parse_indices = tf.placeholder(tf.int64, [1, None, 3])\n",
    "    labels = tf.placeholder(tf.int64, [None])\n",
    "    \n",
    "    # define LSTM cell + dropout wrapper (like Cross & Huang)\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=lstm_units, state_is_tuple=True)\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=dropout)\n",
    "\n",
    "    # define bidirectional architecture\n",
    "    outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=cell,\n",
    "        cell_bw=cell,\n",
    "        dtype=tf.float64,\n",
    "        sequence_length=sentence_length,\n",
    "        inputs=lstm_x\n",
    "    )\n",
    "\n",
    "    # fw/bw output\n",
    "    output_fw, output_bw = outputs\n",
    "\n",
    "    # concatenate forward & backward outputs per word\n",
    "    output_lstm = output_bw\n",
    "    \n",
    "    # zero-padding of LSTM-output (sentence gets a \"dummy word\" in front of it)\n",
    "    zero_padding = tf.zeros([1, 1, n_input], tf.float64)\n",
    "    output_lstm = tf.concat(1, [zero_padding, output_lstm])\n",
    "   \n",
    "    # mlp_x: make a matrix with all corresponding word vector 3-tuples, CONCATENATED (up to the no of parse steps)\n",
    "    mlp_x = tf.nn.embedding_lookup(output_lstm[0,:,:], parse_indices[0,:,:])\n",
    "    dims = tf.shape(mlp_x)\n",
    "    mlp_x = tf.reshape(mlp_x, [dims[0], dims[1]*dims[2]])\n",
    "\n",
    "    output_mlp = mlp(mlp_x, weights, biases)\n",
    "\n",
    "    cost = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(output_mlp, labels))\n",
    "    cost_indication = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(output_mlp, labels))\n",
    "\n",
    "    train_op = tf.train.AdadeltaOptimizer(rho=rho, epsilon=epsilon, learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        init = tf.initialize_all_variables()\n",
    "        session.run(init)\n",
    "        \n",
    "        loss_data = []\n",
    "        \n",
    "        for epoch in range(0,num_epochs): \n",
    "            \n",
    "            epoch_loss_data = []\n",
    "            \n",
    "            # randomize the order of the sentences per epoch\n",
    "            c = list(zip(sentence_embeddings, action_data))\n",
    "            random.shuffle(c)\n",
    "            sentence_embeddings, action_data = zip(*c)\n",
    "            \n",
    "            for i in range(0,len(sentence_embeddings)): \n",
    "\n",
    "                # get sentence embedding\n",
    "                sentence = sentence_embeddings[i]\n",
    "                \n",
    "                # get parse data for sentence\n",
    "                parse_data = action_data[i]\n",
    "                \n",
    "                indices = parse_data[:,:3]\n",
    "                actions = parse_data[:,3]\n",
    "                \n",
    "                # important variables\n",
    "                seq_length = len(sentence)\n",
    "                                \n",
    "                feed_dict_batch = {sentence_length: [seq_length], lstm_x: [sentence],  parse_indices: [indices], labels: actions}\n",
    "                \n",
    "                result = session.run([train_op, cost_indication], feed_dict_batch)\n",
    "                \n",
    "                if i%1000 == 0:\n",
    "                    epoch_loss_data.append(result[1])\n",
    "                    print(\"epoch \", epoch, \", iteration \", i, \"\\n\", \"current average loss: \", result[1])\n",
    "                \n",
    "                if i%5000 == 0:\n",
    "                    print(\"evaluating..\")\n",
    "                    sentences_to_conll(dev_sentences[:20], arc_dict, \"dev_pred\")\n",
    "                    val_output = subprocess.check_output([\"perl\", \"eval.pl\", \"-g\", \"dev_true.conll\", \"-s\", \"dev_pred.conll\", \"-q\"])\n",
    "                    print(val_output.decode(\"utf-8\"))\n",
    "                \n",
    "                # check performance on first 100 sentences of the test set after every epoch\n",
    "                if i == len(sentence_embeddings)-1:\n",
    "                    print(\"evaluating on test set..\")\n",
    "                    sentences_to_conll(test_sentences[:100], arc_dict, \"test_pred_100\")\n",
    "                    test_output = subprocess.check_output([\"perl\", \"eval.pl\", \"-g\", \"test_true_100.conll\", \"-s\", \"test_pred_100.conll\", \"-q\"])\n",
    "                    print(test_output.decode(\"utf-8\"))\n",
    "                    \n",
    "            loss_data.append(epoch_loss_data)\n",
    "        \n",
    "        np.savetxt('loss_data_backward.txt', np.array(loss_data))\n",
    "        print(\"DONE\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
